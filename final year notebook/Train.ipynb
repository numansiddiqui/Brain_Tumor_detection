{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc03148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [1/179], Loss: 1.3723\n",
      "Epoch [1/8], Step [2/179], Loss: 1.7862\n",
      "Epoch [1/8], Step [3/179], Loss: 1.6221\n",
      "Epoch [1/8], Step [4/179], Loss: 1.3796\n",
      "Epoch [1/8], Step [5/179], Loss: 1.4366\n",
      "Epoch [1/8], Step [6/179], Loss: 1.3365\n",
      "Epoch [1/8], Step [7/179], Loss: 1.6159\n",
      "Epoch [1/8], Step [8/179], Loss: 1.5261\n",
      "Epoch [1/8], Step [9/179], Loss: 1.5549\n",
      "Epoch [1/8], Step [10/179], Loss: 1.5730\n",
      "Epoch [1/8], Step [11/179], Loss: 1.4025\n",
      "Epoch [1/8], Step [12/179], Loss: 1.3624\n",
      "Epoch [1/8], Step [13/179], Loss: 1.3112\n",
      "Epoch [1/8], Step [14/179], Loss: 1.4970\n",
      "Epoch [1/8], Step [15/179], Loss: 1.5679\n",
      "Epoch [1/8], Step [16/179], Loss: 1.4157\n",
      "Epoch [1/8], Step [17/179], Loss: 1.3363\n",
      "Epoch [1/8], Step [18/179], Loss: 1.4282\n",
      "Epoch [1/8], Step [19/179], Loss: 1.4936\n",
      "Epoch [1/8], Step [20/179], Loss: 1.5309\n",
      "Epoch [1/8], Step [21/179], Loss: 1.3357\n",
      "Epoch [1/8], Step [22/179], Loss: 1.3987\n",
      "Epoch [1/8], Step [23/179], Loss: 1.4131\n",
      "Epoch [1/8], Step [24/179], Loss: 1.3804\n",
      "Epoch [1/8], Step [25/179], Loss: 1.3614\n",
      "Epoch [1/8], Step [26/179], Loss: 1.3752\n",
      "Epoch [1/8], Step [27/179], Loss: 1.3991\n",
      "Epoch [1/8], Step [28/179], Loss: 1.3972\n",
      "Epoch [1/8], Step [29/179], Loss: 1.4382\n",
      "Epoch [1/8], Step [30/179], Loss: 1.3693\n",
      "Epoch [1/8], Step [31/179], Loss: 1.4126\n",
      "Epoch [1/8], Step [32/179], Loss: 1.4075\n",
      "Epoch [1/8], Step [33/179], Loss: 1.4361\n",
      "Epoch [1/8], Step [34/179], Loss: 1.4012\n",
      "Epoch [1/8], Step [35/179], Loss: 1.3851\n",
      "Epoch [1/8], Step [36/179], Loss: 1.3757\n",
      "Epoch [1/8], Step [37/179], Loss: 1.3291\n",
      "Epoch [1/8], Step [38/179], Loss: 1.4005\n",
      "Epoch [1/8], Step [39/179], Loss: 1.3889\n",
      "Epoch [1/8], Step [40/179], Loss: 1.3896\n",
      "Epoch [1/8], Step [41/179], Loss: 1.4387\n",
      "Epoch [1/8], Step [42/179], Loss: 1.4023\n",
      "Epoch [1/8], Step [43/179], Loss: 1.4117\n",
      "Epoch [1/8], Step [44/179], Loss: 1.4057\n",
      "Epoch [1/8], Step [45/179], Loss: 1.4239\n",
      "Epoch [1/8], Step [46/179], Loss: 1.3777\n",
      "Epoch [1/8], Step [47/179], Loss: 1.4017\n",
      "Epoch [1/8], Step [48/179], Loss: 1.3747\n",
      "Epoch [1/8], Step [49/179], Loss: 1.3883\n",
      "Epoch [1/8], Step [50/179], Loss: 1.3744\n",
      "Epoch [1/8], Step [51/179], Loss: 1.3854\n",
      "Epoch [1/8], Step [52/179], Loss: 1.4354\n",
      "Epoch [1/8], Step [53/179], Loss: 1.3770\n",
      "Epoch [1/8], Step [54/179], Loss: 1.4368\n",
      "Epoch [1/8], Step [55/179], Loss: 1.3382\n",
      "Epoch [1/8], Step [56/179], Loss: 1.4203\n",
      "Epoch [1/8], Step [57/179], Loss: 1.4623\n",
      "Epoch [1/8], Step [58/179], Loss: 1.4223\n",
      "Epoch [1/8], Step [59/179], Loss: 1.4046\n",
      "Epoch [1/8], Step [60/179], Loss: 1.3890\n",
      "Epoch [1/8], Step [61/179], Loss: 1.4173\n",
      "Epoch [1/8], Step [62/179], Loss: 1.4533\n",
      "Epoch [1/8], Step [63/179], Loss: 1.4036\n",
      "Epoch [1/8], Step [64/179], Loss: 1.4169\n",
      "Epoch [1/8], Step [65/179], Loss: 1.4483\n",
      "Epoch [1/8], Step [66/179], Loss: 1.5204\n",
      "Epoch [1/8], Step [67/179], Loss: 1.4023\n",
      "Epoch [1/8], Step [68/179], Loss: 1.2868\n",
      "Epoch [1/8], Step [69/179], Loss: 1.3447\n",
      "Epoch [1/8], Step [70/179], Loss: 1.4185\n",
      "Epoch [1/8], Step [71/179], Loss: 1.3560\n",
      "Epoch [1/8], Step [72/179], Loss: 1.4395\n",
      "Epoch [1/8], Step [73/179], Loss: 1.3978\n",
      "Epoch [1/8], Step [74/179], Loss: 1.3820\n",
      "Epoch [1/8], Step [75/179], Loss: 1.5322\n",
      "Epoch [1/8], Step [76/179], Loss: 1.5657\n",
      "Epoch [1/8], Step [77/179], Loss: 1.3731\n",
      "Epoch [1/8], Step [78/179], Loss: 1.3743\n",
      "Epoch [1/8], Step [79/179], Loss: 1.3771\n",
      "Epoch [1/8], Step [80/179], Loss: 1.3840\n",
      "Epoch [1/8], Step [81/179], Loss: 1.3832\n",
      "Epoch [1/8], Step [82/179], Loss: 1.4488\n",
      "Epoch [1/8], Step [83/179], Loss: 1.3491\n",
      "Epoch [1/8], Step [84/179], Loss: 1.3166\n",
      "Epoch [1/8], Step [85/179], Loss: 1.4134\n",
      "Epoch [1/8], Step [86/179], Loss: 1.3583\n",
      "Epoch [1/8], Step [87/179], Loss: 1.4393\n",
      "Epoch [1/8], Step [88/179], Loss: 1.4345\n",
      "Epoch [1/8], Step [89/179], Loss: 1.4016\n",
      "Epoch [1/8], Step [90/179], Loss: 1.3630\n",
      "Epoch [1/8], Step [91/179], Loss: 1.4198\n",
      "Epoch [1/8], Step [92/179], Loss: 1.3736\n",
      "Epoch [1/8], Step [93/179], Loss: 1.3819\n",
      "Epoch [1/8], Step [94/179], Loss: 1.3720\n",
      "Epoch [1/8], Step [95/179], Loss: 1.3514\n",
      "Epoch [1/8], Step [96/179], Loss: 1.4415\n",
      "Epoch [1/8], Step [97/179], Loss: 1.3957\n",
      "Epoch [1/8], Step [98/179], Loss: 1.3815\n",
      "Epoch [1/8], Step [99/179], Loss: 1.3643\n",
      "Epoch [1/8], Step [100/179], Loss: 1.3867\n",
      "Epoch [1/8], Step [101/179], Loss: 1.3417\n",
      "Epoch [1/8], Step [102/179], Loss: 1.3961\n",
      "Epoch [1/8], Step [103/179], Loss: 1.3161\n",
      "Epoch [1/8], Step [104/179], Loss: 1.4258\n",
      "Epoch [1/8], Step [105/179], Loss: 1.4932\n",
      "Epoch [1/8], Step [106/179], Loss: 1.5670\n",
      "Epoch [1/8], Step [107/179], Loss: 1.4686\n",
      "Epoch [1/8], Step [108/179], Loss: 1.5327\n",
      "Epoch [1/8], Step [109/179], Loss: 1.3930\n",
      "Epoch [1/8], Step [110/179], Loss: 1.4065\n",
      "Epoch [1/8], Step [111/179], Loss: 1.3631\n",
      "Epoch [1/8], Step [112/179], Loss: 1.3871\n",
      "Epoch [1/8], Step [113/179], Loss: 1.4261\n",
      "Epoch [1/8], Step [114/179], Loss: 1.4204\n",
      "Epoch [1/8], Step [115/179], Loss: 1.4450\n",
      "Epoch [1/8], Step [116/179], Loss: 1.4664\n",
      "Epoch [1/8], Step [117/179], Loss: 1.3689\n",
      "Epoch [1/8], Step [118/179], Loss: 1.3929\n",
      "Epoch [1/8], Step [119/179], Loss: 1.4645\n",
      "Epoch [1/8], Step [120/179], Loss: 1.3482\n",
      "Epoch [1/8], Step [121/179], Loss: 1.4222\n",
      "Epoch [1/8], Step [122/179], Loss: 1.3974\n",
      "Epoch [1/8], Step [123/179], Loss: 1.4682\n",
      "Epoch [1/8], Step [124/179], Loss: 1.3548\n",
      "Epoch [1/8], Step [125/179], Loss: 1.4510\n",
      "Epoch [1/8], Step [126/179], Loss: 1.4078\n",
      "Epoch [1/8], Step [127/179], Loss: 1.3695\n",
      "Epoch [1/8], Step [128/179], Loss: 1.3668\n",
      "Epoch [1/8], Step [129/179], Loss: 1.4106\n",
      "Epoch [1/8], Step [130/179], Loss: 1.4174\n",
      "Epoch [1/8], Step [131/179], Loss: 1.3957\n",
      "Epoch [1/8], Step [132/179], Loss: 1.3867\n",
      "Epoch [1/8], Step [133/179], Loss: 1.3671\n",
      "Epoch [1/8], Step [134/179], Loss: 1.3958\n",
      "Epoch [1/8], Step [135/179], Loss: 1.3840\n",
      "Epoch [1/8], Step [136/179], Loss: 1.4094\n",
      "Epoch [1/8], Step [137/179], Loss: 1.3683\n",
      "Epoch [1/8], Step [138/179], Loss: 1.3906\n",
      "Epoch [1/8], Step [139/179], Loss: 1.3625\n",
      "Epoch [1/8], Step [140/179], Loss: 1.3804\n",
      "Epoch [1/8], Step [141/179], Loss: 1.3473\n",
      "Epoch [1/8], Step [142/179], Loss: 1.4146\n",
      "Epoch [1/8], Step [143/179], Loss: 1.4556\n",
      "Epoch [1/8], Step [144/179], Loss: 1.4828\n",
      "Epoch [1/8], Step [145/179], Loss: 1.4128\n",
      "Epoch [1/8], Step [146/179], Loss: 1.3863\n",
      "Epoch [1/8], Step [147/179], Loss: 1.3639\n",
      "Epoch [1/8], Step [148/179], Loss: 1.4022\n",
      "Epoch [1/8], Step [149/179], Loss: 1.4033\n",
      "Epoch [1/8], Step [150/179], Loss: 1.3173\n",
      "Epoch [1/8], Step [151/179], Loss: 1.4527\n",
      "Epoch [1/8], Step [152/179], Loss: 1.3684\n",
      "Epoch [1/8], Step [153/179], Loss: 1.4718\n",
      "Epoch [1/8], Step [154/179], Loss: 1.3388\n",
      "Epoch [1/8], Step [155/179], Loss: 1.3928\n",
      "Epoch [1/8], Step [156/179], Loss: 1.4468\n",
      "Epoch [1/8], Step [157/179], Loss: 1.4143\n",
      "Epoch [1/8], Step [158/179], Loss: 1.3979\n",
      "Epoch [1/8], Step [159/179], Loss: 1.3795\n",
      "Epoch [1/8], Step [160/179], Loss: 1.3997\n",
      "Epoch [1/8], Step [161/179], Loss: 1.3745\n",
      "Epoch [1/8], Step [162/179], Loss: 1.3849\n",
      "Epoch [1/8], Step [163/179], Loss: 1.3710\n",
      "Epoch [1/8], Step [164/179], Loss: 1.4435\n",
      "Epoch [1/8], Step [165/179], Loss: 1.4207\n",
      "Epoch [1/8], Step [166/179], Loss: 1.3992\n",
      "Epoch [1/8], Step [167/179], Loss: 1.3692\n",
      "Epoch [1/8], Step [168/179], Loss: 1.3606\n",
      "Epoch [1/8], Step [169/179], Loss: 1.4346\n",
      "Epoch [1/8], Step [170/179], Loss: 1.3986\n",
      "Epoch [1/8], Step [171/179], Loss: 1.4004\n",
      "Epoch [1/8], Step [172/179], Loss: 1.3827\n",
      "Epoch [1/8], Step [173/179], Loss: 1.4032\n",
      "Epoch [1/8], Step [174/179], Loss: 1.3556\n",
      "Epoch [1/8], Step [175/179], Loss: 1.3908\n",
      "Epoch [1/8], Step [176/179], Loss: 1.3703\n",
      "Epoch [1/8], Step [177/179], Loss: 1.3876\n",
      "Epoch [1/8], Step [178/179], Loss: 1.3955\n",
      "Epoch [1/8], Step [179/179], Loss: 1.3865\n",
      "Epoch [2/8], Step [1/179], Loss: 1.3841\n",
      "Epoch [2/8], Step [2/179], Loss: 1.3960\n",
      "Epoch [2/8], Step [3/179], Loss: 1.3639\n",
      "Epoch [2/8], Step [4/179], Loss: 1.3515\n",
      "Epoch [2/8], Step [5/179], Loss: 1.4444\n",
      "Epoch [2/8], Step [6/179], Loss: 1.3463\n",
      "Epoch [2/8], Step [7/179], Loss: 1.3641\n",
      "Epoch [2/8], Step [8/179], Loss: 1.4125\n",
      "Epoch [2/8], Step [9/179], Loss: 1.3996\n",
      "Epoch [2/8], Step [10/179], Loss: 1.3990\n",
      "Epoch [2/8], Step [11/179], Loss: 1.3329\n",
      "Epoch [2/8], Step [12/179], Loss: 1.3690\n",
      "Epoch [2/8], Step [13/179], Loss: 1.3868\n",
      "Epoch [2/8], Step [14/179], Loss: 1.4617\n",
      "Epoch [2/8], Step [15/179], Loss: 1.4670\n",
      "Epoch [2/8], Step [16/179], Loss: 1.3300\n",
      "Epoch [2/8], Step [17/179], Loss: 1.4267\n",
      "Epoch [2/8], Step [18/179], Loss: 1.3444\n",
      "Epoch [2/8], Step [19/179], Loss: 1.4366\n",
      "Epoch [2/8], Step [20/179], Loss: 1.3357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [21/179], Loss: 1.4000\n",
      "Epoch [2/8], Step [22/179], Loss: 1.3666\n",
      "Epoch [2/8], Step [23/179], Loss: 1.3639\n",
      "Epoch [2/8], Step [24/179], Loss: 1.3843\n",
      "Epoch [2/8], Step [25/179], Loss: 1.4167\n",
      "Epoch [2/8], Step [26/179], Loss: 1.3163\n",
      "Epoch [2/8], Step [27/179], Loss: 1.3821\n",
      "Epoch [2/8], Step [28/179], Loss: 1.4400\n",
      "Epoch [2/8], Step [29/179], Loss: 1.3393\n",
      "Epoch [2/8], Step [30/179], Loss: 1.3481\n",
      "Epoch [2/8], Step [31/179], Loss: 1.3897\n",
      "Epoch [2/8], Step [32/179], Loss: 1.4069\n",
      "Epoch [2/8], Step [33/179], Loss: 1.3863\n",
      "Epoch [2/8], Step [34/179], Loss: 1.3822\n",
      "Epoch [2/8], Step [35/179], Loss: 1.3914\n",
      "Epoch [2/8], Step [36/179], Loss: 1.3849\n",
      "Epoch [2/8], Step [37/179], Loss: 1.3626\n",
      "Epoch [2/8], Step [38/179], Loss: 1.4094\n",
      "Epoch [2/8], Step [39/179], Loss: 1.3635\n",
      "Epoch [2/8], Step [40/179], Loss: 1.3567\n",
      "Epoch [2/8], Step [41/179], Loss: 1.3879\n",
      "Epoch [2/8], Step [42/179], Loss: 1.3994\n",
      "Epoch [2/8], Step [43/179], Loss: 1.4067\n",
      "Epoch [2/8], Step [44/179], Loss: 1.3637\n",
      "Epoch [2/8], Step [45/179], Loss: 1.3380\n",
      "Epoch [2/8], Step [46/179], Loss: 1.3590\n",
      "Epoch [2/8], Step [47/179], Loss: 1.3255\n",
      "Epoch [2/8], Step [48/179], Loss: 1.3660\n",
      "Epoch [2/8], Step [49/179], Loss: 1.3537\n",
      "Epoch [2/8], Step [50/179], Loss: 1.3446\n",
      "Epoch [2/8], Step [51/179], Loss: 1.4637\n",
      "Epoch [2/8], Step [52/179], Loss: 1.3747\n",
      "Epoch [2/8], Step [53/179], Loss: 1.4398\n",
      "Epoch [2/8], Step [54/179], Loss: 1.3926\n",
      "Epoch [2/8], Step [55/179], Loss: 1.3407\n",
      "Epoch [2/8], Step [56/179], Loss: 1.3212\n",
      "Epoch [2/8], Step [57/179], Loss: 1.3734\n",
      "Epoch [2/8], Step [58/179], Loss: 1.3397\n",
      "Epoch [2/8], Step [59/179], Loss: 1.3775\n",
      "Epoch [2/8], Step [60/179], Loss: 1.3416\n",
      "Epoch [2/8], Step [61/179], Loss: 1.3448\n",
      "Epoch [2/8], Step [62/179], Loss: 1.3370\n",
      "Epoch [2/8], Step [63/179], Loss: 1.3592\n",
      "Epoch [2/8], Step [64/179], Loss: 1.3939\n",
      "Epoch [2/8], Step [65/179], Loss: 1.3655\n",
      "Epoch [2/8], Step [66/179], Loss: 1.3488\n",
      "Epoch [2/8], Step [67/179], Loss: 1.3860\n",
      "Epoch [2/8], Step [68/179], Loss: 1.3035\n",
      "Epoch [2/8], Step [69/179], Loss: 1.3422\n",
      "Epoch [2/8], Step [70/179], Loss: 1.3525\n",
      "Epoch [2/8], Step [71/179], Loss: 1.3700\n",
      "Epoch [2/8], Step [72/179], Loss: 1.3305\n",
      "Epoch [2/8], Step [73/179], Loss: 1.3158\n",
      "Epoch [2/8], Step [74/179], Loss: 1.2991\n",
      "Epoch [2/8], Step [75/179], Loss: 1.3315\n",
      "Epoch [2/8], Step [76/179], Loss: 1.3259\n",
      "Epoch [2/8], Step [77/179], Loss: 1.3766\n",
      "Epoch [2/8], Step [78/179], Loss: 1.3178\n",
      "Epoch [2/8], Step [79/179], Loss: 1.3009\n",
      "Epoch [2/8], Step [80/179], Loss: 1.3053\n",
      "Epoch [2/8], Step [81/179], Loss: 1.3573\n",
      "Epoch [2/8], Step [82/179], Loss: 1.2978\n",
      "Epoch [2/8], Step [83/179], Loss: 1.3442\n",
      "Epoch [2/8], Step [84/179], Loss: 1.3300\n",
      "Epoch [2/8], Step [85/179], Loss: 1.3185\n",
      "Epoch [2/8], Step [86/179], Loss: 1.3031\n",
      "Epoch [2/8], Step [87/179], Loss: 1.2931\n",
      "Epoch [2/8], Step [88/179], Loss: 1.3255\n",
      "Epoch [2/8], Step [89/179], Loss: 1.2892\n",
      "Epoch [2/8], Step [90/179], Loss: 1.2732\n",
      "Epoch [2/8], Step [91/179], Loss: 1.3214\n",
      "Epoch [2/8], Step [92/179], Loss: 1.3624\n",
      "Epoch [2/8], Step [93/179], Loss: 1.2716\n",
      "Epoch [2/8], Step [94/179], Loss: 1.3027\n",
      "Epoch [2/8], Step [95/179], Loss: 1.2372\n",
      "Epoch [2/8], Step [96/179], Loss: 1.3169\n",
      "Epoch [2/8], Step [97/179], Loss: 1.1318\n",
      "Epoch [2/8], Step [98/179], Loss: 1.2580\n",
      "Epoch [2/8], Step [99/179], Loss: 1.2214\n",
      "Epoch [2/8], Step [100/179], Loss: 1.3060\n",
      "Epoch [2/8], Step [101/179], Loss: 1.2780\n",
      "Epoch [2/8], Step [102/179], Loss: 1.2675\n",
      "Epoch [2/8], Step [103/179], Loss: 1.2277\n",
      "Epoch [2/8], Step [104/179], Loss: 1.3778\n",
      "Epoch [2/8], Step [105/179], Loss: 1.2244\n",
      "Epoch [2/8], Step [106/179], Loss: 1.2496\n",
      "Epoch [2/8], Step [107/179], Loss: 1.2319\n",
      "Epoch [2/8], Step [108/179], Loss: 1.2531\n",
      "Epoch [2/8], Step [109/179], Loss: 1.2491\n",
      "Epoch [2/8], Step [110/179], Loss: 1.2593\n",
      "Epoch [2/8], Step [111/179], Loss: 1.2188\n",
      "Epoch [2/8], Step [112/179], Loss: 1.2194\n",
      "Epoch [2/8], Step [113/179], Loss: 1.2063\n",
      "Epoch [2/8], Step [114/179], Loss: 1.1923\n",
      "Epoch [2/8], Step [115/179], Loss: 1.2318\n",
      "Epoch [2/8], Step [116/179], Loss: 1.1439\n",
      "Epoch [2/8], Step [117/179], Loss: 1.1431\n",
      "Epoch [2/8], Step [118/179], Loss: 1.1746\n",
      "Epoch [2/8], Step [119/179], Loss: 1.0462\n",
      "Epoch [2/8], Step [120/179], Loss: 1.2242\n",
      "Epoch [2/8], Step [121/179], Loss: 1.1305\n",
      "Epoch [2/8], Step [122/179], Loss: 1.2667\n",
      "Epoch [2/8], Step [123/179], Loss: 1.1672\n",
      "Epoch [2/8], Step [124/179], Loss: 1.1391\n",
      "Epoch [2/8], Step [125/179], Loss: 1.1271\n",
      "Epoch [2/8], Step [126/179], Loss: 1.1044\n",
      "Epoch [2/8], Step [127/179], Loss: 1.1850\n",
      "Epoch [2/8], Step [128/179], Loss: 1.2364\n",
      "Epoch [2/8], Step [129/179], Loss: 1.1275\n",
      "Epoch [2/8], Step [130/179], Loss: 1.1187\n",
      "Epoch [2/8], Step [131/179], Loss: 0.9770\n",
      "Epoch [2/8], Step [132/179], Loss: 0.9407\n",
      "Epoch [2/8], Step [133/179], Loss: 0.9968\n",
      "Epoch [2/8], Step [134/179], Loss: 1.0981\n",
      "Epoch [2/8], Step [135/179], Loss: 1.0112\n",
      "Epoch [2/8], Step [136/179], Loss: 1.2956\n",
      "Epoch [2/8], Step [137/179], Loss: 1.0625\n",
      "Epoch [2/8], Step [138/179], Loss: 1.0967\n",
      "Epoch [2/8], Step [139/179], Loss: 1.0403\n",
      "Epoch [2/8], Step [140/179], Loss: 1.1448\n",
      "Epoch [2/8], Step [141/179], Loss: 1.1608\n",
      "Epoch [2/8], Step [142/179], Loss: 1.0841\n",
      "Epoch [2/8], Step [143/179], Loss: 1.0435\n",
      "Epoch [2/8], Step [144/179], Loss: 1.1326\n",
      "Epoch [2/8], Step [145/179], Loss: 1.0546\n",
      "Epoch [2/8], Step [146/179], Loss: 0.8402\n",
      "Epoch [2/8], Step [147/179], Loss: 1.0981\n",
      "Epoch [2/8], Step [148/179], Loss: 0.8657\n",
      "Epoch [2/8], Step [149/179], Loss: 0.8274\n",
      "Epoch [2/8], Step [150/179], Loss: 0.9380\n",
      "Epoch [2/8], Step [151/179], Loss: 0.8415\n",
      "Epoch [2/8], Step [152/179], Loss: 0.9554\n",
      "Epoch [2/8], Step [153/179], Loss: 1.0261\n",
      "Epoch [2/8], Step [154/179], Loss: 0.9350\n",
      "Epoch [2/8], Step [155/179], Loss: 0.9040\n",
      "Epoch [2/8], Step [156/179], Loss: 0.9317\n",
      "Epoch [2/8], Step [157/179], Loss: 0.9259\n",
      "Epoch [2/8], Step [158/179], Loss: 0.8350\n",
      "Epoch [2/8], Step [159/179], Loss: 1.0956\n",
      "Epoch [2/8], Step [160/179], Loss: 0.7689\n",
      "Epoch [2/8], Step [161/179], Loss: 0.7020\n",
      "Epoch [2/8], Step [162/179], Loss: 0.7402\n",
      "Epoch [2/8], Step [163/179], Loss: 1.1094\n",
      "Epoch [2/8], Step [164/179], Loss: 0.8602\n",
      "Epoch [2/8], Step [165/179], Loss: 0.8428\n",
      "Epoch [2/8], Step [166/179], Loss: 0.9022\n",
      "Epoch [2/8], Step [167/179], Loss: 0.9037\n",
      "Epoch [2/8], Step [168/179], Loss: 0.6150\n",
      "Epoch [2/8], Step [169/179], Loss: 0.7639\n",
      "Epoch [2/8], Step [170/179], Loss: 0.8516\n",
      "Epoch [2/8], Step [171/179], Loss: 0.7532\n",
      "Epoch [2/8], Step [172/179], Loss: 1.0349\n",
      "Epoch [2/8], Step [173/179], Loss: 0.7603\n",
      "Epoch [2/8], Step [174/179], Loss: 0.8621\n",
      "Epoch [2/8], Step [175/179], Loss: 0.6173\n",
      "Epoch [2/8], Step [176/179], Loss: 1.0344\n",
      "Epoch [2/8], Step [177/179], Loss: 0.7419\n",
      "Epoch [2/8], Step [178/179], Loss: 1.0617\n",
      "Epoch [2/8], Step [179/179], Loss: 1.2910\n",
      "Epoch [3/8], Step [1/179], Loss: 0.7799\n",
      "Epoch [3/8], Step [2/179], Loss: 1.0474\n",
      "Epoch [3/8], Step [3/179], Loss: 0.7664\n",
      "Epoch [3/8], Step [4/179], Loss: 0.9639\n",
      "Epoch [3/8], Step [5/179], Loss: 0.7085\n",
      "Epoch [3/8], Step [6/179], Loss: 0.6943\n",
      "Epoch [3/8], Step [7/179], Loss: 0.6624\n",
      "Epoch [3/8], Step [8/179], Loss: 0.6936\n",
      "Epoch [3/8], Step [9/179], Loss: 0.8847\n",
      "Epoch [3/8], Step [10/179], Loss: 0.6318\n",
      "Epoch [3/8], Step [11/179], Loss: 0.9835\n",
      "Epoch [3/8], Step [12/179], Loss: 0.6784\n",
      "Epoch [3/8], Step [13/179], Loss: 0.6216\n",
      "Epoch [3/8], Step [14/179], Loss: 0.8541\n",
      "Epoch [3/8], Step [15/179], Loss: 0.8403\n",
      "Epoch [3/8], Step [16/179], Loss: 0.7441\n",
      "Epoch [3/8], Step [17/179], Loss: 0.8332\n",
      "Epoch [3/8], Step [18/179], Loss: 0.6596\n",
      "Epoch [3/8], Step [19/179], Loss: 0.7737\n",
      "Epoch [3/8], Step [20/179], Loss: 0.8651\n",
      "Epoch [3/8], Step [21/179], Loss: 0.8016\n",
      "Epoch [3/8], Step [22/179], Loss: 0.7150\n",
      "Epoch [3/8], Step [23/179], Loss: 0.7669\n",
      "Epoch [3/8], Step [24/179], Loss: 0.9841\n",
      "Epoch [3/8], Step [25/179], Loss: 0.8564\n",
      "Epoch [3/8], Step [26/179], Loss: 0.8121\n",
      "Epoch [3/8], Step [27/179], Loss: 0.9319\n",
      "Epoch [3/8], Step [28/179], Loss: 0.7606\n",
      "Epoch [3/8], Step [29/179], Loss: 0.6550\n",
      "Epoch [3/8], Step [30/179], Loss: 0.7058\n",
      "Epoch [3/8], Step [31/179], Loss: 0.8851\n",
      "Epoch [3/8], Step [32/179], Loss: 0.7237\n",
      "Epoch [3/8], Step [33/179], Loss: 0.7164\n",
      "Epoch [3/8], Step [34/179], Loss: 0.9031\n",
      "Epoch [3/8], Step [35/179], Loss: 0.6417\n",
      "Epoch [3/8], Step [36/179], Loss: 0.9880\n",
      "Epoch [3/8], Step [37/179], Loss: 0.6634\n",
      "Epoch [3/8], Step [38/179], Loss: 0.7193\n",
      "Epoch [3/8], Step [39/179], Loss: 0.7422\n",
      "Epoch [3/8], Step [40/179], Loss: 0.9930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [41/179], Loss: 0.6888\n",
      "Epoch [3/8], Step [42/179], Loss: 0.7696\n",
      "Epoch [3/8], Step [43/179], Loss: 1.3023\n",
      "Epoch [3/8], Step [44/179], Loss: 0.6724\n",
      "Epoch [3/8], Step [45/179], Loss: 0.5933\n",
      "Epoch [3/8], Step [46/179], Loss: 0.4899\n",
      "Epoch [3/8], Step [47/179], Loss: 0.5249\n",
      "Epoch [3/8], Step [48/179], Loss: 0.7531\n",
      "Epoch [3/8], Step [49/179], Loss: 0.5898\n",
      "Epoch [3/8], Step [50/179], Loss: 0.6318\n",
      "Epoch [3/8], Step [51/179], Loss: 0.8039\n",
      "Epoch [3/8], Step [52/179], Loss: 0.5610\n",
      "Epoch [3/8], Step [53/179], Loss: 0.7672\n",
      "Epoch [3/8], Step [54/179], Loss: 0.7743\n",
      "Epoch [3/8], Step [55/179], Loss: 0.7807\n",
      "Epoch [3/8], Step [56/179], Loss: 0.7059\n",
      "Epoch [3/8], Step [57/179], Loss: 0.7013\n",
      "Epoch [3/8], Step [58/179], Loss: 0.7392\n",
      "Epoch [3/8], Step [59/179], Loss: 0.6068\n",
      "Epoch [3/8], Step [60/179], Loss: 0.7886\n",
      "Epoch [3/8], Step [61/179], Loss: 0.7837\n",
      "Epoch [3/8], Step [62/179], Loss: 0.6690\n",
      "Epoch [3/8], Step [63/179], Loss: 0.7190\n",
      "Epoch [3/8], Step [64/179], Loss: 0.7205\n",
      "Epoch [3/8], Step [65/179], Loss: 1.2433\n",
      "Epoch [3/8], Step [66/179], Loss: 0.7220\n",
      "Epoch [3/8], Step [67/179], Loss: 0.7679\n",
      "Epoch [3/8], Step [68/179], Loss: 1.0008\n",
      "Epoch [3/8], Step [69/179], Loss: 0.8071\n",
      "Epoch [3/8], Step [70/179], Loss: 0.6021\n",
      "Epoch [3/8], Step [71/179], Loss: 0.4829\n",
      "Epoch [3/8], Step [72/179], Loss: 0.5701\n",
      "Epoch [3/8], Step [73/179], Loss: 0.6771\n",
      "Epoch [3/8], Step [74/179], Loss: 0.6553\n",
      "Epoch [3/8], Step [75/179], Loss: 0.7008\n",
      "Epoch [3/8], Step [76/179], Loss: 0.7876\n",
      "Epoch [3/8], Step [77/179], Loss: 0.6494\n",
      "Epoch [3/8], Step [78/179], Loss: 0.5986\n",
      "Epoch [3/8], Step [79/179], Loss: 0.8033\n",
      "Epoch [3/8], Step [80/179], Loss: 0.6250\n",
      "Epoch [3/8], Step [81/179], Loss: 1.0004\n",
      "Epoch [3/8], Step [82/179], Loss: 0.7778\n",
      "Epoch [3/8], Step [83/179], Loss: 0.7514\n",
      "Epoch [3/8], Step [84/179], Loss: 0.8815\n",
      "Epoch [3/8], Step [85/179], Loss: 0.7148\n",
      "Epoch [3/8], Step [86/179], Loss: 0.8279\n",
      "Epoch [3/8], Step [87/179], Loss: 0.7395\n",
      "Epoch [3/8], Step [88/179], Loss: 0.6454\n",
      "Epoch [3/8], Step [89/179], Loss: 0.7680\n",
      "Epoch [3/8], Step [90/179], Loss: 0.9815\n",
      "Epoch [3/8], Step [91/179], Loss: 0.7077\n",
      "Epoch [3/8], Step [92/179], Loss: 0.5732\n",
      "Epoch [3/8], Step [93/179], Loss: 0.9730\n",
      "Epoch [3/8], Step [94/179], Loss: 0.8338\n",
      "Epoch [3/8], Step [95/179], Loss: 0.6083\n",
      "Epoch [3/8], Step [96/179], Loss: 0.9932\n",
      "Epoch [3/8], Step [97/179], Loss: 0.5570\n",
      "Epoch [3/8], Step [98/179], Loss: 0.6336\n",
      "Epoch [3/8], Step [99/179], Loss: 1.0661\n",
      "Epoch [3/8], Step [100/179], Loss: 0.6747\n",
      "Epoch [3/8], Step [101/179], Loss: 0.8587\n",
      "Epoch [3/8], Step [102/179], Loss: 0.8072\n",
      "Epoch [3/8], Step [103/179], Loss: 0.7316\n",
      "Epoch [3/8], Step [104/179], Loss: 0.8296\n",
      "Epoch [3/8], Step [105/179], Loss: 0.8575\n",
      "Epoch [3/8], Step [106/179], Loss: 0.7454\n",
      "Epoch [3/8], Step [107/179], Loss: 0.4402\n",
      "Epoch [3/8], Step [108/179], Loss: 0.6195\n",
      "Epoch [3/8], Step [109/179], Loss: 0.4260\n",
      "Epoch [3/8], Step [110/179], Loss: 0.6174\n",
      "Epoch [3/8], Step [111/179], Loss: 0.7536\n",
      "Epoch [3/8], Step [112/179], Loss: 0.5931\n",
      "Epoch [3/8], Step [113/179], Loss: 0.7450\n",
      "Epoch [3/8], Step [114/179], Loss: 0.8467\n",
      "Epoch [3/8], Step [115/179], Loss: 0.6036\n",
      "Epoch [3/8], Step [116/179], Loss: 0.9930\n",
      "Epoch [3/8], Step [117/179], Loss: 0.5913\n",
      "Epoch [3/8], Step [118/179], Loss: 0.5742\n",
      "Epoch [3/8], Step [119/179], Loss: 0.5989\n",
      "Epoch [3/8], Step [120/179], Loss: 0.6452\n",
      "Epoch [3/8], Step [121/179], Loss: 0.6736\n",
      "Epoch [3/8], Step [122/179], Loss: 0.6736\n",
      "Epoch [3/8], Step [123/179], Loss: 0.5006\n",
      "Epoch [3/8], Step [124/179], Loss: 0.6368\n",
      "Epoch [3/8], Step [125/179], Loss: 0.8012\n",
      "Epoch [3/8], Step [126/179], Loss: 0.5865\n",
      "Epoch [3/8], Step [127/179], Loss: 0.5768\n",
      "Epoch [3/8], Step [128/179], Loss: 0.5094\n",
      "Epoch [3/8], Step [129/179], Loss: 0.5408\n",
      "Epoch [3/8], Step [130/179], Loss: 0.4978\n",
      "Epoch [3/8], Step [131/179], Loss: 0.7559\n",
      "Epoch [3/8], Step [132/179], Loss: 0.7493\n",
      "Epoch [3/8], Step [133/179], Loss: 0.6313\n",
      "Epoch [3/8], Step [134/179], Loss: 0.7942\n",
      "Epoch [3/8], Step [135/179], Loss: 0.8717\n",
      "Epoch [3/8], Step [136/179], Loss: 0.6191\n",
      "Epoch [3/8], Step [137/179], Loss: 0.9666\n",
      "Epoch [3/8], Step [138/179], Loss: 0.5321\n",
      "Epoch [3/8], Step [139/179], Loss: 0.5077\n",
      "Epoch [3/8], Step [140/179], Loss: 0.5463\n",
      "Epoch [3/8], Step [141/179], Loss: 0.6002\n",
      "Epoch [3/8], Step [142/179], Loss: 0.7518\n",
      "Epoch [3/8], Step [143/179], Loss: 0.7377\n",
      "Epoch [3/8], Step [144/179], Loss: 0.9651\n",
      "Epoch [3/8], Step [145/179], Loss: 0.5388\n",
      "Epoch [3/8], Step [146/179], Loss: 0.4515\n",
      "Epoch [3/8], Step [147/179], Loss: 0.4906\n",
      "Epoch [3/8], Step [148/179], Loss: 0.6164\n",
      "Epoch [3/8], Step [149/179], Loss: 0.5744\n",
      "Epoch [3/8], Step [150/179], Loss: 0.4277\n",
      "Epoch [3/8], Step [151/179], Loss: 0.4402\n",
      "Epoch [3/8], Step [152/179], Loss: 0.7892\n",
      "Epoch [3/8], Step [153/179], Loss: 0.5491\n",
      "Epoch [3/8], Step [154/179], Loss: 0.4823\n",
      "Epoch [3/8], Step [155/179], Loss: 0.8389\n",
      "Epoch [3/8], Step [156/179], Loss: 0.6154\n",
      "Epoch [3/8], Step [157/179], Loss: 0.4840\n",
      "Epoch [3/8], Step [158/179], Loss: 0.5110\n",
      "Epoch [3/8], Step [159/179], Loss: 0.4270\n",
      "Epoch [3/8], Step [160/179], Loss: 0.9171\n",
      "Epoch [3/8], Step [161/179], Loss: 0.8672\n",
      "Epoch [3/8], Step [162/179], Loss: 0.3617\n",
      "Epoch [3/8], Step [163/179], Loss: 0.5867\n",
      "Epoch [3/8], Step [164/179], Loss: 0.5917\n",
      "Epoch [3/8], Step [165/179], Loss: 0.8922\n",
      "Epoch [3/8], Step [166/179], Loss: 0.6739\n",
      "Epoch [3/8], Step [167/179], Loss: 0.6410\n",
      "Epoch [3/8], Step [168/179], Loss: 0.6336\n",
      "Epoch [3/8], Step [169/179], Loss: 0.6820\n",
      "Epoch [3/8], Step [170/179], Loss: 0.8060\n",
      "Epoch [3/8], Step [171/179], Loss: 0.5380\n",
      "Epoch [3/8], Step [172/179], Loss: 0.5436\n",
      "Epoch [3/8], Step [173/179], Loss: 0.6732\n",
      "Epoch [3/8], Step [174/179], Loss: 0.4577\n",
      "Epoch [3/8], Step [175/179], Loss: 0.7422\n",
      "Epoch [3/8], Step [176/179], Loss: 0.4477\n",
      "Epoch [3/8], Step [177/179], Loss: 0.7668\n",
      "Epoch [3/8], Step [178/179], Loss: 0.5682\n",
      "Epoch [3/8], Step [179/179], Loss: 0.9782\n",
      "Epoch [4/8], Step [1/179], Loss: 0.7001\n",
      "Epoch [4/8], Step [2/179], Loss: 0.9206\n",
      "Epoch [4/8], Step [3/179], Loss: 0.6001\n",
      "Epoch [4/8], Step [4/179], Loss: 0.6049\n",
      "Epoch [4/8], Step [5/179], Loss: 0.5554\n",
      "Epoch [4/8], Step [6/179], Loss: 0.7439\n",
      "Epoch [4/8], Step [7/179], Loss: 0.5591\n",
      "Epoch [4/8], Step [8/179], Loss: 0.9407\n",
      "Epoch [4/8], Step [9/179], Loss: 0.6602\n",
      "Epoch [4/8], Step [10/179], Loss: 0.3827\n",
      "Epoch [4/8], Step [11/179], Loss: 0.6296\n",
      "Epoch [4/8], Step [12/179], Loss: 0.6309\n",
      "Epoch [4/8], Step [13/179], Loss: 0.4011\n",
      "Epoch [4/8], Step [14/179], Loss: 0.6101\n",
      "Epoch [4/8], Step [15/179], Loss: 0.3588\n",
      "Epoch [4/8], Step [16/179], Loss: 0.6115\n",
      "Epoch [4/8], Step [17/179], Loss: 0.5893\n",
      "Epoch [4/8], Step [18/179], Loss: 0.5910\n",
      "Epoch [4/8], Step [19/179], Loss: 0.3954\n",
      "Epoch [4/8], Step [20/179], Loss: 0.6481\n",
      "Epoch [4/8], Step [21/179], Loss: 0.5022\n",
      "Epoch [4/8], Step [22/179], Loss: 0.5613\n",
      "Epoch [4/8], Step [23/179], Loss: 0.5282\n",
      "Epoch [4/8], Step [24/179], Loss: 0.5520\n",
      "Epoch [4/8], Step [25/179], Loss: 0.5366\n",
      "Epoch [4/8], Step [26/179], Loss: 0.5631\n",
      "Epoch [4/8], Step [27/179], Loss: 0.7053\n",
      "Epoch [4/8], Step [28/179], Loss: 0.4050\n",
      "Epoch [4/8], Step [29/179], Loss: 0.5839\n",
      "Epoch [4/8], Step [30/179], Loss: 0.7053\n",
      "Epoch [4/8], Step [31/179], Loss: 0.4971\n",
      "Epoch [4/8], Step [32/179], Loss: 0.6143\n",
      "Epoch [4/8], Step [33/179], Loss: 0.5838\n",
      "Epoch [4/8], Step [34/179], Loss: 0.6400\n",
      "Epoch [4/8], Step [35/179], Loss: 0.4910\n",
      "Epoch [4/8], Step [36/179], Loss: 0.6608\n",
      "Epoch [4/8], Step [37/179], Loss: 0.5547\n",
      "Epoch [4/8], Step [38/179], Loss: 0.7735\n",
      "Epoch [4/8], Step [39/179], Loss: 0.5284\n",
      "Epoch [4/8], Step [40/179], Loss: 0.3714\n",
      "Epoch [4/8], Step [41/179], Loss: 0.7274\n",
      "Epoch [4/8], Step [42/179], Loss: 0.6040\n",
      "Epoch [4/8], Step [43/179], Loss: 0.6698\n",
      "Epoch [4/8], Step [44/179], Loss: 0.5478\n",
      "Epoch [4/8], Step [45/179], Loss: 0.4840\n",
      "Epoch [4/8], Step [46/179], Loss: 0.5707\n",
      "Epoch [4/8], Step [47/179], Loss: 0.6090\n",
      "Epoch [4/8], Step [48/179], Loss: 0.7743\n",
      "Epoch [4/8], Step [49/179], Loss: 0.6070\n",
      "Epoch [4/8], Step [50/179], Loss: 0.7191\n",
      "Epoch [4/8], Step [51/179], Loss: 0.3459\n",
      "Epoch [4/8], Step [52/179], Loss: 0.6744\n",
      "Epoch [4/8], Step [53/179], Loss: 0.6616\n",
      "Epoch [4/8], Step [54/179], Loss: 0.4677\n",
      "Epoch [4/8], Step [55/179], Loss: 0.4381\n",
      "Epoch [4/8], Step [56/179], Loss: 0.5561\n",
      "Epoch [4/8], Step [57/179], Loss: 0.5364\n",
      "Epoch [4/8], Step [58/179], Loss: 0.4758\n",
      "Epoch [4/8], Step [59/179], Loss: 0.6745\n",
      "Epoch [4/8], Step [60/179], Loss: 0.4590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [61/179], Loss: 0.4746\n",
      "Epoch [4/8], Step [62/179], Loss: 0.3324\n",
      "Epoch [4/8], Step [63/179], Loss: 0.5115\n",
      "Epoch [4/8], Step [64/179], Loss: 0.4944\n",
      "Epoch [4/8], Step [65/179], Loss: 0.4838\n",
      "Epoch [4/8], Step [66/179], Loss: 0.7562\n",
      "Epoch [4/8], Step [67/179], Loss: 0.6181\n",
      "Epoch [4/8], Step [68/179], Loss: 0.7856\n",
      "Epoch [4/8], Step [69/179], Loss: 0.5575\n",
      "Epoch [4/8], Step [70/179], Loss: 0.4656\n",
      "Epoch [4/8], Step [71/179], Loss: 0.2388\n",
      "Epoch [4/8], Step [72/179], Loss: 0.3981\n",
      "Epoch [4/8], Step [73/179], Loss: 0.3771\n",
      "Epoch [4/8], Step [74/179], Loss: 0.6077\n",
      "Epoch [4/8], Step [75/179], Loss: 0.4756\n",
      "Epoch [4/8], Step [76/179], Loss: 0.4854\n",
      "Epoch [4/8], Step [77/179], Loss: 0.6545\n",
      "Epoch [4/8], Step [78/179], Loss: 0.7284\n",
      "Epoch [4/8], Step [79/179], Loss: 0.7410\n",
      "Epoch [4/8], Step [80/179], Loss: 0.3411\n",
      "Epoch [4/8], Step [81/179], Loss: 0.6344\n",
      "Epoch [4/8], Step [82/179], Loss: 0.4573\n",
      "Epoch [4/8], Step [83/179], Loss: 0.5810\n",
      "Epoch [4/8], Step [84/179], Loss: 0.3884\n",
      "Epoch [4/8], Step [85/179], Loss: 0.5845\n",
      "Epoch [4/8], Step [86/179], Loss: 0.4517\n",
      "Epoch [4/8], Step [87/179], Loss: 0.5232\n",
      "Epoch [4/8], Step [88/179], Loss: 0.7931\n",
      "Epoch [4/8], Step [89/179], Loss: 0.8500\n",
      "Epoch [4/8], Step [90/179], Loss: 0.7008\n",
      "Epoch [4/8], Step [91/179], Loss: 0.5613\n",
      "Epoch [4/8], Step [92/179], Loss: 0.4495\n",
      "Epoch [4/8], Step [93/179], Loss: 0.3930\n",
      "Epoch [4/8], Step [94/179], Loss: 0.3661\n",
      "Epoch [4/8], Step [95/179], Loss: 0.4790\n",
      "Epoch [4/8], Step [96/179], Loss: 0.6139\n",
      "Epoch [4/8], Step [97/179], Loss: 0.6129\n",
      "Epoch [4/8], Step [98/179], Loss: 0.5230\n",
      "Epoch [4/8], Step [99/179], Loss: 0.4071\n",
      "Epoch [4/8], Step [100/179], Loss: 0.4713\n",
      "Epoch [4/8], Step [101/179], Loss: 0.7625\n",
      "Epoch [4/8], Step [102/179], Loss: 0.6349\n",
      "Epoch [4/8], Step [103/179], Loss: 0.4849\n",
      "Epoch [4/8], Step [104/179], Loss: 0.5895\n",
      "Epoch [4/8], Step [105/179], Loss: 0.2707\n",
      "Epoch [4/8], Step [106/179], Loss: 0.3674\n",
      "Epoch [4/8], Step [107/179], Loss: 0.7638\n",
      "Epoch [4/8], Step [108/179], Loss: 0.5551\n",
      "Epoch [4/8], Step [109/179], Loss: 0.6959\n",
      "Epoch [4/8], Step [110/179], Loss: 0.5454\n",
      "Epoch [4/8], Step [111/179], Loss: 0.3342\n",
      "Epoch [4/8], Step [112/179], Loss: 0.5501\n",
      "Epoch [4/8], Step [113/179], Loss: 0.7226\n",
      "Epoch [4/8], Step [114/179], Loss: 0.5480\n",
      "Epoch [4/8], Step [115/179], Loss: 0.4965\n",
      "Epoch [4/8], Step [116/179], Loss: 0.3376\n",
      "Epoch [4/8], Step [117/179], Loss: 0.5634\n",
      "Epoch [4/8], Step [118/179], Loss: 0.5063\n",
      "Epoch [4/8], Step [119/179], Loss: 0.7252\n",
      "Epoch [4/8], Step [120/179], Loss: 1.1780\n",
      "Epoch [4/8], Step [121/179], Loss: 0.5742\n",
      "Epoch [4/8], Step [122/179], Loss: 0.6356\n",
      "Epoch [4/8], Step [123/179], Loss: 0.6478\n",
      "Epoch [4/8], Step [124/179], Loss: 0.6062\n",
      "Epoch [4/8], Step [125/179], Loss: 0.5557\n",
      "Epoch [4/8], Step [126/179], Loss: 0.4197\n",
      "Epoch [4/8], Step [127/179], Loss: 0.7061\n",
      "Epoch [4/8], Step [128/179], Loss: 0.5898\n",
      "Epoch [4/8], Step [129/179], Loss: 0.5756\n",
      "Epoch [4/8], Step [130/179], Loss: 0.6611\n",
      "Epoch [4/8], Step [131/179], Loss: 0.6054\n",
      "Epoch [4/8], Step [132/179], Loss: 0.6974\n",
      "Epoch [4/8], Step [133/179], Loss: 0.4619\n",
      "Epoch [4/8], Step [134/179], Loss: 0.6849\n",
      "Epoch [4/8], Step [135/179], Loss: 0.5504\n",
      "Epoch [4/8], Step [136/179], Loss: 0.5674\n",
      "Epoch [4/8], Step [137/179], Loss: 0.6247\n",
      "Epoch [4/8], Step [138/179], Loss: 0.7144\n",
      "Epoch [4/8], Step [139/179], Loss: 0.4434\n",
      "Epoch [4/8], Step [140/179], Loss: 0.5380\n",
      "Epoch [4/8], Step [141/179], Loss: 0.6758\n",
      "Epoch [4/8], Step [142/179], Loss: 0.5239\n",
      "Epoch [4/8], Step [143/179], Loss: 0.6012\n",
      "Epoch [4/8], Step [144/179], Loss: 0.4403\n",
      "Epoch [4/8], Step [145/179], Loss: 0.3159\n",
      "Epoch [4/8], Step [146/179], Loss: 0.5580\n",
      "Epoch [4/8], Step [147/179], Loss: 0.5764\n",
      "Epoch [4/8], Step [148/179], Loss: 0.5975\n",
      "Epoch [4/8], Step [149/179], Loss: 0.5730\n",
      "Epoch [4/8], Step [150/179], Loss: 0.6920\n",
      "Epoch [4/8], Step [151/179], Loss: 0.7382\n",
      "Epoch [4/8], Step [152/179], Loss: 0.3881\n",
      "Epoch [4/8], Step [153/179], Loss: 0.4408\n",
      "Epoch [4/8], Step [154/179], Loss: 0.6937\n",
      "Epoch [4/8], Step [155/179], Loss: 0.5600\n",
      "Epoch [4/8], Step [156/179], Loss: 0.5127\n",
      "Epoch [4/8], Step [157/179], Loss: 0.3962\n",
      "Epoch [4/8], Step [158/179], Loss: 0.6863\n",
      "Epoch [4/8], Step [159/179], Loss: 0.5591\n",
      "Epoch [4/8], Step [160/179], Loss: 0.3590\n",
      "Epoch [4/8], Step [161/179], Loss: 0.4374\n",
      "Epoch [4/8], Step [162/179], Loss: 0.6431\n",
      "Epoch [4/8], Step [163/179], Loss: 0.5884\n",
      "Epoch [4/8], Step [164/179], Loss: 0.4191\n",
      "Epoch [4/8], Step [165/179], Loss: 0.6580\n",
      "Epoch [4/8], Step [166/179], Loss: 0.6045\n",
      "Epoch [4/8], Step [167/179], Loss: 0.4585\n",
      "Epoch [4/8], Step [168/179], Loss: 0.5729\n",
      "Epoch [4/8], Step [169/179], Loss: 0.5409\n",
      "Epoch [4/8], Step [170/179], Loss: 0.5742\n",
      "Epoch [4/8], Step [171/179], Loss: 0.4087\n",
      "Epoch [4/8], Step [172/179], Loss: 0.5425\n",
      "Epoch [4/8], Step [173/179], Loss: 0.5963\n",
      "Epoch [4/8], Step [174/179], Loss: 0.6491\n",
      "Epoch [4/8], Step [175/179], Loss: 0.4500\n",
      "Epoch [4/8], Step [176/179], Loss: 0.5430\n",
      "Epoch [4/8], Step [177/179], Loss: 0.6730\n",
      "Epoch [4/8], Step [178/179], Loss: 0.3475\n",
      "Epoch [4/8], Step [179/179], Loss: 1.1098\n",
      "Epoch [5/8], Step [1/179], Loss: 0.6929\n",
      "Epoch [5/8], Step [2/179], Loss: 0.5594\n",
      "Epoch [5/8], Step [3/179], Loss: 0.5901\n",
      "Epoch [5/8], Step [4/179], Loss: 0.7407\n",
      "Epoch [5/8], Step [5/179], Loss: 0.5849\n",
      "Epoch [5/8], Step [6/179], Loss: 0.5270\n",
      "Epoch [5/8], Step [7/179], Loss: 0.6226\n",
      "Epoch [5/8], Step [8/179], Loss: 0.4596\n",
      "Epoch [5/8], Step [9/179], Loss: 0.5209\n",
      "Epoch [5/8], Step [10/179], Loss: 0.2907\n",
      "Epoch [5/8], Step [11/179], Loss: 0.3261\n",
      "Epoch [5/8], Step [12/179], Loss: 0.4847\n",
      "Epoch [5/8], Step [13/179], Loss: 0.2879\n",
      "Epoch [5/8], Step [14/179], Loss: 0.5907\n",
      "Epoch [5/8], Step [15/179], Loss: 0.5814\n",
      "Epoch [5/8], Step [16/179], Loss: 0.5220\n",
      "Epoch [5/8], Step [17/179], Loss: 0.5371\n",
      "Epoch [5/8], Step [18/179], Loss: 0.3996\n",
      "Epoch [5/8], Step [19/179], Loss: 0.2881\n",
      "Epoch [5/8], Step [20/179], Loss: 0.5449\n",
      "Epoch [5/8], Step [21/179], Loss: 0.6169\n",
      "Epoch [5/8], Step [22/179], Loss: 0.6145\n",
      "Epoch [5/8], Step [23/179], Loss: 0.4460\n",
      "Epoch [5/8], Step [24/179], Loss: 0.4480\n",
      "Epoch [5/8], Step [25/179], Loss: 0.3876\n",
      "Epoch [5/8], Step [26/179], Loss: 0.4432\n",
      "Epoch [5/8], Step [27/179], Loss: 0.5690\n",
      "Epoch [5/8], Step [28/179], Loss: 0.3431\n",
      "Epoch [5/8], Step [29/179], Loss: 0.6459\n",
      "Epoch [5/8], Step [30/179], Loss: 0.3376\n",
      "Epoch [5/8], Step [31/179], Loss: 0.5131\n",
      "Epoch [5/8], Step [32/179], Loss: 0.3214\n",
      "Epoch [5/8], Step [33/179], Loss: 0.4113\n",
      "Epoch [5/8], Step [34/179], Loss: 0.5879\n",
      "Epoch [5/8], Step [35/179], Loss: 0.3825\n",
      "Epoch [5/8], Step [36/179], Loss: 0.2453\n",
      "Epoch [5/8], Step [37/179], Loss: 0.7028\n",
      "Epoch [5/8], Step [38/179], Loss: 0.3043\n",
      "Epoch [5/8], Step [39/179], Loss: 0.4304\n",
      "Epoch [5/8], Step [40/179], Loss: 0.2630\n",
      "Epoch [5/8], Step [41/179], Loss: 0.7516\n",
      "Epoch [5/8], Step [42/179], Loss: 0.4682\n",
      "Epoch [5/8], Step [43/179], Loss: 0.4310\n",
      "Epoch [5/8], Step [44/179], Loss: 0.3469\n",
      "Epoch [5/8], Step [45/179], Loss: 0.3082\n",
      "Epoch [5/8], Step [46/179], Loss: 0.6624\n",
      "Epoch [5/8], Step [47/179], Loss: 0.2482\n",
      "Epoch [5/8], Step [48/179], Loss: 0.5387\n",
      "Epoch [5/8], Step [49/179], Loss: 0.3811\n",
      "Epoch [5/8], Step [50/179], Loss: 0.3604\n",
      "Epoch [5/8], Step [51/179], Loss: 0.6880\n",
      "Epoch [5/8], Step [52/179], Loss: 0.4577\n",
      "Epoch [5/8], Step [53/179], Loss: 0.4464\n",
      "Epoch [5/8], Step [54/179], Loss: 0.3954\n",
      "Epoch [5/8], Step [55/179], Loss: 0.4325\n",
      "Epoch [5/8], Step [56/179], Loss: 0.5891\n",
      "Epoch [5/8], Step [57/179], Loss: 0.3525\n",
      "Epoch [5/8], Step [58/179], Loss: 0.5203\n",
      "Epoch [5/8], Step [59/179], Loss: 0.6712\n",
      "Epoch [5/8], Step [60/179], Loss: 0.7257\n",
      "Epoch [5/8], Step [61/179], Loss: 0.5133\n",
      "Epoch [5/8], Step [62/179], Loss: 0.8764\n",
      "Epoch [5/8], Step [63/179], Loss: 0.6720\n",
      "Epoch [5/8], Step [64/179], Loss: 0.3526\n",
      "Epoch [5/8], Step [65/179], Loss: 0.4017\n",
      "Epoch [5/8], Step [66/179], Loss: 0.2320\n",
      "Epoch [5/8], Step [67/179], Loss: 0.5288\n",
      "Epoch [5/8], Step [68/179], Loss: 0.4477\n",
      "Epoch [5/8], Step [69/179], Loss: 0.6272\n",
      "Epoch [5/8], Step [70/179], Loss: 0.3020\n",
      "Epoch [5/8], Step [71/179], Loss: 0.6899\n",
      "Epoch [5/8], Step [72/179], Loss: 0.5421\n",
      "Epoch [5/8], Step [73/179], Loss: 0.2953\n",
      "Epoch [5/8], Step [74/179], Loss: 0.6274\n",
      "Epoch [5/8], Step [75/179], Loss: 0.4289\n",
      "Epoch [5/8], Step [76/179], Loss: 0.5196\n",
      "Epoch [5/8], Step [77/179], Loss: 0.6285\n",
      "Epoch [5/8], Step [78/179], Loss: 0.6818\n",
      "Epoch [5/8], Step [79/179], Loss: 0.3923\n",
      "Epoch [5/8], Step [80/179], Loss: 0.4902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [81/179], Loss: 0.3146\n",
      "Epoch [5/8], Step [82/179], Loss: 0.6512\n",
      "Epoch [5/8], Step [83/179], Loss: 0.6898\n",
      "Epoch [5/8], Step [84/179], Loss: 0.5551\n",
      "Epoch [5/8], Step [85/179], Loss: 0.3070\n",
      "Epoch [5/8], Step [86/179], Loss: 0.3739\n",
      "Epoch [5/8], Step [87/179], Loss: 0.5231\n",
      "Epoch [5/8], Step [88/179], Loss: 0.4591\n",
      "Epoch [5/8], Step [89/179], Loss: 0.3195\n",
      "Epoch [5/8], Step [90/179], Loss: 0.7579\n",
      "Epoch [5/8], Step [91/179], Loss: 0.4903\n",
      "Epoch [5/8], Step [92/179], Loss: 0.4558\n",
      "Epoch [5/8], Step [93/179], Loss: 0.4765\n",
      "Epoch [5/8], Step [94/179], Loss: 0.4122\n",
      "Epoch [5/8], Step [95/179], Loss: 0.4736\n",
      "Epoch [5/8], Step [96/179], Loss: 0.6674\n",
      "Epoch [5/8], Step [97/179], Loss: 0.2853\n",
      "Epoch [5/8], Step [98/179], Loss: 0.4431\n",
      "Epoch [5/8], Step [99/179], Loss: 0.3402\n",
      "Epoch [5/8], Step [100/179], Loss: 0.4361\n",
      "Epoch [5/8], Step [101/179], Loss: 0.4701\n",
      "Epoch [5/8], Step [102/179], Loss: 0.5643\n",
      "Epoch [5/8], Step [103/179], Loss: 0.5117\n",
      "Epoch [5/8], Step [104/179], Loss: 0.5771\n",
      "Epoch [5/8], Step [105/179], Loss: 0.4953\n",
      "Epoch [5/8], Step [106/179], Loss: 0.4217\n",
      "Epoch [5/8], Step [107/179], Loss: 0.5460\n",
      "Epoch [5/8], Step [108/179], Loss: 0.5765\n",
      "Epoch [5/8], Step [109/179], Loss: 0.3124\n",
      "Epoch [5/8], Step [110/179], Loss: 0.5892\n",
      "Epoch [5/8], Step [111/179], Loss: 0.5368\n",
      "Epoch [5/8], Step [112/179], Loss: 0.3697\n",
      "Epoch [5/8], Step [113/179], Loss: 0.7839\n",
      "Epoch [5/8], Step [114/179], Loss: 0.2847\n",
      "Epoch [5/8], Step [115/179], Loss: 0.3695\n",
      "Epoch [5/8], Step [116/179], Loss: 0.4397\n",
      "Epoch [5/8], Step [117/179], Loss: 0.4908\n",
      "Epoch [5/8], Step [118/179], Loss: 0.5616\n",
      "Epoch [5/8], Step [119/179], Loss: 0.4081\n",
      "Epoch [5/8], Step [120/179], Loss: 0.3353\n",
      "Epoch [5/8], Step [121/179], Loss: 0.4134\n",
      "Epoch [5/8], Step [122/179], Loss: 0.3383\n",
      "Epoch [5/8], Step [123/179], Loss: 0.4174\n",
      "Epoch [5/8], Step [124/179], Loss: 0.5324\n",
      "Epoch [5/8], Step [125/179], Loss: 0.3468\n",
      "Epoch [5/8], Step [126/179], Loss: 0.4066\n",
      "Epoch [5/8], Step [127/179], Loss: 0.6540\n",
      "Epoch [5/8], Step [128/179], Loss: 0.6023\n",
      "Epoch [5/8], Step [129/179], Loss: 0.6199\n",
      "Epoch [5/8], Step [130/179], Loss: 0.3379\n",
      "Epoch [5/8], Step [131/179], Loss: 0.7687\n",
      "Epoch [5/8], Step [132/179], Loss: 0.3398\n",
      "Epoch [5/8], Step [133/179], Loss: 0.5529\n",
      "Epoch [5/8], Step [134/179], Loss: 0.5530\n",
      "Epoch [5/8], Step [135/179], Loss: 0.5052\n",
      "Epoch [5/8], Step [136/179], Loss: 0.5140\n",
      "Epoch [5/8], Step [137/179], Loss: 0.5178\n",
      "Epoch [5/8], Step [138/179], Loss: 0.4649\n",
      "Epoch [5/8], Step [139/179], Loss: 0.4104\n",
      "Epoch [5/8], Step [140/179], Loss: 0.5060\n",
      "Epoch [5/8], Step [141/179], Loss: 0.5246\n",
      "Epoch [5/8], Step [142/179], Loss: 0.3999\n",
      "Epoch [5/8], Step [143/179], Loss: 0.6472\n",
      "Epoch [5/8], Step [144/179], Loss: 0.4980\n",
      "Epoch [5/8], Step [145/179], Loss: 0.7305\n",
      "Epoch [5/8], Step [146/179], Loss: 0.3868\n",
      "Epoch [5/8], Step [147/179], Loss: 0.3819\n",
      "Epoch [5/8], Step [148/179], Loss: 0.3859\n",
      "Epoch [5/8], Step [149/179], Loss: 0.4586\n",
      "Epoch [5/8], Step [150/179], Loss: 0.3283\n",
      "Epoch [5/8], Step [151/179], Loss: 0.5391\n",
      "Epoch [5/8], Step [152/179], Loss: 0.5366\n",
      "Epoch [5/8], Step [153/179], Loss: 0.9226\n",
      "Epoch [5/8], Step [154/179], Loss: 0.4183\n",
      "Epoch [5/8], Step [155/179], Loss: 0.4177\n",
      "Epoch [5/8], Step [156/179], Loss: 0.3309\n",
      "Epoch [5/8], Step [157/179], Loss: 0.2231\n",
      "Epoch [5/8], Step [158/179], Loss: 0.5419\n",
      "Epoch [5/8], Step [159/179], Loss: 0.5002\n",
      "Epoch [5/8], Step [160/179], Loss: 0.4207\n",
      "Epoch [5/8], Step [161/179], Loss: 0.2949\n",
      "Epoch [5/8], Step [162/179], Loss: 0.4905\n",
      "Epoch [5/8], Step [163/179], Loss: 0.5142\n",
      "Epoch [5/8], Step [164/179], Loss: 0.6016\n",
      "Epoch [5/8], Step [165/179], Loss: 0.6504\n",
      "Epoch [5/8], Step [166/179], Loss: 0.2512\n",
      "Epoch [5/8], Step [167/179], Loss: 0.6823\n",
      "Epoch [5/8], Step [168/179], Loss: 0.3854\n",
      "Epoch [5/8], Step [169/179], Loss: 0.3864\n",
      "Epoch [5/8], Step [170/179], Loss: 0.5150\n",
      "Epoch [5/8], Step [171/179], Loss: 0.5119\n",
      "Epoch [5/8], Step [172/179], Loss: 0.3863\n",
      "Epoch [5/8], Step [173/179], Loss: 0.3757\n",
      "Epoch [5/8], Step [174/179], Loss: 0.3442\n",
      "Epoch [5/8], Step [175/179], Loss: 0.4415\n",
      "Epoch [5/8], Step [176/179], Loss: 0.4459\n",
      "Epoch [5/8], Step [177/179], Loss: 0.6230\n",
      "Epoch [5/8], Step [178/179], Loss: 0.5384\n",
      "Epoch [5/8], Step [179/179], Loss: 0.5096\n",
      "Epoch [6/8], Step [1/179], Loss: 0.4085\n",
      "Epoch [6/8], Step [2/179], Loss: 0.3675\n",
      "Epoch [6/8], Step [3/179], Loss: 0.3678\n",
      "Epoch [6/8], Step [4/179], Loss: 0.2940\n",
      "Epoch [6/8], Step [5/179], Loss: 0.5754\n",
      "Epoch [6/8], Step [6/179], Loss: 0.3600\n",
      "Epoch [6/8], Step [7/179], Loss: 0.3993\n",
      "Epoch [6/8], Step [8/179], Loss: 0.2729\n",
      "Epoch [6/8], Step [9/179], Loss: 0.4218\n",
      "Epoch [6/8], Step [10/179], Loss: 0.2440\n",
      "Epoch [6/8], Step [11/179], Loss: 0.5258\n",
      "Epoch [6/8], Step [12/179], Loss: 0.2376\n",
      "Epoch [6/8], Step [13/179], Loss: 0.4399\n",
      "Epoch [6/8], Step [14/179], Loss: 0.6566\n",
      "Epoch [6/8], Step [15/179], Loss: 0.2295\n",
      "Epoch [6/8], Step [16/179], Loss: 0.1944\n",
      "Epoch [6/8], Step [17/179], Loss: 0.2893\n",
      "Epoch [6/8], Step [18/179], Loss: 0.5255\n",
      "Epoch [6/8], Step [19/179], Loss: 0.4391\n",
      "Epoch [6/8], Step [20/179], Loss: 0.5210\n",
      "Epoch [6/8], Step [21/179], Loss: 0.4038\n",
      "Epoch [6/8], Step [22/179], Loss: 0.6418\n",
      "Epoch [6/8], Step [23/179], Loss: 0.5113\n",
      "Epoch [6/8], Step [24/179], Loss: 0.4467\n",
      "Epoch [6/8], Step [25/179], Loss: 0.4312\n",
      "Epoch [6/8], Step [26/179], Loss: 0.3952\n",
      "Epoch [6/8], Step [27/179], Loss: 0.3025\n",
      "Epoch [6/8], Step [28/179], Loss: 0.2421\n",
      "Epoch [6/8], Step [29/179], Loss: 0.5309\n",
      "Epoch [6/8], Step [30/179], Loss: 0.3904\n",
      "Epoch [6/8], Step [31/179], Loss: 0.6987\n",
      "Epoch [6/8], Step [32/179], Loss: 0.2992\n",
      "Epoch [6/8], Step [33/179], Loss: 0.3848\n",
      "Epoch [6/8], Step [34/179], Loss: 0.4047\n",
      "Epoch [6/8], Step [35/179], Loss: 0.3447\n",
      "Epoch [6/8], Step [36/179], Loss: 0.2558\n",
      "Epoch [6/8], Step [37/179], Loss: 0.5537\n",
      "Epoch [6/8], Step [38/179], Loss: 0.3774\n",
      "Epoch [6/8], Step [39/179], Loss: 0.5533\n",
      "Epoch [6/8], Step [40/179], Loss: 0.5002\n",
      "Epoch [6/8], Step [41/179], Loss: 0.4258\n",
      "Epoch [6/8], Step [42/179], Loss: 0.3013\n",
      "Epoch [6/8], Step [43/179], Loss: 0.4341\n",
      "Epoch [6/8], Step [44/179], Loss: 0.4144\n",
      "Epoch [6/8], Step [45/179], Loss: 0.3583\n",
      "Epoch [6/8], Step [46/179], Loss: 0.4312\n",
      "Epoch [6/8], Step [47/179], Loss: 0.6637\n",
      "Epoch [6/8], Step [48/179], Loss: 0.7482\n",
      "Epoch [6/8], Step [49/179], Loss: 0.7140\n",
      "Epoch [6/8], Step [50/179], Loss: 0.7715\n",
      "Epoch [6/8], Step [51/179], Loss: 0.5759\n",
      "Epoch [6/8], Step [52/179], Loss: 0.4629\n",
      "Epoch [6/8], Step [53/179], Loss: 0.5526\n",
      "Epoch [6/8], Step [54/179], Loss: 0.3565\n",
      "Epoch [6/8], Step [55/179], Loss: 0.4201\n",
      "Epoch [6/8], Step [56/179], Loss: 0.4107\n",
      "Epoch [6/8], Step [57/179], Loss: 0.3228\n",
      "Epoch [6/8], Step [58/179], Loss: 0.3743\n",
      "Epoch [6/8], Step [59/179], Loss: 0.2952\n",
      "Epoch [6/8], Step [60/179], Loss: 0.2986\n",
      "Epoch [6/8], Step [61/179], Loss: 0.4384\n",
      "Epoch [6/8], Step [62/179], Loss: 0.3953\n",
      "Epoch [6/8], Step [63/179], Loss: 0.2834\n",
      "Epoch [6/8], Step [64/179], Loss: 0.2466\n",
      "Epoch [6/8], Step [65/179], Loss: 0.4956\n",
      "Epoch [6/8], Step [66/179], Loss: 0.6129\n",
      "Epoch [6/8], Step [67/179], Loss: 0.2808\n",
      "Epoch [6/8], Step [68/179], Loss: 0.5800\n",
      "Epoch [6/8], Step [69/179], Loss: 0.4123\n",
      "Epoch [6/8], Step [70/179], Loss: 0.2566\n",
      "Epoch [6/8], Step [71/179], Loss: 0.5529\n",
      "Epoch [6/8], Step [72/179], Loss: 0.3439\n",
      "Epoch [6/8], Step [73/179], Loss: 0.3107\n",
      "Epoch [6/8], Step [74/179], Loss: 0.3215\n",
      "Epoch [6/8], Step [75/179], Loss: 0.3607\n",
      "Epoch [6/8], Step [76/179], Loss: 0.4635\n",
      "Epoch [6/8], Step [77/179], Loss: 0.4351\n",
      "Epoch [6/8], Step [78/179], Loss: 0.4250\n",
      "Epoch [6/8], Step [79/179], Loss: 0.3372\n",
      "Epoch [6/8], Step [80/179], Loss: 0.2873\n",
      "Epoch [6/8], Step [81/179], Loss: 0.6661\n",
      "Epoch [6/8], Step [82/179], Loss: 0.5085\n",
      "Epoch [6/8], Step [83/179], Loss: 0.5705\n",
      "Epoch [6/8], Step [84/179], Loss: 0.5955\n",
      "Epoch [6/8], Step [85/179], Loss: 0.3831\n",
      "Epoch [6/8], Step [86/179], Loss: 0.2888\n",
      "Epoch [6/8], Step [87/179], Loss: 0.7948\n",
      "Epoch [6/8], Step [88/179], Loss: 0.4291\n",
      "Epoch [6/8], Step [89/179], Loss: 0.7400\n",
      "Epoch [6/8], Step [90/179], Loss: 0.4193\n",
      "Epoch [6/8], Step [91/179], Loss: 0.4420\n",
      "Epoch [6/8], Step [92/179], Loss: 0.5205\n",
      "Epoch [6/8], Step [93/179], Loss: 0.4956\n",
      "Epoch [6/8], Step [94/179], Loss: 0.5038\n",
      "Epoch [6/8], Step [95/179], Loss: 0.4213\n",
      "Epoch [6/8], Step [96/179], Loss: 0.5895\n",
      "Epoch [6/8], Step [97/179], Loss: 0.4345\n",
      "Epoch [6/8], Step [98/179], Loss: 0.3529\n",
      "Epoch [6/8], Step [99/179], Loss: 0.3746\n",
      "Epoch [6/8], Step [100/179], Loss: 0.3762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Step [101/179], Loss: 0.4207\n",
      "Epoch [6/8], Step [102/179], Loss: 0.4943\n",
      "Epoch [6/8], Step [103/179], Loss: 0.4391\n",
      "Epoch [6/8], Step [104/179], Loss: 0.3090\n",
      "Epoch [6/8], Step [105/179], Loss: 0.4355\n",
      "Epoch [6/8], Step [106/179], Loss: 0.5264\n",
      "Epoch [6/8], Step [107/179], Loss: 0.4265\n",
      "Epoch [6/8], Step [108/179], Loss: 0.3803\n",
      "Epoch [6/8], Step [109/179], Loss: 0.5657\n",
      "Epoch [6/8], Step [110/179], Loss: 0.2444\n",
      "Epoch [6/8], Step [111/179], Loss: 0.6087\n",
      "Epoch [6/8], Step [112/179], Loss: 0.3070\n",
      "Epoch [6/8], Step [113/179], Loss: 0.5239\n",
      "Epoch [6/8], Step [114/179], Loss: 0.4877\n",
      "Epoch [6/8], Step [115/179], Loss: 0.3954\n",
      "Epoch [6/8], Step [116/179], Loss: 0.4058\n",
      "Epoch [6/8], Step [117/179], Loss: 0.4642\n",
      "Epoch [6/8], Step [118/179], Loss: 0.4835\n",
      "Epoch [6/8], Step [119/179], Loss: 0.3069\n",
      "Epoch [6/8], Step [120/179], Loss: 0.2864\n",
      "Epoch [6/8], Step [121/179], Loss: 0.3810\n",
      "Epoch [6/8], Step [122/179], Loss: 0.8252\n",
      "Epoch [6/8], Step [123/179], Loss: 0.1725\n",
      "Epoch [6/8], Step [124/179], Loss: 0.5509\n",
      "Epoch [6/8], Step [125/179], Loss: 0.5401\n",
      "Epoch [6/8], Step [126/179], Loss: 0.3992\n",
      "Epoch [6/8], Step [127/179], Loss: 0.2300\n",
      "Epoch [6/8], Step [128/179], Loss: 0.3984\n",
      "Epoch [6/8], Step [129/179], Loss: 0.7068\n",
      "Epoch [6/8], Step [130/179], Loss: 0.3722\n",
      "Epoch [6/8], Step [131/179], Loss: 0.2767\n",
      "Epoch [6/8], Step [132/179], Loss: 0.6198\n",
      "Epoch [6/8], Step [133/179], Loss: 0.8694\n",
      "Epoch [6/8], Step [134/179], Loss: 0.5543\n",
      "Epoch [6/8], Step [135/179], Loss: 0.5729\n",
      "Epoch [6/8], Step [136/179], Loss: 0.4687\n",
      "Epoch [6/8], Step [137/179], Loss: 0.4643\n",
      "Epoch [6/8], Step [138/179], Loss: 0.4751\n",
      "Epoch [6/8], Step [139/179], Loss: 0.4975\n",
      "Epoch [6/8], Step [140/179], Loss: 0.3008\n",
      "Epoch [6/8], Step [141/179], Loss: 0.3990\n",
      "Epoch [6/8], Step [142/179], Loss: 0.5798\n",
      "Epoch [6/8], Step [143/179], Loss: 0.4800\n",
      "Epoch [6/8], Step [144/179], Loss: 0.5635\n",
      "Epoch [6/8], Step [145/179], Loss: 0.2375\n",
      "Epoch [6/8], Step [146/179], Loss: 0.3897\n",
      "Epoch [6/8], Step [147/179], Loss: 0.3979\n",
      "Epoch [6/8], Step [148/179], Loss: 0.5587\n",
      "Epoch [6/8], Step [149/179], Loss: 0.4356\n",
      "Epoch [6/8], Step [150/179], Loss: 0.3760\n",
      "Epoch [6/8], Step [151/179], Loss: 0.7239\n",
      "Epoch [6/8], Step [152/179], Loss: 0.2990\n",
      "Epoch [6/8], Step [153/179], Loss: 0.2807\n",
      "Epoch [6/8], Step [154/179], Loss: 0.4321\n",
      "Epoch [6/8], Step [155/179], Loss: 0.2615\n",
      "Epoch [6/8], Step [156/179], Loss: 0.2328\n",
      "Epoch [6/8], Step [157/179], Loss: 0.6231\n",
      "Epoch [6/8], Step [158/179], Loss: 0.3761\n",
      "Epoch [6/8], Step [159/179], Loss: 0.6828\n",
      "Epoch [6/8], Step [160/179], Loss: 0.6286\n",
      "Epoch [6/8], Step [161/179], Loss: 0.1935\n",
      "Epoch [6/8], Step [162/179], Loss: 0.5335\n",
      "Epoch [6/8], Step [163/179], Loss: 0.4770\n",
      "Epoch [6/8], Step [164/179], Loss: 0.3684\n",
      "Epoch [6/8], Step [165/179], Loss: 0.3383\n",
      "Epoch [6/8], Step [166/179], Loss: 0.2954\n",
      "Epoch [6/8], Step [167/179], Loss: 0.3510\n",
      "Epoch [6/8], Step [168/179], Loss: 0.3996\n",
      "Epoch [6/8], Step [169/179], Loss: 0.3568\n",
      "Epoch [6/8], Step [170/179], Loss: 0.3581\n",
      "Epoch [6/8], Step [171/179], Loss: 0.6884\n",
      "Epoch [6/8], Step [172/179], Loss: 0.3619\n",
      "Epoch [6/8], Step [173/179], Loss: 0.1683\n",
      "Epoch [6/8], Step [174/179], Loss: 0.4563\n",
      "Epoch [6/8], Step [175/179], Loss: 0.3683\n",
      "Epoch [6/8], Step [176/179], Loss: 0.2699\n",
      "Epoch [6/8], Step [177/179], Loss: 0.3491\n",
      "Epoch [6/8], Step [178/179], Loss: 0.2974\n",
      "Epoch [6/8], Step [179/179], Loss: 0.4534\n",
      "Epoch [7/8], Step [1/179], Loss: 0.2280\n",
      "Epoch [7/8], Step [2/179], Loss: 0.5293\n",
      "Epoch [7/8], Step [3/179], Loss: 0.5056\n",
      "Epoch [7/8], Step [4/179], Loss: 0.4057\n",
      "Epoch [7/8], Step [5/179], Loss: 0.4878\n",
      "Epoch [7/8], Step [6/179], Loss: 0.3074\n",
      "Epoch [7/8], Step [7/179], Loss: 0.3791\n",
      "Epoch [7/8], Step [8/179], Loss: 0.3287\n",
      "Epoch [7/8], Step [9/179], Loss: 0.4051\n",
      "Epoch [7/8], Step [10/179], Loss: 0.2524\n",
      "Epoch [7/8], Step [11/179], Loss: 0.4578\n",
      "Epoch [7/8], Step [12/179], Loss: 0.3579\n",
      "Epoch [7/8], Step [13/179], Loss: 0.3995\n",
      "Epoch [7/8], Step [14/179], Loss: 0.2971\n",
      "Epoch [7/8], Step [15/179], Loss: 0.3935\n",
      "Epoch [7/8], Step [16/179], Loss: 0.5832\n",
      "Epoch [7/8], Step [17/179], Loss: 0.3075\n",
      "Epoch [7/8], Step [18/179], Loss: 0.2576\n",
      "Epoch [7/8], Step [19/179], Loss: 0.6034\n",
      "Epoch [7/8], Step [20/179], Loss: 0.3523\n",
      "Epoch [7/8], Step [21/179], Loss: 0.4163\n",
      "Epoch [7/8], Step [22/179], Loss: 0.2927\n",
      "Epoch [7/8], Step [23/179], Loss: 0.3822\n",
      "Epoch [7/8], Step [24/179], Loss: 0.2936\n",
      "Epoch [7/8], Step [25/179], Loss: 0.3314\n",
      "Epoch [7/8], Step [26/179], Loss: 0.4721\n",
      "Epoch [7/8], Step [27/179], Loss: 0.3852\n",
      "Epoch [7/8], Step [28/179], Loss: 0.6044\n",
      "Epoch [7/8], Step [29/179], Loss: 0.2585\n",
      "Epoch [7/8], Step [30/179], Loss: 0.7769\n",
      "Epoch [7/8], Step [31/179], Loss: 0.4765\n",
      "Epoch [7/8], Step [32/179], Loss: 0.4821\n",
      "Epoch [7/8], Step [33/179], Loss: 0.3460\n",
      "Epoch [7/8], Step [34/179], Loss: 0.3247\n",
      "Epoch [7/8], Step [35/179], Loss: 0.4390\n",
      "Epoch [7/8], Step [36/179], Loss: 0.5484\n",
      "Epoch [7/8], Step [37/179], Loss: 0.6151\n",
      "Epoch [7/8], Step [38/179], Loss: 0.3710\n",
      "Epoch [7/8], Step [39/179], Loss: 0.4688\n",
      "Epoch [7/8], Step [40/179], Loss: 0.4577\n",
      "Epoch [7/8], Step [41/179], Loss: 0.2759\n",
      "Epoch [7/8], Step [42/179], Loss: 0.3535\n",
      "Epoch [7/8], Step [43/179], Loss: 0.2694\n",
      "Epoch [7/8], Step [44/179], Loss: 0.4256\n",
      "Epoch [7/8], Step [45/179], Loss: 0.4870\n",
      "Epoch [7/8], Step [46/179], Loss: 0.4717\n",
      "Epoch [7/8], Step [47/179], Loss: 0.3412\n",
      "Epoch [7/8], Step [48/179], Loss: 0.3191\n",
      "Epoch [7/8], Step [49/179], Loss: 0.2668\n",
      "Epoch [7/8], Step [50/179], Loss: 0.2477\n",
      "Epoch [7/8], Step [51/179], Loss: 0.3287\n",
      "Epoch [7/8], Step [52/179], Loss: 0.5725\n",
      "Epoch [7/8], Step [53/179], Loss: 0.3619\n",
      "Epoch [7/8], Step [54/179], Loss: 0.3605\n",
      "Epoch [7/8], Step [55/179], Loss: 0.3550\n",
      "Epoch [7/8], Step [56/179], Loss: 0.4649\n",
      "Epoch [7/8], Step [57/179], Loss: 0.3224\n",
      "Epoch [7/8], Step [58/179], Loss: 0.4244\n",
      "Epoch [7/8], Step [59/179], Loss: 0.4419\n",
      "Epoch [7/8], Step [60/179], Loss: 0.2467\n",
      "Epoch [7/8], Step [61/179], Loss: 0.3754\n",
      "Epoch [7/8], Step [62/179], Loss: 0.4488\n",
      "Epoch [7/8], Step [63/179], Loss: 0.1844\n",
      "Epoch [7/8], Step [64/179], Loss: 0.4863\n",
      "Epoch [7/8], Step [65/179], Loss: 0.3604\n",
      "Epoch [7/8], Step [66/179], Loss: 0.2988\n",
      "Epoch [7/8], Step [67/179], Loss: 0.4346\n",
      "Epoch [7/8], Step [68/179], Loss: 0.5461\n",
      "Epoch [7/8], Step [69/179], Loss: 0.2454\n",
      "Epoch [7/8], Step [70/179], Loss: 0.4638\n",
      "Epoch [7/8], Step [71/179], Loss: 0.2650\n",
      "Epoch [7/8], Step [72/179], Loss: 0.2525\n",
      "Epoch [7/8], Step [73/179], Loss: 0.3293\n",
      "Epoch [7/8], Step [74/179], Loss: 0.2528\n",
      "Epoch [7/8], Step [75/179], Loss: 0.2796\n",
      "Epoch [7/8], Step [76/179], Loss: 0.1557\n",
      "Epoch [7/8], Step [77/179], Loss: 0.5116\n",
      "Epoch [7/8], Step [78/179], Loss: 0.2685\n",
      "Epoch [7/8], Step [79/179], Loss: 0.3365\n",
      "Epoch [7/8], Step [80/179], Loss: 0.5314\n",
      "Epoch [7/8], Step [81/179], Loss: 0.3874\n",
      "Epoch [7/8], Step [82/179], Loss: 0.2340\n",
      "Epoch [7/8], Step [83/179], Loss: 0.4214\n",
      "Epoch [7/8], Step [84/179], Loss: 0.3090\n",
      "Epoch [7/8], Step [85/179], Loss: 0.6593\n",
      "Epoch [7/8], Step [86/179], Loss: 0.6877\n",
      "Epoch [7/8], Step [87/179], Loss: 0.4187\n",
      "Epoch [7/8], Step [88/179], Loss: 0.1973\n",
      "Epoch [7/8], Step [89/179], Loss: 0.4220\n",
      "Epoch [7/8], Step [90/179], Loss: 0.4867\n",
      "Epoch [7/8], Step [91/179], Loss: 0.5961\n",
      "Epoch [7/8], Step [92/179], Loss: 0.3669\n",
      "Epoch [7/8], Step [93/179], Loss: 0.4437\n",
      "Epoch [7/8], Step [94/179], Loss: 0.4081\n",
      "Epoch [7/8], Step [95/179], Loss: 0.2949\n",
      "Epoch [7/8], Step [96/179], Loss: 0.3081\n",
      "Epoch [7/8], Step [97/179], Loss: 0.4686\n",
      "Epoch [7/8], Step [98/179], Loss: 0.4625\n",
      "Epoch [7/8], Step [99/179], Loss: 0.2867\n",
      "Epoch [7/8], Step [100/179], Loss: 0.2979\n",
      "Epoch [7/8], Step [101/179], Loss: 0.4575\n",
      "Epoch [7/8], Step [102/179], Loss: 0.1956\n",
      "Epoch [7/8], Step [103/179], Loss: 0.2901\n",
      "Epoch [7/8], Step [104/179], Loss: 0.4573\n",
      "Epoch [7/8], Step [105/179], Loss: 0.4018\n",
      "Epoch [7/8], Step [106/179], Loss: 0.3645\n",
      "Epoch [7/8], Step [107/179], Loss: 0.3420\n",
      "Epoch [7/8], Step [108/179], Loss: 0.4659\n",
      "Epoch [7/8], Step [109/179], Loss: 0.4476\n",
      "Epoch [7/8], Step [110/179], Loss: 0.4579\n",
      "Epoch [7/8], Step [111/179], Loss: 0.3636\n",
      "Epoch [7/8], Step [112/179], Loss: 0.2634\n",
      "Epoch [7/8], Step [113/179], Loss: 0.3316\n",
      "Epoch [7/8], Step [114/179], Loss: 0.2504\n",
      "Epoch [7/8], Step [115/179], Loss: 0.3609\n",
      "Epoch [7/8], Step [116/179], Loss: 0.3767\n",
      "Epoch [7/8], Step [117/179], Loss: 0.3439\n",
      "Epoch [7/8], Step [118/179], Loss: 0.2505\n",
      "Epoch [7/8], Step [119/179], Loss: 0.2365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [120/179], Loss: 0.3393\n",
      "Epoch [7/8], Step [121/179], Loss: 0.4568\n",
      "Epoch [7/8], Step [122/179], Loss: 0.1677\n",
      "Epoch [7/8], Step [123/179], Loss: 0.1743\n",
      "Epoch [7/8], Step [124/179], Loss: 0.3400\n",
      "Epoch [7/8], Step [125/179], Loss: 0.4795\n",
      "Epoch [7/8], Step [126/179], Loss: 0.4389\n",
      "Epoch [7/8], Step [127/179], Loss: 0.5961\n",
      "Epoch [7/8], Step [128/179], Loss: 0.1857\n",
      "Epoch [7/8], Step [129/179], Loss: 0.1886\n",
      "Epoch [7/8], Step [130/179], Loss: 0.4617\n",
      "Epoch [7/8], Step [131/179], Loss: 0.2802\n",
      "Epoch [7/8], Step [132/179], Loss: 0.3023\n",
      "Epoch [7/8], Step [133/179], Loss: 0.2988\n",
      "Epoch [7/8], Step [134/179], Loss: 0.3950\n",
      "Epoch [7/8], Step [135/179], Loss: 0.3694\n",
      "Epoch [7/8], Step [136/179], Loss: 0.4195\n",
      "Epoch [7/8], Step [137/179], Loss: 0.4133\n",
      "Epoch [7/8], Step [138/179], Loss: 0.3557\n",
      "Epoch [7/8], Step [139/179], Loss: 0.5311\n",
      "Epoch [7/8], Step [140/179], Loss: 0.6221\n",
      "Epoch [7/8], Step [141/179], Loss: 0.3503\n",
      "Epoch [7/8], Step [142/179], Loss: 0.3193\n",
      "Epoch [7/8], Step [143/179], Loss: 0.2903\n",
      "Epoch [7/8], Step [144/179], Loss: 0.2928\n",
      "Epoch [7/8], Step [145/179], Loss: 0.3030\n",
      "Epoch [7/8], Step [146/179], Loss: 0.3115\n",
      "Epoch [7/8], Step [147/179], Loss: 0.4324\n",
      "Epoch [7/8], Step [148/179], Loss: 0.4942\n",
      "Epoch [7/8], Step [149/179], Loss: 0.3792\n",
      "Epoch [7/8], Step [150/179], Loss: 0.2580\n",
      "Epoch [7/8], Step [151/179], Loss: 0.3291\n",
      "Epoch [7/8], Step [152/179], Loss: 0.1485\n",
      "Epoch [7/8], Step [153/179], Loss: 0.2650\n",
      "Epoch [7/8], Step [154/179], Loss: 0.3129\n",
      "Epoch [7/8], Step [155/179], Loss: 0.4767\n",
      "Epoch [7/8], Step [156/179], Loss: 0.4247\n",
      "Epoch [7/8], Step [157/179], Loss: 0.2139\n",
      "Epoch [7/8], Step [158/179], Loss: 0.3049\n",
      "Epoch [7/8], Step [159/179], Loss: 0.3516\n",
      "Epoch [7/8], Step [160/179], Loss: 0.6929\n",
      "Epoch [7/8], Step [161/179], Loss: 0.3773\n",
      "Epoch [7/8], Step [162/179], Loss: 0.4189\n",
      "Epoch [7/8], Step [163/179], Loss: 0.4578\n",
      "Epoch [7/8], Step [164/179], Loss: 0.3607\n",
      "Epoch [7/8], Step [165/179], Loss: 0.3856\n",
      "Epoch [7/8], Step [166/179], Loss: 0.4547\n",
      "Epoch [7/8], Step [167/179], Loss: 0.2573\n",
      "Epoch [7/8], Step [168/179], Loss: 0.5166\n",
      "Epoch [7/8], Step [169/179], Loss: 0.4607\n",
      "Epoch [7/8], Step [170/179], Loss: 0.5542\n",
      "Epoch [7/8], Step [171/179], Loss: 0.3945\n",
      "Epoch [7/8], Step [172/179], Loss: 0.3689\n",
      "Epoch [7/8], Step [173/179], Loss: 0.3543\n",
      "Epoch [7/8], Step [174/179], Loss: 0.3194\n",
      "Epoch [7/8], Step [175/179], Loss: 0.3763\n",
      "Epoch [7/8], Step [176/179], Loss: 0.5235\n",
      "Epoch [7/8], Step [177/179], Loss: 0.3436\n",
      "Epoch [7/8], Step [178/179], Loss: 0.3613\n",
      "Epoch [7/8], Step [179/179], Loss: 0.4840\n",
      "Epoch [8/8], Step [1/179], Loss: 0.2503\n",
      "Epoch [8/8], Step [2/179], Loss: 0.2489\n",
      "Epoch [8/8], Step [3/179], Loss: 0.2376\n",
      "Epoch [8/8], Step [4/179], Loss: 0.1536\n",
      "Epoch [8/8], Step [5/179], Loss: 0.2913\n",
      "Epoch [8/8], Step [6/179], Loss: 0.3094\n",
      "Epoch [8/8], Step [7/179], Loss: 0.5149\n",
      "Epoch [8/8], Step [8/179], Loss: 0.4179\n",
      "Epoch [8/8], Step [9/179], Loss: 0.3267\n",
      "Epoch [8/8], Step [10/179], Loss: 0.2446\n",
      "Epoch [8/8], Step [11/179], Loss: 0.4377\n",
      "Epoch [8/8], Step [12/179], Loss: 0.1288\n",
      "Epoch [8/8], Step [13/179], Loss: 0.5390\n",
      "Epoch [8/8], Step [14/179], Loss: 0.0985\n",
      "Epoch [8/8], Step [15/179], Loss: 0.5053\n",
      "Epoch [8/8], Step [16/179], Loss: 0.3663\n",
      "Epoch [8/8], Step [17/179], Loss: 0.2076\n",
      "Epoch [8/8], Step [18/179], Loss: 0.2756\n",
      "Epoch [8/8], Step [19/179], Loss: 0.3349\n",
      "Epoch [8/8], Step [20/179], Loss: 0.5298\n",
      "Epoch [8/8], Step [21/179], Loss: 0.3519\n",
      "Epoch [8/8], Step [22/179], Loss: 0.4516\n",
      "Epoch [8/8], Step [23/179], Loss: 0.2800\n",
      "Epoch [8/8], Step [24/179], Loss: 0.3460\n",
      "Epoch [8/8], Step [25/179], Loss: 0.3973\n",
      "Epoch [8/8], Step [26/179], Loss: 0.3545\n",
      "Epoch [8/8], Step [27/179], Loss: 0.1837\n",
      "Epoch [8/8], Step [28/179], Loss: 0.2771\n",
      "Epoch [8/8], Step [29/179], Loss: 0.4721\n",
      "Epoch [8/8], Step [30/179], Loss: 0.3089\n",
      "Epoch [8/8], Step [31/179], Loss: 0.1949\n",
      "Epoch [8/8], Step [32/179], Loss: 0.2492\n",
      "Epoch [8/8], Step [33/179], Loss: 0.3385\n",
      "Epoch [8/8], Step [34/179], Loss: 0.2714\n",
      "Epoch [8/8], Step [35/179], Loss: 0.2798\n",
      "Epoch [8/8], Step [36/179], Loss: 0.5333\n",
      "Epoch [8/8], Step [37/179], Loss: 0.4243\n",
      "Epoch [8/8], Step [38/179], Loss: 0.4765\n",
      "Epoch [8/8], Step [39/179], Loss: 0.2601\n",
      "Epoch [8/8], Step [40/179], Loss: 0.2315\n",
      "Epoch [8/8], Step [41/179], Loss: 0.2673\n",
      "Epoch [8/8], Step [42/179], Loss: 0.2872\n",
      "Epoch [8/8], Step [43/179], Loss: 0.1409\n",
      "Epoch [8/8], Step [44/179], Loss: 0.2732\n",
      "Epoch [8/8], Step [45/179], Loss: 0.3489\n",
      "Epoch [8/8], Step [46/179], Loss: 0.3670\n",
      "Epoch [8/8], Step [47/179], Loss: 0.2856\n",
      "Epoch [8/8], Step [48/179], Loss: 0.2260\n",
      "Epoch [8/8], Step [49/179], Loss: 0.4833\n",
      "Epoch [8/8], Step [50/179], Loss: 0.1880\n",
      "Epoch [8/8], Step [51/179], Loss: 0.2217\n",
      "Epoch [8/8], Step [52/179], Loss: 0.2299\n",
      "Epoch [8/8], Step [53/179], Loss: 0.2162\n",
      "Epoch [8/8], Step [54/179], Loss: 0.2329\n",
      "Epoch [8/8], Step [55/179], Loss: 0.3170\n",
      "Epoch [8/8], Step [56/179], Loss: 0.2013\n",
      "Epoch [8/8], Step [57/179], Loss: 0.4095\n",
      "Epoch [8/8], Step [58/179], Loss: 0.5809\n",
      "Epoch [8/8], Step [59/179], Loss: 0.2296\n",
      "Epoch [8/8], Step [60/179], Loss: 0.2114\n",
      "Epoch [8/8], Step [61/179], Loss: 0.2291\n",
      "Epoch [8/8], Step [62/179], Loss: 0.4028\n",
      "Epoch [8/8], Step [63/179], Loss: 0.3387\n",
      "Epoch [8/8], Step [64/179], Loss: 0.2093\n",
      "Epoch [8/8], Step [65/179], Loss: 0.3021\n",
      "Epoch [8/8], Step [66/179], Loss: 0.1851\n",
      "Epoch [8/8], Step [67/179], Loss: 0.1947\n",
      "Epoch [8/8], Step [68/179], Loss: 0.3651\n",
      "Epoch [8/8], Step [69/179], Loss: 0.5189\n",
      "Epoch [8/8], Step [70/179], Loss: 0.1731\n",
      "Epoch [8/8], Step [71/179], Loss: 0.4237\n",
      "Epoch [8/8], Step [72/179], Loss: 0.1383\n",
      "Epoch [8/8], Step [73/179], Loss: 0.4389\n",
      "Epoch [8/8], Step [74/179], Loss: 0.2286\n",
      "Epoch [8/8], Step [75/179], Loss: 0.2717\n",
      "Epoch [8/8], Step [76/179], Loss: 0.4320\n",
      "Epoch [8/8], Step [77/179], Loss: 0.2998\n",
      "Epoch [8/8], Step [78/179], Loss: 0.3292\n",
      "Epoch [8/8], Step [79/179], Loss: 0.4792\n",
      "Epoch [8/8], Step [80/179], Loss: 0.3410\n",
      "Epoch [8/8], Step [81/179], Loss: 0.1043\n",
      "Epoch [8/8], Step [82/179], Loss: 0.2967\n",
      "Epoch [8/8], Step [83/179], Loss: 0.2038\n",
      "Epoch [8/8], Step [84/179], Loss: 0.3498\n",
      "Epoch [8/8], Step [85/179], Loss: 0.6789\n",
      "Epoch [8/8], Step [86/179], Loss: 0.3801\n",
      "Epoch [8/8], Step [87/179], Loss: 0.2968\n",
      "Epoch [8/8], Step [88/179], Loss: 0.4584\n",
      "Epoch [8/8], Step [89/179], Loss: 0.3853\n",
      "Epoch [8/8], Step [90/179], Loss: 0.2544\n",
      "Epoch [8/8], Step [91/179], Loss: 0.7196\n",
      "Epoch [8/8], Step [92/179], Loss: 0.2886\n",
      "Epoch [8/8], Step [93/179], Loss: 0.2888\n",
      "Epoch [8/8], Step [94/179], Loss: 0.5734\n",
      "Epoch [8/8], Step [95/179], Loss: 0.4778\n",
      "Epoch [8/8], Step [96/179], Loss: 0.6825\n",
      "Epoch [8/8], Step [97/179], Loss: 0.3271\n",
      "Epoch [8/8], Step [98/179], Loss: 0.4468\n",
      "Epoch [8/8], Step [99/179], Loss: 0.3338\n",
      "Epoch [8/8], Step [100/179], Loss: 0.3808\n",
      "Epoch [8/8], Step [101/179], Loss: 0.3839\n",
      "Epoch [8/8], Step [102/179], Loss: 0.3547\n",
      "Epoch [8/8], Step [103/179], Loss: 0.3814\n",
      "Epoch [8/8], Step [104/179], Loss: 0.5983\n",
      "Epoch [8/8], Step [105/179], Loss: 0.1862\n",
      "Epoch [8/8], Step [106/179], Loss: 0.3331\n",
      "Epoch [8/8], Step [107/179], Loss: 0.5187\n",
      "Epoch [8/8], Step [108/179], Loss: 0.6684\n",
      "Epoch [8/8], Step [109/179], Loss: 0.3310\n",
      "Epoch [8/8], Step [110/179], Loss: 0.1949\n",
      "Epoch [8/8], Step [111/179], Loss: 0.6700\n",
      "Epoch [8/8], Step [112/179], Loss: 0.5711\n",
      "Epoch [8/8], Step [113/179], Loss: 0.2315\n",
      "Epoch [8/8], Step [114/179], Loss: 0.4350\n",
      "Epoch [8/8], Step [115/179], Loss: 0.1674\n",
      "Epoch [8/8], Step [116/179], Loss: 0.3509\n",
      "Epoch [8/8], Step [117/179], Loss: 0.3206\n",
      "Epoch [8/8], Step [118/179], Loss: 0.4648\n",
      "Epoch [8/8], Step [119/179], Loss: 0.2286\n",
      "Epoch [8/8], Step [120/179], Loss: 0.3652\n",
      "Epoch [8/8], Step [121/179], Loss: 0.4993\n",
      "Epoch [8/8], Step [122/179], Loss: 0.3772\n",
      "Epoch [8/8], Step [123/179], Loss: 0.3358\n",
      "Epoch [8/8], Step [124/179], Loss: 0.2133\n",
      "Epoch [8/8], Step [125/179], Loss: 0.2870\n",
      "Epoch [8/8], Step [126/179], Loss: 0.7677\n",
      "Epoch [8/8], Step [127/179], Loss: 0.4036\n",
      "Epoch [8/8], Step [128/179], Loss: 0.3080\n",
      "Epoch [8/8], Step [129/179], Loss: 0.3882\n",
      "Epoch [8/8], Step [130/179], Loss: 0.3079\n",
      "Epoch [8/8], Step [131/179], Loss: 0.2579\n",
      "Epoch [8/8], Step [132/179], Loss: 0.5065\n",
      "Epoch [8/8], Step [133/179], Loss: 0.4815\n",
      "Epoch [8/8], Step [134/179], Loss: 0.2631\n",
      "Epoch [8/8], Step [135/179], Loss: 0.2913\n",
      "Epoch [8/8], Step [136/179], Loss: 0.7057\n",
      "Epoch [8/8], Step [137/179], Loss: 0.2281\n",
      "Epoch [8/8], Step [138/179], Loss: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [139/179], Loss: 0.2095\n",
      "Epoch [8/8], Step [140/179], Loss: 0.5756\n",
      "Epoch [8/8], Step [141/179], Loss: 0.2146\n",
      "Epoch [8/8], Step [142/179], Loss: 0.4942\n",
      "Epoch [8/8], Step [143/179], Loss: 0.4276\n",
      "Epoch [8/8], Step [144/179], Loss: 0.4235\n",
      "Epoch [8/8], Step [145/179], Loss: 0.3497\n",
      "Epoch [8/8], Step [146/179], Loss: 0.1765\n",
      "Epoch [8/8], Step [147/179], Loss: 0.3704\n",
      "Epoch [8/8], Step [148/179], Loss: 0.3526\n",
      "Epoch [8/8], Step [149/179], Loss: 0.3762\n",
      "Epoch [8/8], Step [150/179], Loss: 0.2689\n",
      "Epoch [8/8], Step [151/179], Loss: 0.2266\n",
      "Epoch [8/8], Step [152/179], Loss: 0.3788\n",
      "Epoch [8/8], Step [153/179], Loss: 0.2249\n",
      "Epoch [8/8], Step [154/179], Loss: 0.4979\n",
      "Epoch [8/8], Step [155/179], Loss: 0.2456\n",
      "Epoch [8/8], Step [156/179], Loss: 0.2894\n",
      "Epoch [8/8], Step [157/179], Loss: 0.3378\n",
      "Epoch [8/8], Step [158/179], Loss: 0.3075\n",
      "Epoch [8/8], Step [159/179], Loss: 0.2879\n",
      "Epoch [8/8], Step [160/179], Loss: 0.2649\n",
      "Epoch [8/8], Step [161/179], Loss: 0.3104\n",
      "Epoch [8/8], Step [162/179], Loss: 0.3754\n",
      "Epoch [8/8], Step [163/179], Loss: 0.5308\n",
      "Epoch [8/8], Step [164/179], Loss: 0.3077\n",
      "Epoch [8/8], Step [165/179], Loss: 0.3019\n",
      "Epoch [8/8], Step [166/179], Loss: 0.5596\n",
      "Epoch [8/8], Step [167/179], Loss: 0.3612\n",
      "Epoch [8/8], Step [168/179], Loss: 0.1400\n",
      "Epoch [8/8], Step [169/179], Loss: 0.4568\n",
      "Epoch [8/8], Step [170/179], Loss: 0.1860\n",
      "Epoch [8/8], Step [171/179], Loss: 0.4965\n",
      "Epoch [8/8], Step [172/179], Loss: 0.6493\n",
      "Epoch [8/8], Step [173/179], Loss: 0.2624\n",
      "Epoch [8/8], Step [174/179], Loss: 0.2191\n",
      "Epoch [8/8], Step [175/179], Loss: 0.3975\n",
      "Epoch [8/8], Step [176/179], Loss: 0.3354\n",
      "Epoch [8/8], Step [177/179], Loss: 0.4219\n",
      "Epoch [8/8], Step [178/179], Loss: 0.4824\n",
      "Epoch [8/8], Step [179/179], Loss: 0.3074\n",
      "Accuracy of the network: 82.83752860411899 %\n",
      "Accuracy of glioma: 86.04651162790698 %\n",
      "Accuracy of meningioma: 85.71428571428571 %\n",
      "Accuracy of notumor: 92.3076923076923 %\n",
      "Accuracy of pituitary: 97.05882352941177 %\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "train_path = './multiple_brain_tumor/Training'\n",
    "test_path = './multiple_brain_tumor/Testing'\n",
    "\n",
    "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "epochs = 8\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "     ]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder(root=train_path, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_path, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# imshow(torchvision.utils.make_grid(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorModel(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        # self.conv_layer3 = nn.Sequential(\n",
    "        #     nn.Conv2d(in_channels=hidden_units,\n",
    "        #               out_channels=hidden_units,\n",
    "        #               kernel_size=3,\n",
    "        #               stride=1,\n",
    "        #               padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(in_channels=hidden_units,\n",
    "        #               out_channels=hidden_units,\n",
    "        #               kernel_size=3,\n",
    "        #               stride=1,\n",
    "        #               padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2)\n",
    "        # )\n",
    "\n",
    "        self.conv_output_size = hidden_units * (224 // 4) * (224 // 4)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=self.conv_output_size,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        # x = self.conv_layer3(x)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c24ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TumorModel(input_shape=3,\n",
    "                    hidden_units=4,\n",
    "                    output_shape=len(class_names)).to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adamax(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss_val = loss(outputs, labels)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 2000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss_val.item():.4f}')\n",
    "\n",
    "# print(\"Training Complete\")\n",
    "torch.save(model.state_dict(), 'Tumor_Model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(4)]\n",
    "    n_class_samples = [0 for i in range(4)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(4):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {class_names[i]}: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
