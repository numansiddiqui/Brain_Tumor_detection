{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ddc7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [1/179], Loss: 1.3782\n",
      "Epoch [1/8], Step [2/179], Loss: 1.6556\n",
      "Epoch [1/8], Step [3/179], Loss: 1.3143\n",
      "Epoch [1/8], Step [4/179], Loss: 1.3739\n",
      "Epoch [1/8], Step [5/179], Loss: 1.4150\n",
      "Epoch [1/8], Step [6/179], Loss: 1.3797\n",
      "Epoch [1/8], Step [7/179], Loss: 1.3561\n",
      "Epoch [1/8], Step [8/179], Loss: 1.3090\n",
      "Epoch [1/8], Step [9/179], Loss: 1.4151\n",
      "Epoch [1/8], Step [10/179], Loss: 1.4866\n",
      "Epoch [1/8], Step [11/179], Loss: 1.4853\n",
      "Epoch [1/8], Step [12/179], Loss: 1.4811\n",
      "Epoch [1/8], Step [13/179], Loss: 1.3624\n",
      "Epoch [1/8], Step [14/179], Loss: 1.3813\n",
      "Epoch [1/8], Step [15/179], Loss: 1.3949\n",
      "Epoch [1/8], Step [16/179], Loss: 1.3689\n",
      "Epoch [1/8], Step [17/179], Loss: 1.4356\n",
      "Epoch [1/8], Step [18/179], Loss: 1.4469\n",
      "Epoch [1/8], Step [19/179], Loss: 1.4737\n",
      "Epoch [1/8], Step [20/179], Loss: 1.4569\n",
      "Epoch [1/8], Step [21/179], Loss: 1.3635\n",
      "Epoch [1/8], Step [22/179], Loss: 1.4171\n",
      "Epoch [1/8], Step [23/179], Loss: 1.3891\n",
      "Epoch [1/8], Step [24/179], Loss: 1.3883\n",
      "Epoch [1/8], Step [25/179], Loss: 1.3875\n",
      "Epoch [1/8], Step [26/179], Loss: 1.3938\n",
      "Epoch [1/8], Step [27/179], Loss: 1.3537\n",
      "Epoch [1/8], Step [28/179], Loss: 1.3711\n",
      "Epoch [1/8], Step [29/179], Loss: 1.4200\n",
      "Epoch [1/8], Step [30/179], Loss: 1.4250\n",
      "Epoch [1/8], Step [31/179], Loss: 1.3508\n",
      "Epoch [1/8], Step [32/179], Loss: 1.4419\n",
      "Epoch [1/8], Step [33/179], Loss: 1.4390\n",
      "Epoch [1/8], Step [34/179], Loss: 1.3684\n",
      "Epoch [1/8], Step [35/179], Loss: 1.4407\n",
      "Epoch [1/8], Step [36/179], Loss: 1.3762\n",
      "Epoch [1/8], Step [37/179], Loss: 1.3844\n",
      "Epoch [1/8], Step [38/179], Loss: 1.3887\n",
      "Epoch [1/8], Step [39/179], Loss: 1.3860\n",
      "Epoch [1/8], Step [40/179], Loss: 1.3713\n",
      "Epoch [1/8], Step [41/179], Loss: 1.4133\n",
      "Epoch [1/8], Step [42/179], Loss: 1.3801\n",
      "Epoch [1/8], Step [43/179], Loss: 1.3562\n",
      "Epoch [1/8], Step [44/179], Loss: 1.3995\n",
      "Epoch [1/8], Step [45/179], Loss: 1.3686\n",
      "Epoch [1/8], Step [46/179], Loss: 1.3463\n",
      "Epoch [1/8], Step [47/179], Loss: 1.4432\n",
      "Epoch [1/8], Step [48/179], Loss: 1.5356\n",
      "Epoch [1/8], Step [49/179], Loss: 1.4828\n",
      "Epoch [1/8], Step [50/179], Loss: 1.3616\n",
      "Epoch [1/8], Step [51/179], Loss: 1.3411\n",
      "Epoch [1/8], Step [52/179], Loss: 1.4300\n",
      "Epoch [1/8], Step [53/179], Loss: 1.3969\n",
      "Epoch [1/8], Step [54/179], Loss: 1.3832\n",
      "Epoch [1/8], Step [55/179], Loss: 1.3772\n",
      "Epoch [1/8], Step [56/179], Loss: 1.4012\n",
      "Epoch [1/8], Step [57/179], Loss: 1.3736\n",
      "Epoch [1/8], Step [58/179], Loss: 1.3719\n",
      "Epoch [1/8], Step [59/179], Loss: 1.4150\n",
      "Epoch [1/8], Step [60/179], Loss: 1.4145\n",
      "Epoch [1/8], Step [61/179], Loss: 1.3770\n",
      "Epoch [1/8], Step [62/179], Loss: 1.3775\n",
      "Epoch [1/8], Step [63/179], Loss: 1.3946\n",
      "Epoch [1/8], Step [64/179], Loss: 1.3715\n",
      "Epoch [1/8], Step [65/179], Loss: 1.3714\n",
      "Epoch [1/8], Step [66/179], Loss: 1.3581\n",
      "Epoch [1/8], Step [67/179], Loss: 1.4109\n",
      "Epoch [1/8], Step [68/179], Loss: 1.4293\n",
      "Epoch [1/8], Step [69/179], Loss: 1.4156\n",
      "Epoch [1/8], Step [70/179], Loss: 1.3579\n",
      "Epoch [1/8], Step [71/179], Loss: 1.4224\n",
      "Epoch [1/8], Step [72/179], Loss: 1.4161\n",
      "Epoch [1/8], Step [73/179], Loss: 1.4218\n",
      "Epoch [1/8], Step [74/179], Loss: 1.4158\n",
      "Epoch [1/8], Step [75/179], Loss: 1.4078\n",
      "Epoch [1/8], Step [76/179], Loss: 1.4005\n",
      "Epoch [1/8], Step [77/179], Loss: 1.3327\n",
      "Epoch [1/8], Step [78/179], Loss: 1.3580\n",
      "Epoch [1/8], Step [79/179], Loss: 1.3986\n",
      "Epoch [1/8], Step [80/179], Loss: 1.4861\n",
      "Epoch [1/8], Step [81/179], Loss: 1.4108\n",
      "Epoch [1/8], Step [82/179], Loss: 1.4341\n",
      "Epoch [1/8], Step [83/179], Loss: 1.3634\n",
      "Epoch [1/8], Step [84/179], Loss: 1.3633\n",
      "Epoch [1/8], Step [85/179], Loss: 1.4318\n",
      "Epoch [1/8], Step [86/179], Loss: 1.3753\n",
      "Epoch [1/8], Step [87/179], Loss: 1.3820\n",
      "Epoch [1/8], Step [88/179], Loss: 1.5199\n",
      "Epoch [1/8], Step [89/179], Loss: 1.4012\n",
      "Epoch [1/8], Step [90/179], Loss: 1.4307\n",
      "Epoch [1/8], Step [91/179], Loss: 1.4585\n",
      "Epoch [1/8], Step [92/179], Loss: 1.3572\n",
      "Epoch [1/8], Step [93/179], Loss: 1.3376\n",
      "Epoch [1/8], Step [94/179], Loss: 1.4152\n",
      "Epoch [1/8], Step [95/179], Loss: 1.3360\n",
      "Epoch [1/8], Step [96/179], Loss: 1.3627\n",
      "Epoch [1/8], Step [97/179], Loss: 1.3814\n",
      "Epoch [1/8], Step [98/179], Loss: 1.3207\n",
      "Epoch [1/8], Step [99/179], Loss: 1.3635\n",
      "Epoch [1/8], Step [100/179], Loss: 1.3762\n",
      "Epoch [1/8], Step [101/179], Loss: 1.3603\n",
      "Epoch [1/8], Step [102/179], Loss: 1.3684\n",
      "Epoch [1/8], Step [103/179], Loss: 1.3483\n",
      "Epoch [1/8], Step [104/179], Loss: 1.3122\n",
      "Epoch [1/8], Step [105/179], Loss: 1.3728\n",
      "Epoch [1/8], Step [106/179], Loss: 1.3304\n",
      "Epoch [1/8], Step [107/179], Loss: 1.3099\n",
      "Epoch [1/8], Step [108/179], Loss: 1.2808\n",
      "Epoch [1/8], Step [109/179], Loss: 1.2879\n",
      "Epoch [1/8], Step [110/179], Loss: 1.1578\n",
      "Epoch [1/8], Step [111/179], Loss: 1.4406\n",
      "Epoch [1/8], Step [112/179], Loss: 1.3853\n",
      "Epoch [1/8], Step [113/179], Loss: 1.1767\n",
      "Epoch [1/8], Step [114/179], Loss: 1.3575\n",
      "Epoch [1/8], Step [115/179], Loss: 1.2364\n",
      "Epoch [1/8], Step [116/179], Loss: 1.1477\n",
      "Epoch [1/8], Step [117/179], Loss: 1.3163\n",
      "Epoch [1/8], Step [118/179], Loss: 1.1602\n",
      "Epoch [1/8], Step [119/179], Loss: 1.1498\n",
      "Epoch [1/8], Step [120/179], Loss: 1.1001\n",
      "Epoch [1/8], Step [121/179], Loss: 1.1550\n",
      "Epoch [1/8], Step [122/179], Loss: 1.1310\n",
      "Epoch [1/8], Step [123/179], Loss: 1.1380\n",
      "Epoch [1/8], Step [124/179], Loss: 1.1265\n",
      "Epoch [1/8], Step [125/179], Loss: 1.2014\n",
      "Epoch [1/8], Step [126/179], Loss: 1.1562\n",
      "Epoch [1/8], Step [127/179], Loss: 1.0780\n",
      "Epoch [1/8], Step [128/179], Loss: 1.2713\n",
      "Epoch [1/8], Step [129/179], Loss: 1.0070\n",
      "Epoch [1/8], Step [130/179], Loss: 1.0628\n",
      "Epoch [1/8], Step [131/179], Loss: 1.0759\n",
      "Epoch [1/8], Step [132/179], Loss: 1.3140\n",
      "Epoch [1/8], Step [133/179], Loss: 1.1379\n",
      "Epoch [1/8], Step [134/179], Loss: 1.0444\n",
      "Epoch [1/8], Step [135/179], Loss: 1.1346\n",
      "Epoch [1/8], Step [136/179], Loss: 1.1340\n",
      "Epoch [1/8], Step [137/179], Loss: 1.0156\n",
      "Epoch [1/8], Step [138/179], Loss: 0.8616\n",
      "Epoch [1/8], Step [139/179], Loss: 1.0516\n",
      "Epoch [1/8], Step [140/179], Loss: 0.8996\n",
      "Epoch [1/8], Step [141/179], Loss: 1.1326\n",
      "Epoch [1/8], Step [142/179], Loss: 0.8834\n",
      "Epoch [1/8], Step [143/179], Loss: 1.3732\n",
      "Epoch [1/8], Step [144/179], Loss: 1.3569\n",
      "Epoch [1/8], Step [145/179], Loss: 1.1414\n",
      "Epoch [1/8], Step [146/179], Loss: 1.0613\n",
      "Epoch [1/8], Step [147/179], Loss: 0.9529\n",
      "Epoch [1/8], Step [148/179], Loss: 1.1371\n",
      "Epoch [1/8], Step [149/179], Loss: 1.1727\n",
      "Epoch [1/8], Step [150/179], Loss: 1.1160\n",
      "Epoch [1/8], Step [151/179], Loss: 1.1047\n",
      "Epoch [1/8], Step [152/179], Loss: 1.0907\n",
      "Epoch [1/8], Step [153/179], Loss: 1.1106\n",
      "Epoch [1/8], Step [154/179], Loss: 0.8755\n",
      "Epoch [1/8], Step [155/179], Loss: 0.8675\n",
      "Epoch [1/8], Step [156/179], Loss: 0.7501\n",
      "Epoch [1/8], Step [157/179], Loss: 0.8225\n",
      "Epoch [1/8], Step [158/179], Loss: 1.2983\n",
      "Epoch [1/8], Step [159/179], Loss: 0.8335\n",
      "Epoch [1/8], Step [160/179], Loss: 0.9467\n",
      "Epoch [1/8], Step [161/179], Loss: 0.6935\n",
      "Epoch [1/8], Step [162/179], Loss: 0.8598\n",
      "Epoch [1/8], Step [163/179], Loss: 0.8540\n",
      "Epoch [1/8], Step [164/179], Loss: 0.8426\n",
      "Epoch [1/8], Step [165/179], Loss: 0.9344\n",
      "Epoch [1/8], Step [166/179], Loss: 1.0363\n",
      "Epoch [1/8], Step [167/179], Loss: 0.7926\n",
      "Epoch [1/8], Step [168/179], Loss: 0.8389\n",
      "Epoch [1/8], Step [169/179], Loss: 0.9175\n",
      "Epoch [1/8], Step [170/179], Loss: 0.7313\n",
      "Epoch [1/8], Step [171/179], Loss: 0.9337\n",
      "Epoch [1/8], Step [172/179], Loss: 0.8208\n",
      "Epoch [1/8], Step [173/179], Loss: 0.6181\n",
      "Epoch [1/8], Step [174/179], Loss: 1.0250\n",
      "Epoch [1/8], Step [175/179], Loss: 0.8196\n",
      "Epoch [1/8], Step [176/179], Loss: 0.8924\n",
      "Epoch [1/8], Step [177/179], Loss: 0.6418\n",
      "Epoch [1/8], Step [178/179], Loss: 0.6896\n",
      "Epoch [1/8], Step [179/179], Loss: 0.9217\n",
      "Epoch [2/8], Step [1/179], Loss: 0.7626\n",
      "Epoch [2/8], Step [2/179], Loss: 0.9360\n",
      "Epoch [2/8], Step [3/179], Loss: 0.9887\n",
      "Epoch [2/8], Step [4/179], Loss: 0.9372\n",
      "Epoch [2/8], Step [5/179], Loss: 0.8337\n",
      "Epoch [2/8], Step [6/179], Loss: 0.9549\n",
      "Epoch [2/8], Step [7/179], Loss: 0.6623\n",
      "Epoch [2/8], Step [8/179], Loss: 0.8099\n",
      "Epoch [2/8], Step [9/179], Loss: 0.6300\n",
      "Epoch [2/8], Step [10/179], Loss: 0.9148\n",
      "Epoch [2/8], Step [11/179], Loss: 0.8962\n",
      "Epoch [2/8], Step [12/179], Loss: 0.5227\n",
      "Epoch [2/8], Step [13/179], Loss: 1.0464\n",
      "Epoch [2/8], Step [14/179], Loss: 0.7504\n",
      "Epoch [2/8], Step [15/179], Loss: 0.9087\n",
      "Epoch [2/8], Step [16/179], Loss: 0.7603\n",
      "Epoch [2/8], Step [17/179], Loss: 0.6938\n",
      "Epoch [2/8], Step [18/179], Loss: 0.7535\n",
      "Epoch [2/8], Step [19/179], Loss: 0.8157\n",
      "Epoch [2/8], Step [20/179], Loss: 0.6324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [21/179], Loss: 0.8937\n",
      "Epoch [2/8], Step [22/179], Loss: 0.7515\n",
      "Epoch [2/8], Step [23/179], Loss: 0.8536\n",
      "Epoch [2/8], Step [24/179], Loss: 0.4893\n",
      "Epoch [2/8], Step [25/179], Loss: 0.6791\n",
      "Epoch [2/8], Step [26/179], Loss: 1.2576\n",
      "Epoch [2/8], Step [27/179], Loss: 0.7515\n",
      "Epoch [2/8], Step [28/179], Loss: 0.8405\n",
      "Epoch [2/8], Step [29/179], Loss: 0.6372\n",
      "Epoch [2/8], Step [30/179], Loss: 0.9060\n",
      "Epoch [2/8], Step [31/179], Loss: 0.7743\n",
      "Epoch [2/8], Step [32/179], Loss: 0.6878\n",
      "Epoch [2/8], Step [33/179], Loss: 0.6573\n",
      "Epoch [2/8], Step [34/179], Loss: 0.8402\n",
      "Epoch [2/8], Step [35/179], Loss: 0.5992\n",
      "Epoch [2/8], Step [36/179], Loss: 1.0711\n",
      "Epoch [2/8], Step [37/179], Loss: 0.5836\n",
      "Epoch [2/8], Step [38/179], Loss: 1.0674\n",
      "Epoch [2/8], Step [39/179], Loss: 0.7092\n",
      "Epoch [2/8], Step [40/179], Loss: 0.6228\n",
      "Epoch [2/8], Step [41/179], Loss: 0.9498\n",
      "Epoch [2/8], Step [42/179], Loss: 0.4875\n",
      "Epoch [2/8], Step [43/179], Loss: 0.8014\n",
      "Epoch [2/8], Step [44/179], Loss: 0.7055\n",
      "Epoch [2/8], Step [45/179], Loss: 0.6986\n",
      "Epoch [2/8], Step [46/179], Loss: 1.2088\n",
      "Epoch [2/8], Step [47/179], Loss: 0.4844\n",
      "Epoch [2/8], Step [48/179], Loss: 0.8240\n",
      "Epoch [2/8], Step [49/179], Loss: 0.8762\n",
      "Epoch [2/8], Step [50/179], Loss: 0.7382\n",
      "Epoch [2/8], Step [51/179], Loss: 0.7412\n",
      "Epoch [2/8], Step [52/179], Loss: 0.6922\n",
      "Epoch [2/8], Step [53/179], Loss: 1.0210\n",
      "Epoch [2/8], Step [54/179], Loss: 0.7136\n",
      "Epoch [2/8], Step [55/179], Loss: 0.5237\n",
      "Epoch [2/8], Step [56/179], Loss: 0.7230\n",
      "Epoch [2/8], Step [57/179], Loss: 0.6864\n",
      "Epoch [2/8], Step [58/179], Loss: 0.6791\n",
      "Epoch [2/8], Step [59/179], Loss: 0.6260\n",
      "Epoch [2/8], Step [60/179], Loss: 0.5136\n",
      "Epoch [2/8], Step [61/179], Loss: 0.7658\n",
      "Epoch [2/8], Step [62/179], Loss: 0.6559\n",
      "Epoch [2/8], Step [63/179], Loss: 0.5540\n",
      "Epoch [2/8], Step [64/179], Loss: 0.9946\n",
      "Epoch [2/8], Step [65/179], Loss: 0.7421\n",
      "Epoch [2/8], Step [66/179], Loss: 0.7446\n",
      "Epoch [2/8], Step [67/179], Loss: 0.5125\n",
      "Epoch [2/8], Step [68/179], Loss: 1.3041\n",
      "Epoch [2/8], Step [69/179], Loss: 1.0177\n",
      "Epoch [2/8], Step [70/179], Loss: 0.6175\n",
      "Epoch [2/8], Step [71/179], Loss: 0.8863\n",
      "Epoch [2/8], Step [72/179], Loss: 1.1288\n",
      "Epoch [2/8], Step [73/179], Loss: 0.9910\n",
      "Epoch [2/8], Step [74/179], Loss: 0.7089\n",
      "Epoch [2/8], Step [75/179], Loss: 0.9231\n",
      "Epoch [2/8], Step [76/179], Loss: 0.8612\n",
      "Epoch [2/8], Step [77/179], Loss: 0.9494\n",
      "Epoch [2/8], Step [78/179], Loss: 0.8618\n",
      "Epoch [2/8], Step [79/179], Loss: 0.7666\n",
      "Epoch [2/8], Step [80/179], Loss: 0.6554\n",
      "Epoch [2/8], Step [81/179], Loss: 0.7782\n",
      "Epoch [2/8], Step [82/179], Loss: 0.8625\n",
      "Epoch [2/8], Step [83/179], Loss: 0.6192\n",
      "Epoch [2/8], Step [84/179], Loss: 0.5065\n",
      "Epoch [2/8], Step [85/179], Loss: 0.6353\n",
      "Epoch [2/8], Step [86/179], Loss: 0.5515\n",
      "Epoch [2/8], Step [87/179], Loss: 0.8435\n",
      "Epoch [2/8], Step [88/179], Loss: 0.7776\n",
      "Epoch [2/8], Step [89/179], Loss: 0.9200\n",
      "Epoch [2/8], Step [90/179], Loss: 0.7719\n",
      "Epoch [2/8], Step [91/179], Loss: 0.8307\n",
      "Epoch [2/8], Step [92/179], Loss: 0.6644\n",
      "Epoch [2/8], Step [93/179], Loss: 0.7232\n",
      "Epoch [2/8], Step [94/179], Loss: 0.8355\n",
      "Epoch [2/8], Step [95/179], Loss: 0.9167\n",
      "Epoch [2/8], Step [96/179], Loss: 0.7916\n",
      "Epoch [2/8], Step [97/179], Loss: 0.5883\n",
      "Epoch [2/8], Step [98/179], Loss: 0.4982\n",
      "Epoch [2/8], Step [99/179], Loss: 0.8801\n",
      "Epoch [2/8], Step [100/179], Loss: 0.5997\n",
      "Epoch [2/8], Step [101/179], Loss: 0.6023\n",
      "Epoch [2/8], Step [102/179], Loss: 0.7626\n",
      "Epoch [2/8], Step [103/179], Loss: 0.7267\n",
      "Epoch [2/8], Step [104/179], Loss: 0.7466\n",
      "Epoch [2/8], Step [105/179], Loss: 0.6839\n",
      "Epoch [2/8], Step [106/179], Loss: 0.7144\n",
      "Epoch [2/8], Step [107/179], Loss: 0.9030\n",
      "Epoch [2/8], Step [108/179], Loss: 0.5219\n",
      "Epoch [2/8], Step [109/179], Loss: 0.6580\n",
      "Epoch [2/8], Step [110/179], Loss: 0.8262\n",
      "Epoch [2/8], Step [111/179], Loss: 0.6708\n",
      "Epoch [2/8], Step [112/179], Loss: 0.9478\n",
      "Epoch [2/8], Step [113/179], Loss: 0.8661\n",
      "Epoch [2/8], Step [114/179], Loss: 0.6899\n",
      "Epoch [2/8], Step [115/179], Loss: 0.6537\n",
      "Epoch [2/8], Step [116/179], Loss: 0.8401\n",
      "Epoch [2/8], Step [117/179], Loss: 0.4361\n",
      "Epoch [2/8], Step [118/179], Loss: 0.7293\n",
      "Epoch [2/8], Step [119/179], Loss: 0.5771\n",
      "Epoch [2/8], Step [120/179], Loss: 0.7604\n",
      "Epoch [2/8], Step [121/179], Loss: 0.4517\n",
      "Epoch [2/8], Step [122/179], Loss: 0.9001\n",
      "Epoch [2/8], Step [123/179], Loss: 0.8019\n",
      "Epoch [2/8], Step [124/179], Loss: 0.7677\n",
      "Epoch [2/8], Step [125/179], Loss: 1.0449\n",
      "Epoch [2/8], Step [126/179], Loss: 0.6193\n",
      "Epoch [2/8], Step [127/179], Loss: 0.6851\n",
      "Epoch [2/8], Step [128/179], Loss: 0.7561\n",
      "Epoch [2/8], Step [129/179], Loss: 0.5203\n",
      "Epoch [2/8], Step [130/179], Loss: 0.7210\n",
      "Epoch [2/8], Step [131/179], Loss: 0.4980\n",
      "Epoch [2/8], Step [132/179], Loss: 0.5426\n",
      "Epoch [2/8], Step [133/179], Loss: 0.7466\n",
      "Epoch [2/8], Step [134/179], Loss: 0.8159\n",
      "Epoch [2/8], Step [135/179], Loss: 0.4253\n",
      "Epoch [2/8], Step [136/179], Loss: 0.9997\n",
      "Epoch [2/8], Step [137/179], Loss: 0.6672\n",
      "Epoch [2/8], Step [138/179], Loss: 0.5829\n",
      "Epoch [2/8], Step [139/179], Loss: 0.8208\n",
      "Epoch [2/8], Step [140/179], Loss: 0.6804\n",
      "Epoch [2/8], Step [141/179], Loss: 1.4997\n",
      "Epoch [2/8], Step [142/179], Loss: 0.6981\n",
      "Epoch [2/8], Step [143/179], Loss: 0.7684\n",
      "Epoch [2/8], Step [144/179], Loss: 0.7328\n",
      "Epoch [2/8], Step [145/179], Loss: 0.7895\n",
      "Epoch [2/8], Step [146/179], Loss: 1.1999\n",
      "Epoch [2/8], Step [147/179], Loss: 0.9056\n",
      "Epoch [2/8], Step [148/179], Loss: 0.7187\n",
      "Epoch [2/8], Step [149/179], Loss: 0.4838\n",
      "Epoch [2/8], Step [150/179], Loss: 0.6487\n",
      "Epoch [2/8], Step [151/179], Loss: 0.8138\n",
      "Epoch [2/8], Step [152/179], Loss: 0.4953\n",
      "Epoch [2/8], Step [153/179], Loss: 0.6447\n",
      "Epoch [2/8], Step [154/179], Loss: 0.8895\n",
      "Epoch [2/8], Step [155/179], Loss: 0.8015\n",
      "Epoch [2/8], Step [156/179], Loss: 0.7883\n",
      "Epoch [2/8], Step [157/179], Loss: 0.6512\n",
      "Epoch [2/8], Step [158/179], Loss: 0.9819\n",
      "Epoch [2/8], Step [159/179], Loss: 0.8345\n",
      "Epoch [2/8], Step [160/179], Loss: 0.3924\n",
      "Epoch [2/8], Step [161/179], Loss: 0.7111\n",
      "Epoch [2/8], Step [162/179], Loss: 0.6915\n",
      "Epoch [2/8], Step [163/179], Loss: 0.6926\n",
      "Epoch [2/8], Step [164/179], Loss: 0.7172\n",
      "Epoch [2/8], Step [165/179], Loss: 0.7976\n",
      "Epoch [2/8], Step [166/179], Loss: 0.9037\n",
      "Epoch [2/8], Step [167/179], Loss: 0.5207\n",
      "Epoch [2/8], Step [168/179], Loss: 0.8168\n",
      "Epoch [2/8], Step [169/179], Loss: 0.5422\n",
      "Epoch [2/8], Step [170/179], Loss: 0.6086\n",
      "Epoch [2/8], Step [171/179], Loss: 0.7818\n",
      "Epoch [2/8], Step [172/179], Loss: 0.8187\n",
      "Epoch [2/8], Step [173/179], Loss: 0.6161\n",
      "Epoch [2/8], Step [174/179], Loss: 0.7871\n",
      "Epoch [2/8], Step [175/179], Loss: 0.5894\n",
      "Epoch [2/8], Step [176/179], Loss: 1.1298\n",
      "Epoch [2/8], Step [177/179], Loss: 0.7506\n",
      "Epoch [2/8], Step [178/179], Loss: 0.7620\n",
      "Epoch [2/8], Step [179/179], Loss: 0.6968\n",
      "Epoch [3/8], Step [1/179], Loss: 0.6052\n",
      "Epoch [3/8], Step [2/179], Loss: 0.6989\n",
      "Epoch [3/8], Step [3/179], Loss: 0.8728\n",
      "Epoch [3/8], Step [4/179], Loss: 0.6757\n",
      "Epoch [3/8], Step [5/179], Loss: 0.4784\n",
      "Epoch [3/8], Step [6/179], Loss: 0.6150\n",
      "Epoch [3/8], Step [7/179], Loss: 0.4485\n",
      "Epoch [3/8], Step [8/179], Loss: 0.4478\n",
      "Epoch [3/8], Step [9/179], Loss: 0.4720\n",
      "Epoch [3/8], Step [10/179], Loss: 0.6090\n",
      "Epoch [3/8], Step [11/179], Loss: 0.5981\n",
      "Epoch [3/8], Step [12/179], Loss: 0.8265\n",
      "Epoch [3/8], Step [13/179], Loss: 0.9178\n",
      "Epoch [3/8], Step [14/179], Loss: 0.3880\n",
      "Epoch [3/8], Step [15/179], Loss: 0.7518\n",
      "Epoch [3/8], Step [16/179], Loss: 0.5783\n",
      "Epoch [3/8], Step [17/179], Loss: 0.5358\n",
      "Epoch [3/8], Step [18/179], Loss: 0.5493\n",
      "Epoch [3/8], Step [19/179], Loss: 0.6590\n",
      "Epoch [3/8], Step [20/179], Loss: 0.4389\n",
      "Epoch [3/8], Step [21/179], Loss: 0.4673\n",
      "Epoch [3/8], Step [22/179], Loss: 0.6831\n",
      "Epoch [3/8], Step [23/179], Loss: 0.5518\n",
      "Epoch [3/8], Step [24/179], Loss: 0.7328\n",
      "Epoch [3/8], Step [25/179], Loss: 0.4472\n",
      "Epoch [3/8], Step [26/179], Loss: 0.8869\n",
      "Epoch [3/8], Step [27/179], Loss: 0.6586\n",
      "Epoch [3/8], Step [28/179], Loss: 0.7143\n",
      "Epoch [3/8], Step [29/179], Loss: 0.5703\n",
      "Epoch [3/8], Step [30/179], Loss: 0.7107\n",
      "Epoch [3/8], Step [31/179], Loss: 0.4816\n",
      "Epoch [3/8], Step [32/179], Loss: 0.8068\n",
      "Epoch [3/8], Step [33/179], Loss: 0.9270\n",
      "Epoch [3/8], Step [34/179], Loss: 0.4724\n",
      "Epoch [3/8], Step [35/179], Loss: 0.5651\n",
      "Epoch [3/8], Step [36/179], Loss: 0.4811\n",
      "Epoch [3/8], Step [37/179], Loss: 0.6031\n",
      "Epoch [3/8], Step [38/179], Loss: 0.5564\n",
      "Epoch [3/8], Step [39/179], Loss: 0.6353\n",
      "Epoch [3/8], Step [40/179], Loss: 0.7444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [41/179], Loss: 0.8215\n",
      "Epoch [3/8], Step [42/179], Loss: 0.6295\n",
      "Epoch [3/8], Step [43/179], Loss: 0.6732\n",
      "Epoch [3/8], Step [44/179], Loss: 0.5894\n",
      "Epoch [3/8], Step [45/179], Loss: 0.6500\n",
      "Epoch [3/8], Step [46/179], Loss: 0.5424\n",
      "Epoch [3/8], Step [47/179], Loss: 0.9113\n",
      "Epoch [3/8], Step [48/179], Loss: 0.3971\n",
      "Epoch [3/8], Step [49/179], Loss: 0.4494\n",
      "Epoch [3/8], Step [50/179], Loss: 1.0244\n",
      "Epoch [3/8], Step [51/179], Loss: 0.6712\n",
      "Epoch [3/8], Step [52/179], Loss: 0.6177\n",
      "Epoch [3/8], Step [53/179], Loss: 0.9033\n",
      "Epoch [3/8], Step [54/179], Loss: 0.5850\n",
      "Epoch [3/8], Step [55/179], Loss: 0.6090\n",
      "Epoch [3/8], Step [56/179], Loss: 0.6678\n",
      "Epoch [3/8], Step [57/179], Loss: 0.5430\n",
      "Epoch [3/8], Step [58/179], Loss: 0.7261\n",
      "Epoch [3/8], Step [59/179], Loss: 0.7069\n",
      "Epoch [3/8], Step [60/179], Loss: 0.4610\n",
      "Epoch [3/8], Step [61/179], Loss: 0.3903\n",
      "Epoch [3/8], Step [62/179], Loss: 0.6378\n",
      "Epoch [3/8], Step [63/179], Loss: 0.5400\n",
      "Epoch [3/8], Step [64/179], Loss: 0.5586\n",
      "Epoch [3/8], Step [65/179], Loss: 0.6417\n",
      "Epoch [3/8], Step [66/179], Loss: 0.6267\n",
      "Epoch [3/8], Step [67/179], Loss: 0.6037\n",
      "Epoch [3/8], Step [68/179], Loss: 0.6232\n",
      "Epoch [3/8], Step [69/179], Loss: 0.7985\n",
      "Epoch [3/8], Step [70/179], Loss: 0.5291\n",
      "Epoch [3/8], Step [71/179], Loss: 0.3584\n",
      "Epoch [3/8], Step [72/179], Loss: 0.5687\n",
      "Epoch [3/8], Step [73/179], Loss: 0.4737\n",
      "Epoch [3/8], Step [74/179], Loss: 0.6669\n",
      "Epoch [3/8], Step [75/179], Loss: 0.6419\n",
      "Epoch [3/8], Step [76/179], Loss: 0.5977\n",
      "Epoch [3/8], Step [77/179], Loss: 0.7200\n",
      "Epoch [3/8], Step [78/179], Loss: 0.5218\n",
      "Epoch [3/8], Step [79/179], Loss: 0.9297\n",
      "Epoch [3/8], Step [80/179], Loss: 0.5928\n",
      "Epoch [3/8], Step [81/179], Loss: 0.4663\n",
      "Epoch [3/8], Step [82/179], Loss: 0.4977\n",
      "Epoch [3/8], Step [83/179], Loss: 0.4894\n",
      "Epoch [3/8], Step [84/179], Loss: 0.9310\n",
      "Epoch [3/8], Step [85/179], Loss: 0.7251\n",
      "Epoch [3/8], Step [86/179], Loss: 0.5652\n",
      "Epoch [3/8], Step [87/179], Loss: 0.3428\n",
      "Epoch [3/8], Step [88/179], Loss: 0.4467\n",
      "Epoch [3/8], Step [89/179], Loss: 0.6039\n",
      "Epoch [3/8], Step [90/179], Loss: 0.3768\n",
      "Epoch [3/8], Step [91/179], Loss: 0.4378\n",
      "Epoch [3/8], Step [92/179], Loss: 0.6803\n",
      "Epoch [3/8], Step [93/179], Loss: 0.5074\n",
      "Epoch [3/8], Step [94/179], Loss: 0.8528\n",
      "Epoch [3/8], Step [95/179], Loss: 0.8005\n",
      "Epoch [3/8], Step [96/179], Loss: 0.8050\n",
      "Epoch [3/8], Step [97/179], Loss: 0.6944\n",
      "Epoch [3/8], Step [98/179], Loss: 1.0088\n",
      "Epoch [3/8], Step [99/179], Loss: 0.6853\n",
      "Epoch [3/8], Step [100/179], Loss: 0.6938\n",
      "Epoch [3/8], Step [101/179], Loss: 0.5381\n",
      "Epoch [3/8], Step [102/179], Loss: 0.5617\n",
      "Epoch [3/8], Step [103/179], Loss: 0.6704\n",
      "Epoch [3/8], Step [104/179], Loss: 0.4118\n",
      "Epoch [3/8], Step [105/179], Loss: 0.7197\n",
      "Epoch [3/8], Step [106/179], Loss: 0.7483\n",
      "Epoch [3/8], Step [107/179], Loss: 0.8888\n",
      "Epoch [3/8], Step [108/179], Loss: 0.7047\n",
      "Epoch [3/8], Step [109/179], Loss: 0.6190\n",
      "Epoch [3/8], Step [110/179], Loss: 0.7614\n",
      "Epoch [3/8], Step [111/179], Loss: 0.7755\n",
      "Epoch [3/8], Step [112/179], Loss: 0.7623\n",
      "Epoch [3/8], Step [113/179], Loss: 1.2243\n",
      "Epoch [3/8], Step [114/179], Loss: 0.4585\n",
      "Epoch [3/8], Step [115/179], Loss: 0.7401\n",
      "Epoch [3/8], Step [116/179], Loss: 0.9670\n",
      "Epoch [3/8], Step [117/179], Loss: 0.7047\n",
      "Epoch [3/8], Step [118/179], Loss: 0.5130\n",
      "Epoch [3/8], Step [119/179], Loss: 0.6553\n",
      "Epoch [3/8], Step [120/179], Loss: 0.6589\n",
      "Epoch [3/8], Step [121/179], Loss: 0.5364\n",
      "Epoch [3/8], Step [122/179], Loss: 0.5000\n",
      "Epoch [3/8], Step [123/179], Loss: 0.6680\n",
      "Epoch [3/8], Step [124/179], Loss: 0.5249\n",
      "Epoch [3/8], Step [125/179], Loss: 0.6623\n",
      "Epoch [3/8], Step [126/179], Loss: 0.9331\n",
      "Epoch [3/8], Step [127/179], Loss: 0.6495\n",
      "Epoch [3/8], Step [128/179], Loss: 0.6105\n",
      "Epoch [3/8], Step [129/179], Loss: 0.6451\n",
      "Epoch [3/8], Step [130/179], Loss: 0.8673\n",
      "Epoch [3/8], Step [131/179], Loss: 0.4238\n",
      "Epoch [3/8], Step [132/179], Loss: 0.5869\n",
      "Epoch [3/8], Step [133/179], Loss: 0.8670\n",
      "Epoch [3/8], Step [134/179], Loss: 0.7956\n",
      "Epoch [3/8], Step [135/179], Loss: 0.9417\n",
      "Epoch [3/8], Step [136/179], Loss: 0.5766\n",
      "Epoch [3/8], Step [137/179], Loss: 0.7438\n",
      "Epoch [3/8], Step [138/179], Loss: 0.4820\n",
      "Epoch [3/8], Step [139/179], Loss: 0.5988\n",
      "Epoch [3/8], Step [140/179], Loss: 0.6256\n",
      "Epoch [3/8], Step [141/179], Loss: 0.5702\n",
      "Epoch [3/8], Step [142/179], Loss: 0.7152\n",
      "Epoch [3/8], Step [143/179], Loss: 0.5476\n",
      "Epoch [3/8], Step [144/179], Loss: 0.8022\n",
      "Epoch [3/8], Step [145/179], Loss: 0.4275\n",
      "Epoch [3/8], Step [146/179], Loss: 0.6573\n",
      "Epoch [3/8], Step [147/179], Loss: 0.5100\n",
      "Epoch [3/8], Step [148/179], Loss: 0.5315\n",
      "Epoch [3/8], Step [149/179], Loss: 0.3871\n",
      "Epoch [3/8], Step [150/179], Loss: 0.4945\n",
      "Epoch [3/8], Step [151/179], Loss: 0.9511\n",
      "Epoch [3/8], Step [152/179], Loss: 0.4577\n",
      "Epoch [3/8], Step [153/179], Loss: 1.1560\n",
      "Epoch [3/8], Step [154/179], Loss: 0.5871\n",
      "Epoch [3/8], Step [155/179], Loss: 0.4635\n",
      "Epoch [3/8], Step [156/179], Loss: 0.7081\n",
      "Epoch [3/8], Step [157/179], Loss: 0.4024\n",
      "Epoch [3/8], Step [158/179], Loss: 0.6811\n",
      "Epoch [3/8], Step [159/179], Loss: 0.7668\n",
      "Epoch [3/8], Step [160/179], Loss: 0.4277\n",
      "Epoch [3/8], Step [161/179], Loss: 0.3513\n",
      "Epoch [3/8], Step [162/179], Loss: 0.4171\n",
      "Epoch [3/8], Step [163/179], Loss: 0.5222\n",
      "Epoch [3/8], Step [164/179], Loss: 0.9258\n",
      "Epoch [3/8], Step [165/179], Loss: 0.5236\n",
      "Epoch [3/8], Step [166/179], Loss: 0.4909\n",
      "Epoch [3/8], Step [167/179], Loss: 0.5712\n",
      "Epoch [3/8], Step [168/179], Loss: 0.8222\n",
      "Epoch [3/8], Step [169/179], Loss: 0.7259\n",
      "Epoch [3/8], Step [170/179], Loss: 0.7834\n",
      "Epoch [3/8], Step [171/179], Loss: 0.5663\n",
      "Epoch [3/8], Step [172/179], Loss: 0.6444\n",
      "Epoch [3/8], Step [173/179], Loss: 0.3401\n",
      "Epoch [3/8], Step [174/179], Loss: 0.4807\n",
      "Epoch [3/8], Step [175/179], Loss: 0.8026\n",
      "Epoch [3/8], Step [176/179], Loss: 0.5488\n",
      "Epoch [3/8], Step [177/179], Loss: 0.7994\n",
      "Epoch [3/8], Step [178/179], Loss: 0.4321\n",
      "Epoch [3/8], Step [179/179], Loss: 1.0114\n",
      "Epoch [4/8], Step [1/179], Loss: 0.5903\n",
      "Epoch [4/8], Step [2/179], Loss: 0.7173\n",
      "Epoch [4/8], Step [3/179], Loss: 0.4215\n",
      "Epoch [4/8], Step [4/179], Loss: 0.4654\n",
      "Epoch [4/8], Step [5/179], Loss: 0.4124\n",
      "Epoch [4/8], Step [6/179], Loss: 0.7064\n",
      "Epoch [4/8], Step [7/179], Loss: 0.5475\n",
      "Epoch [4/8], Step [8/179], Loss: 0.5600\n",
      "Epoch [4/8], Step [9/179], Loss: 0.5653\n",
      "Epoch [4/8], Step [10/179], Loss: 0.4218\n",
      "Epoch [4/8], Step [11/179], Loss: 1.1316\n",
      "Epoch [4/8], Step [12/179], Loss: 0.5272\n",
      "Epoch [4/8], Step [13/179], Loss: 0.3825\n",
      "Epoch [4/8], Step [14/179], Loss: 0.6251\n",
      "Epoch [4/8], Step [15/179], Loss: 0.7513\n",
      "Epoch [4/8], Step [16/179], Loss: 1.2579\n",
      "Epoch [4/8], Step [17/179], Loss: 0.6208\n",
      "Epoch [4/8], Step [18/179], Loss: 0.7799\n",
      "Epoch [4/8], Step [19/179], Loss: 0.2650\n",
      "Epoch [4/8], Step [20/179], Loss: 0.3941\n",
      "Epoch [4/8], Step [21/179], Loss: 0.4594\n",
      "Epoch [4/8], Step [22/179], Loss: 0.8942\n",
      "Epoch [4/8], Step [23/179], Loss: 0.5372\n",
      "Epoch [4/8], Step [24/179], Loss: 0.3445\n",
      "Epoch [4/8], Step [25/179], Loss: 0.6437\n",
      "Epoch [4/8], Step [26/179], Loss: 0.3696\n",
      "Epoch [4/8], Step [27/179], Loss: 0.3877\n",
      "Epoch [4/8], Step [28/179], Loss: 0.4929\n",
      "Epoch [4/8], Step [29/179], Loss: 0.4780\n",
      "Epoch [4/8], Step [30/179], Loss: 0.6088\n",
      "Epoch [4/8], Step [31/179], Loss: 0.5031\n",
      "Epoch [4/8], Step [32/179], Loss: 0.6115\n",
      "Epoch [4/8], Step [33/179], Loss: 0.6105\n",
      "Epoch [4/8], Step [34/179], Loss: 0.4704\n",
      "Epoch [4/8], Step [35/179], Loss: 0.7660\n",
      "Epoch [4/8], Step [36/179], Loss: 0.5337\n",
      "Epoch [4/8], Step [37/179], Loss: 0.4514\n",
      "Epoch [4/8], Step [38/179], Loss: 0.4578\n",
      "Epoch [4/8], Step [39/179], Loss: 0.6361\n",
      "Epoch [4/8], Step [40/179], Loss: 0.4889\n",
      "Epoch [4/8], Step [41/179], Loss: 0.6126\n",
      "Epoch [4/8], Step [42/179], Loss: 0.5305\n",
      "Epoch [4/8], Step [43/179], Loss: 0.6420\n",
      "Epoch [4/8], Step [44/179], Loss: 0.5132\n",
      "Epoch [4/8], Step [45/179], Loss: 0.3105\n",
      "Epoch [4/8], Step [46/179], Loss: 1.2237\n",
      "Epoch [4/8], Step [47/179], Loss: 0.4554\n",
      "Epoch [4/8], Step [48/179], Loss: 0.3828\n",
      "Epoch [4/8], Step [49/179], Loss: 0.5573\n",
      "Epoch [4/8], Step [50/179], Loss: 0.8303\n",
      "Epoch [4/8], Step [51/179], Loss: 0.7465\n",
      "Epoch [4/8], Step [52/179], Loss: 0.4126\n",
      "Epoch [4/8], Step [53/179], Loss: 0.5672\n",
      "Epoch [4/8], Step [54/179], Loss: 0.4523\n",
      "Epoch [4/8], Step [55/179], Loss: 1.0118\n",
      "Epoch [4/8], Step [56/179], Loss: 0.4757\n",
      "Epoch [4/8], Step [57/179], Loss: 0.8279\n",
      "Epoch [4/8], Step [58/179], Loss: 0.8317\n",
      "Epoch [4/8], Step [59/179], Loss: 0.5209\n",
      "Epoch [4/8], Step [60/179], Loss: 1.0046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [61/179], Loss: 0.5251\n",
      "Epoch [4/8], Step [62/179], Loss: 0.5258\n",
      "Epoch [4/8], Step [63/179], Loss: 0.3807\n",
      "Epoch [4/8], Step [64/179], Loss: 0.5636\n",
      "Epoch [4/8], Step [65/179], Loss: 0.3979\n",
      "Epoch [4/8], Step [66/179], Loss: 0.3844\n",
      "Epoch [4/8], Step [67/179], Loss: 0.4191\n",
      "Epoch [4/8], Step [68/179], Loss: 1.0587\n",
      "Epoch [4/8], Step [69/179], Loss: 0.5553\n",
      "Epoch [4/8], Step [70/179], Loss: 0.6206\n",
      "Epoch [4/8], Step [71/179], Loss: 0.5290\n",
      "Epoch [4/8], Step [72/179], Loss: 0.2910\n",
      "Epoch [4/8], Step [73/179], Loss: 0.7820\n",
      "Epoch [4/8], Step [74/179], Loss: 0.5233\n",
      "Epoch [4/8], Step [75/179], Loss: 0.5872\n",
      "Epoch [4/8], Step [76/179], Loss: 0.9603\n",
      "Epoch [4/8], Step [77/179], Loss: 0.6757\n",
      "Epoch [4/8], Step [78/179], Loss: 0.5111\n",
      "Epoch [4/8], Step [79/179], Loss: 0.3965\n",
      "Epoch [4/8], Step [80/179], Loss: 0.2799\n",
      "Epoch [4/8], Step [81/179], Loss: 0.5548\n",
      "Epoch [4/8], Step [82/179], Loss: 0.6378\n",
      "Epoch [4/8], Step [83/179], Loss: 0.5747\n",
      "Epoch [4/8], Step [84/179], Loss: 0.5167\n",
      "Epoch [4/8], Step [85/179], Loss: 0.4288\n",
      "Epoch [4/8], Step [86/179], Loss: 0.4844\n",
      "Epoch [4/8], Step [87/179], Loss: 0.7471\n",
      "Epoch [4/8], Step [88/179], Loss: 0.3503\n",
      "Epoch [4/8], Step [89/179], Loss: 0.9175\n",
      "Epoch [4/8], Step [90/179], Loss: 0.9697\n",
      "Epoch [4/8], Step [91/179], Loss: 0.6039\n",
      "Epoch [4/8], Step [92/179], Loss: 0.4993\n",
      "Epoch [4/8], Step [93/179], Loss: 0.5978\n",
      "Epoch [4/8], Step [94/179], Loss: 0.4849\n",
      "Epoch [4/8], Step [95/179], Loss: 0.6118\n",
      "Epoch [4/8], Step [96/179], Loss: 0.3871\n",
      "Epoch [4/8], Step [97/179], Loss: 0.4629\n",
      "Epoch [4/8], Step [98/179], Loss: 0.6340\n",
      "Epoch [4/8], Step [99/179], Loss: 0.7460\n",
      "Epoch [4/8], Step [100/179], Loss: 0.5588\n",
      "Epoch [4/8], Step [101/179], Loss: 0.4049\n",
      "Epoch [4/8], Step [102/179], Loss: 0.7702\n",
      "Epoch [4/8], Step [103/179], Loss: 0.5211\n",
      "Epoch [4/8], Step [104/179], Loss: 0.5012\n",
      "Epoch [4/8], Step [105/179], Loss: 0.4230\n",
      "Epoch [4/8], Step [106/179], Loss: 0.5260\n",
      "Epoch [4/8], Step [107/179], Loss: 0.8172\n",
      "Epoch [4/8], Step [108/179], Loss: 0.6098\n",
      "Epoch [4/8], Step [109/179], Loss: 0.4914\n",
      "Epoch [4/8], Step [110/179], Loss: 0.4808\n",
      "Epoch [4/8], Step [111/179], Loss: 0.7197\n",
      "Epoch [4/8], Step [112/179], Loss: 0.5512\n",
      "Epoch [4/8], Step [113/179], Loss: 0.7926\n",
      "Epoch [4/8], Step [114/179], Loss: 0.4954\n",
      "Epoch [4/8], Step [115/179], Loss: 0.5830\n",
      "Epoch [4/8], Step [116/179], Loss: 0.6877\n",
      "Epoch [4/8], Step [117/179], Loss: 0.5994\n",
      "Epoch [4/8], Step [118/179], Loss: 0.5370\n",
      "Epoch [4/8], Step [119/179], Loss: 0.5919\n",
      "Epoch [4/8], Step [120/179], Loss: 0.8870\n",
      "Epoch [4/8], Step [121/179], Loss: 0.4301\n",
      "Epoch [4/8], Step [122/179], Loss: 0.7813\n",
      "Epoch [4/8], Step [123/179], Loss: 0.5854\n",
      "Epoch [4/8], Step [124/179], Loss: 0.5948\n",
      "Epoch [4/8], Step [125/179], Loss: 0.5996\n",
      "Epoch [4/8], Step [126/179], Loss: 0.6542\n",
      "Epoch [4/8], Step [127/179], Loss: 0.6928\n",
      "Epoch [4/8], Step [128/179], Loss: 0.4198\n",
      "Epoch [4/8], Step [129/179], Loss: 0.8721\n",
      "Epoch [4/8], Step [130/179], Loss: 0.3857\n",
      "Epoch [4/8], Step [131/179], Loss: 0.4003\n",
      "Epoch [4/8], Step [132/179], Loss: 0.5290\n",
      "Epoch [4/8], Step [133/179], Loss: 0.4171\n",
      "Epoch [4/8], Step [134/179], Loss: 0.4178\n",
      "Epoch [4/8], Step [135/179], Loss: 0.5220\n",
      "Epoch [4/8], Step [136/179], Loss: 0.5563\n",
      "Epoch [4/8], Step [137/179], Loss: 0.5680\n",
      "Epoch [4/8], Step [138/179], Loss: 0.4435\n",
      "Epoch [4/8], Step [139/179], Loss: 0.3517\n",
      "Epoch [4/8], Step [140/179], Loss: 0.7124\n",
      "Epoch [4/8], Step [141/179], Loss: 0.4640\n",
      "Epoch [4/8], Step [142/179], Loss: 0.3641\n",
      "Epoch [4/8], Step [143/179], Loss: 0.4373\n",
      "Epoch [4/8], Step [144/179], Loss: 0.5602\n",
      "Epoch [4/8], Step [145/179], Loss: 0.4419\n",
      "Epoch [4/8], Step [146/179], Loss: 0.3632\n",
      "Epoch [4/8], Step [147/179], Loss: 0.4919\n",
      "Epoch [4/8], Step [148/179], Loss: 0.7294\n",
      "Epoch [4/8], Step [149/179], Loss: 0.4532\n",
      "Epoch [4/8], Step [150/179], Loss: 0.6789\n",
      "Epoch [4/8], Step [151/179], Loss: 0.5147\n",
      "Epoch [4/8], Step [152/179], Loss: 0.5877\n",
      "Epoch [4/8], Step [153/179], Loss: 0.6032\n",
      "Epoch [4/8], Step [154/179], Loss: 0.8698\n",
      "Epoch [4/8], Step [155/179], Loss: 0.4980\n",
      "Epoch [4/8], Step [156/179], Loss: 0.7396\n",
      "Epoch [4/8], Step [157/179], Loss: 0.6372\n",
      "Epoch [4/8], Step [158/179], Loss: 0.2961\n",
      "Epoch [4/8], Step [159/179], Loss: 0.5113\n",
      "Epoch [4/8], Step [160/179], Loss: 0.4981\n",
      "Epoch [4/8], Step [161/179], Loss: 0.4816\n",
      "Epoch [4/8], Step [162/179], Loss: 0.6109\n",
      "Epoch [4/8], Step [163/179], Loss: 0.4062\n",
      "Epoch [4/8], Step [164/179], Loss: 1.0714\n",
      "Epoch [4/8], Step [165/179], Loss: 0.4238\n",
      "Epoch [4/8], Step [166/179], Loss: 0.5730\n",
      "Epoch [4/8], Step [167/179], Loss: 0.4511\n",
      "Epoch [4/8], Step [168/179], Loss: 0.6479\n",
      "Epoch [4/8], Step [169/179], Loss: 0.6079\n",
      "Epoch [4/8], Step [170/179], Loss: 0.5007\n",
      "Epoch [4/8], Step [171/179], Loss: 0.5184\n",
      "Epoch [4/8], Step [172/179], Loss: 0.2619\n",
      "Epoch [4/8], Step [173/179], Loss: 0.5643\n",
      "Epoch [4/8], Step [174/179], Loss: 0.8030\n",
      "Epoch [4/8], Step [175/179], Loss: 0.4851\n",
      "Epoch [4/8], Step [176/179], Loss: 0.5852\n",
      "Epoch [4/8], Step [177/179], Loss: 0.6313\n",
      "Epoch [4/8], Step [178/179], Loss: 0.7358\n",
      "Epoch [4/8], Step [179/179], Loss: 0.6642\n",
      "Epoch [5/8], Step [1/179], Loss: 0.4010\n",
      "Epoch [5/8], Step [2/179], Loss: 0.5696\n",
      "Epoch [5/8], Step [3/179], Loss: 0.6207\n",
      "Epoch [5/8], Step [4/179], Loss: 0.5660\n",
      "Epoch [5/8], Step [5/179], Loss: 0.5728\n",
      "Epoch [5/8], Step [6/179], Loss: 0.4544\n",
      "Epoch [5/8], Step [7/179], Loss: 0.6105\n",
      "Epoch [5/8], Step [8/179], Loss: 0.3623\n",
      "Epoch [5/8], Step [9/179], Loss: 0.3855\n",
      "Epoch [5/8], Step [10/179], Loss: 0.5126\n",
      "Epoch [5/8], Step [11/179], Loss: 0.3790\n",
      "Epoch [5/8], Step [12/179], Loss: 0.3915\n",
      "Epoch [5/8], Step [13/179], Loss: 0.4266\n",
      "Epoch [5/8], Step [14/179], Loss: 0.4144\n",
      "Epoch [5/8], Step [15/179], Loss: 0.5185\n",
      "Epoch [5/8], Step [16/179], Loss: 0.4462\n",
      "Epoch [5/8], Step [17/179], Loss: 0.6781\n",
      "Epoch [5/8], Step [18/179], Loss: 0.4129\n",
      "Epoch [5/8], Step [19/179], Loss: 0.4497\n",
      "Epoch [5/8], Step [20/179], Loss: 0.4619\n",
      "Epoch [5/8], Step [21/179], Loss: 0.5580\n",
      "Epoch [5/8], Step [22/179], Loss: 0.4621\n",
      "Epoch [5/8], Step [23/179], Loss: 0.3881\n",
      "Epoch [5/8], Step [24/179], Loss: 0.6855\n",
      "Epoch [5/8], Step [25/179], Loss: 0.5207\n",
      "Epoch [5/8], Step [26/179], Loss: 0.2827\n",
      "Epoch [5/8], Step [27/179], Loss: 0.4073\n",
      "Epoch [5/8], Step [28/179], Loss: 0.5449\n",
      "Epoch [5/8], Step [29/179], Loss: 0.3370\n",
      "Epoch [5/8], Step [30/179], Loss: 0.4204\n",
      "Epoch [5/8], Step [31/179], Loss: 0.5576\n",
      "Epoch [5/8], Step [32/179], Loss: 0.5406\n",
      "Epoch [5/8], Step [33/179], Loss: 0.2696\n",
      "Epoch [5/8], Step [34/179], Loss: 0.4614\n",
      "Epoch [5/8], Step [35/179], Loss: 0.6823\n",
      "Epoch [5/8], Step [36/179], Loss: 0.4414\n",
      "Epoch [5/8], Step [37/179], Loss: 0.6464\n",
      "Epoch [5/8], Step [38/179], Loss: 0.6351\n",
      "Epoch [5/8], Step [39/179], Loss: 0.4321\n",
      "Epoch [5/8], Step [40/179], Loss: 0.3786\n",
      "Epoch [5/8], Step [41/179], Loss: 0.4757\n",
      "Epoch [5/8], Step [42/179], Loss: 0.4349\n",
      "Epoch [5/8], Step [43/179], Loss: 1.1811\n",
      "Epoch [5/8], Step [44/179], Loss: 0.4810\n",
      "Epoch [5/8], Step [45/179], Loss: 0.6029\n",
      "Epoch [5/8], Step [46/179], Loss: 0.6625\n",
      "Epoch [5/8], Step [47/179], Loss: 0.5842\n",
      "Epoch [5/8], Step [48/179], Loss: 0.6761\n",
      "Epoch [5/8], Step [49/179], Loss: 0.5195\n",
      "Epoch [5/8], Step [50/179], Loss: 0.4503\n",
      "Epoch [5/8], Step [51/179], Loss: 0.3884\n",
      "Epoch [5/8], Step [52/179], Loss: 0.3782\n",
      "Epoch [5/8], Step [53/179], Loss: 0.5270\n",
      "Epoch [5/8], Step [54/179], Loss: 0.4992\n",
      "Epoch [5/8], Step [55/179], Loss: 0.5130\n",
      "Epoch [5/8], Step [56/179], Loss: 0.4480\n",
      "Epoch [5/8], Step [57/179], Loss: 0.4516\n",
      "Epoch [5/8], Step [58/179], Loss: 0.3676\n",
      "Epoch [5/8], Step [59/179], Loss: 0.5932\n",
      "Epoch [5/8], Step [60/179], Loss: 0.4510\n",
      "Epoch [5/8], Step [61/179], Loss: 0.6683\n",
      "Epoch [5/8], Step [62/179], Loss: 0.4802\n",
      "Epoch [5/8], Step [63/179], Loss: 0.5729\n",
      "Epoch [5/8], Step [64/179], Loss: 0.4405\n",
      "Epoch [5/8], Step [65/179], Loss: 0.5393\n",
      "Epoch [5/8], Step [66/179], Loss: 0.3368\n",
      "Epoch [5/8], Step [67/179], Loss: 0.3806\n",
      "Epoch [5/8], Step [68/179], Loss: 0.4036\n",
      "Epoch [5/8], Step [69/179], Loss: 0.6189\n",
      "Epoch [5/8], Step [70/179], Loss: 0.4506\n",
      "Epoch [5/8], Step [71/179], Loss: 0.4000\n",
      "Epoch [5/8], Step [72/179], Loss: 0.4054\n",
      "Epoch [5/8], Step [73/179], Loss: 0.4968\n",
      "Epoch [5/8], Step [74/179], Loss: 0.3463\n",
      "Epoch [5/8], Step [75/179], Loss: 0.5430\n",
      "Epoch [5/8], Step [76/179], Loss: 0.4746\n",
      "Epoch [5/8], Step [77/179], Loss: 0.3706\n",
      "Epoch [5/8], Step [78/179], Loss: 0.4132\n",
      "Epoch [5/8], Step [79/179], Loss: 0.6524\n",
      "Epoch [5/8], Step [80/179], Loss: 0.6825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [81/179], Loss: 0.4895\n",
      "Epoch [5/8], Step [82/179], Loss: 0.5187\n",
      "Epoch [5/8], Step [83/179], Loss: 0.8029\n",
      "Epoch [5/8], Step [84/179], Loss: 0.5960\n",
      "Epoch [5/8], Step [85/179], Loss: 0.3758\n",
      "Epoch [5/8], Step [86/179], Loss: 0.7191\n",
      "Epoch [5/8], Step [87/179], Loss: 0.4760\n",
      "Epoch [5/8], Step [88/179], Loss: 0.7283\n",
      "Epoch [5/8], Step [89/179], Loss: 0.5202\n",
      "Epoch [5/8], Step [90/179], Loss: 0.5263\n",
      "Epoch [5/8], Step [91/179], Loss: 0.6679\n",
      "Epoch [5/8], Step [92/179], Loss: 0.5404\n",
      "Epoch [5/8], Step [93/179], Loss: 0.7292\n",
      "Epoch [5/8], Step [94/179], Loss: 0.5643\n",
      "Epoch [5/8], Step [95/179], Loss: 0.3382\n",
      "Epoch [5/8], Step [96/179], Loss: 0.5773\n",
      "Epoch [5/8], Step [97/179], Loss: 0.5429\n",
      "Epoch [5/8], Step [98/179], Loss: 0.3671\n",
      "Epoch [5/8], Step [99/179], Loss: 0.4026\n",
      "Epoch [5/8], Step [100/179], Loss: 0.5535\n",
      "Epoch [5/8], Step [101/179], Loss: 0.5628\n",
      "Epoch [5/8], Step [102/179], Loss: 0.6239\n",
      "Epoch [5/8], Step [103/179], Loss: 0.3310\n",
      "Epoch [5/8], Step [104/179], Loss: 0.6557\n",
      "Epoch [5/8], Step [105/179], Loss: 0.7499\n",
      "Epoch [5/8], Step [106/179], Loss: 0.7230\n",
      "Epoch [5/8], Step [107/179], Loss: 0.6201\n",
      "Epoch [5/8], Step [108/179], Loss: 0.5174\n",
      "Epoch [5/8], Step [109/179], Loss: 0.5434\n",
      "Epoch [5/8], Step [110/179], Loss: 0.4333\n",
      "Epoch [5/8], Step [111/179], Loss: 0.2481\n",
      "Epoch [5/8], Step [112/179], Loss: 0.5156\n",
      "Epoch [5/8], Step [113/179], Loss: 0.8995\n",
      "Epoch [5/8], Step [114/179], Loss: 0.5626\n",
      "Epoch [5/8], Step [115/179], Loss: 0.6526\n",
      "Epoch [5/8], Step [116/179], Loss: 0.4033\n",
      "Epoch [5/8], Step [117/179], Loss: 0.7534\n",
      "Epoch [5/8], Step [118/179], Loss: 0.4644\n",
      "Epoch [5/8], Step [119/179], Loss: 0.7238\n",
      "Epoch [5/8], Step [120/179], Loss: 0.4824\n",
      "Epoch [5/8], Step [121/179], Loss: 0.6964\n",
      "Epoch [5/8], Step [122/179], Loss: 0.5709\n",
      "Epoch [5/8], Step [123/179], Loss: 0.5228\n",
      "Epoch [5/8], Step [124/179], Loss: 0.3159\n",
      "Epoch [5/8], Step [125/179], Loss: 0.6220\n",
      "Epoch [5/8], Step [126/179], Loss: 0.3442\n",
      "Epoch [5/8], Step [127/179], Loss: 0.4005\n",
      "Epoch [5/8], Step [128/179], Loss: 0.7192\n",
      "Epoch [5/8], Step [129/179], Loss: 0.5205\n",
      "Epoch [5/8], Step [130/179], Loss: 0.4854\n",
      "Epoch [5/8], Step [131/179], Loss: 0.4190\n",
      "Epoch [5/8], Step [132/179], Loss: 0.5261\n",
      "Epoch [5/8], Step [133/179], Loss: 0.4653\n",
      "Epoch [5/8], Step [134/179], Loss: 0.4535\n",
      "Epoch [5/8], Step [135/179], Loss: 0.7250\n",
      "Epoch [5/8], Step [136/179], Loss: 0.6081\n",
      "Epoch [5/8], Step [137/179], Loss: 0.5911\n",
      "Epoch [5/8], Step [138/179], Loss: 0.4442\n",
      "Epoch [5/8], Step [139/179], Loss: 0.6549\n",
      "Epoch [5/8], Step [140/179], Loss: 0.6679\n",
      "Epoch [5/8], Step [141/179], Loss: 0.3885\n",
      "Epoch [5/8], Step [142/179], Loss: 0.6177\n",
      "Epoch [5/8], Step [143/179], Loss: 0.4027\n",
      "Epoch [5/8], Step [144/179], Loss: 0.5354\n",
      "Epoch [5/8], Step [145/179], Loss: 0.5296\n",
      "Epoch [5/8], Step [146/179], Loss: 0.3856\n",
      "Epoch [5/8], Step [147/179], Loss: 0.4672\n",
      "Epoch [5/8], Step [148/179], Loss: 0.6969\n",
      "Epoch [5/8], Step [149/179], Loss: 0.2948\n",
      "Epoch [5/8], Step [150/179], Loss: 0.6794\n",
      "Epoch [5/8], Step [151/179], Loss: 0.4808\n",
      "Epoch [5/8], Step [152/179], Loss: 0.3111\n",
      "Epoch [5/8], Step [153/179], Loss: 0.6428\n",
      "Epoch [5/8], Step [154/179], Loss: 0.7067\n",
      "Epoch [5/8], Step [155/179], Loss: 0.4330\n",
      "Epoch [5/8], Step [156/179], Loss: 0.6033\n",
      "Epoch [5/8], Step [157/179], Loss: 0.5161\n",
      "Epoch [5/8], Step [158/179], Loss: 0.2917\n",
      "Epoch [5/8], Step [159/179], Loss: 0.5806\n",
      "Epoch [5/8], Step [160/179], Loss: 0.4039\n",
      "Epoch [5/8], Step [161/179], Loss: 0.6887\n",
      "Epoch [5/8], Step [162/179], Loss: 0.4874\n",
      "Epoch [5/8], Step [163/179], Loss: 0.6420\n",
      "Epoch [5/8], Step [164/179], Loss: 0.5715\n",
      "Epoch [5/8], Step [165/179], Loss: 0.6497\n",
      "Epoch [5/8], Step [166/179], Loss: 0.8388\n",
      "Epoch [5/8], Step [167/179], Loss: 0.5450\n",
      "Epoch [5/8], Step [168/179], Loss: 0.5196\n",
      "Epoch [5/8], Step [169/179], Loss: 0.3545\n",
      "Epoch [5/8], Step [170/179], Loss: 0.8548\n",
      "Epoch [5/8], Step [171/179], Loss: 0.5235\n",
      "Epoch [5/8], Step [172/179], Loss: 0.5633\n",
      "Epoch [5/8], Step [173/179], Loss: 0.3813\n",
      "Epoch [5/8], Step [174/179], Loss: 0.4040\n",
      "Epoch [5/8], Step [175/179], Loss: 0.4871\n",
      "Epoch [5/8], Step [176/179], Loss: 0.5472\n",
      "Epoch [5/8], Step [177/179], Loss: 0.7519\n",
      "Epoch [5/8], Step [178/179], Loss: 0.3703\n",
      "Epoch [5/8], Step [179/179], Loss: 0.3644\n",
      "Epoch [6/8], Step [1/179], Loss: 0.3654\n",
      "Epoch [6/8], Step [2/179], Loss: 0.5185\n",
      "Epoch [6/8], Step [3/179], Loss: 0.3017\n",
      "Epoch [6/8], Step [4/179], Loss: 0.3407\n",
      "Epoch [6/8], Step [5/179], Loss: 0.2929\n",
      "Epoch [6/8], Step [6/179], Loss: 0.2803\n",
      "Epoch [6/8], Step [7/179], Loss: 0.3937\n",
      "Epoch [6/8], Step [8/179], Loss: 0.4319\n",
      "Epoch [6/8], Step [9/179], Loss: 0.4877\n",
      "Epoch [6/8], Step [10/179], Loss: 0.2072\n",
      "Epoch [6/8], Step [11/179], Loss: 0.5285\n",
      "Epoch [6/8], Step [12/179], Loss: 0.6245\n",
      "Epoch [6/8], Step [13/179], Loss: 0.2964\n",
      "Epoch [6/8], Step [14/179], Loss: 0.5508\n",
      "Epoch [6/8], Step [15/179], Loss: 0.5363\n",
      "Epoch [6/8], Step [16/179], Loss: 0.5831\n",
      "Epoch [6/8], Step [17/179], Loss: 0.3991\n",
      "Epoch [6/8], Step [18/179], Loss: 0.5882\n",
      "Epoch [6/8], Step [19/179], Loss: 0.5208\n",
      "Epoch [6/8], Step [20/179], Loss: 0.3852\n",
      "Epoch [6/8], Step [21/179], Loss: 0.4732\n",
      "Epoch [6/8], Step [22/179], Loss: 0.4965\n",
      "Epoch [6/8], Step [23/179], Loss: 0.4615\n",
      "Epoch [6/8], Step [24/179], Loss: 0.2250\n",
      "Epoch [6/8], Step [25/179], Loss: 0.5981\n",
      "Epoch [6/8], Step [26/179], Loss: 0.5328\n",
      "Epoch [6/8], Step [27/179], Loss: 0.4415\n",
      "Epoch [6/8], Step [28/179], Loss: 0.4999\n",
      "Epoch [6/8], Step [29/179], Loss: 0.6156\n",
      "Epoch [6/8], Step [30/179], Loss: 0.5499\n",
      "Epoch [6/8], Step [31/179], Loss: 0.5460\n",
      "Epoch [6/8], Step [32/179], Loss: 0.4526\n",
      "Epoch [6/8], Step [33/179], Loss: 0.6574\n",
      "Epoch [6/8], Step [34/179], Loss: 0.3818\n",
      "Epoch [6/8], Step [35/179], Loss: 0.5417\n",
      "Epoch [6/8], Step [36/179], Loss: 0.6820\n",
      "Epoch [6/8], Step [37/179], Loss: 0.5453\n",
      "Epoch [6/8], Step [38/179], Loss: 0.3577\n",
      "Epoch [6/8], Step [39/179], Loss: 0.4909\n",
      "Epoch [6/8], Step [40/179], Loss: 0.3954\n",
      "Epoch [6/8], Step [41/179], Loss: 0.5576\n",
      "Epoch [6/8], Step [42/179], Loss: 0.3542\n",
      "Epoch [6/8], Step [43/179], Loss: 0.2868\n",
      "Epoch [6/8], Step [44/179], Loss: 0.4250\n",
      "Epoch [6/8], Step [45/179], Loss: 0.4752\n",
      "Epoch [6/8], Step [46/179], Loss: 0.4554\n",
      "Epoch [6/8], Step [47/179], Loss: 0.5564\n",
      "Epoch [6/8], Step [48/179], Loss: 0.5673\n",
      "Epoch [6/8], Step [49/179], Loss: 0.3133\n",
      "Epoch [6/8], Step [50/179], Loss: 0.6871\n",
      "Epoch [6/8], Step [51/179], Loss: 0.7543\n",
      "Epoch [6/8], Step [52/179], Loss: 0.4903\n",
      "Epoch [6/8], Step [53/179], Loss: 0.4607\n",
      "Epoch [6/8], Step [54/179], Loss: 0.2502\n",
      "Epoch [6/8], Step [55/179], Loss: 0.3206\n",
      "Epoch [6/8], Step [56/179], Loss: 0.6393\n",
      "Epoch [6/8], Step [57/179], Loss: 0.3738\n",
      "Epoch [6/8], Step [58/179], Loss: 0.6683\n",
      "Epoch [6/8], Step [59/179], Loss: 0.6508\n",
      "Epoch [6/8], Step [60/179], Loss: 0.4176\n",
      "Epoch [6/8], Step [61/179], Loss: 0.7338\n",
      "Epoch [6/8], Step [62/179], Loss: 0.4118\n",
      "Epoch [6/8], Step [63/179], Loss: 0.6267\n",
      "Epoch [6/8], Step [64/179], Loss: 0.3587\n",
      "Epoch [6/8], Step [65/179], Loss: 0.3224\n",
      "Epoch [6/8], Step [66/179], Loss: 0.5699\n",
      "Epoch [6/8], Step [67/179], Loss: 0.4540\n",
      "Epoch [6/8], Step [68/179], Loss: 0.2923\n",
      "Epoch [6/8], Step [69/179], Loss: 0.4375\n",
      "Epoch [6/8], Step [70/179], Loss: 0.3108\n",
      "Epoch [6/8], Step [71/179], Loss: 0.6398\n",
      "Epoch [6/8], Step [72/179], Loss: 0.5819\n",
      "Epoch [6/8], Step [73/179], Loss: 0.4239\n",
      "Epoch [6/8], Step [74/179], Loss: 0.4975\n",
      "Epoch [6/8], Step [75/179], Loss: 0.4826\n",
      "Epoch [6/8], Step [76/179], Loss: 0.3707\n",
      "Epoch [6/8], Step [77/179], Loss: 0.2498\n",
      "Epoch [6/8], Step [78/179], Loss: 0.4810\n",
      "Epoch [6/8], Step [79/179], Loss: 0.6228\n",
      "Epoch [6/8], Step [80/179], Loss: 0.6909\n",
      "Epoch [6/8], Step [81/179], Loss: 0.5156\n",
      "Epoch [6/8], Step [82/179], Loss: 0.6189\n",
      "Epoch [6/8], Step [83/179], Loss: 0.5867\n",
      "Epoch [6/8], Step [84/179], Loss: 0.4750\n",
      "Epoch [6/8], Step [85/179], Loss: 0.3785\n",
      "Epoch [6/8], Step [86/179], Loss: 0.3254\n",
      "Epoch [6/8], Step [87/179], Loss: 0.5840\n",
      "Epoch [6/8], Step [88/179], Loss: 0.4015\n",
      "Epoch [6/8], Step [89/179], Loss: 0.5150\n",
      "Epoch [6/8], Step [90/179], Loss: 0.3809\n",
      "Epoch [6/8], Step [91/179], Loss: 0.9066\n",
      "Epoch [6/8], Step [92/179], Loss: 0.4719\n",
      "Epoch [6/8], Step [93/179], Loss: 0.7350\n",
      "Epoch [6/8], Step [94/179], Loss: 0.2986\n",
      "Epoch [6/8], Step [95/179], Loss: 0.5980\n",
      "Epoch [6/8], Step [96/179], Loss: 0.5858\n",
      "Epoch [6/8], Step [97/179], Loss: 0.3684\n",
      "Epoch [6/8], Step [98/179], Loss: 0.3595\n",
      "Epoch [6/8], Step [99/179], Loss: 0.4865\n",
      "Epoch [6/8], Step [100/179], Loss: 0.7223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Step [101/179], Loss: 0.5208\n",
      "Epoch [6/8], Step [102/179], Loss: 0.3284\n",
      "Epoch [6/8], Step [103/179], Loss: 0.5699\n",
      "Epoch [6/8], Step [104/179], Loss: 0.7301\n",
      "Epoch [6/8], Step [105/179], Loss: 0.4661\n",
      "Epoch [6/8], Step [106/179], Loss: 0.3914\n",
      "Epoch [6/8], Step [107/179], Loss: 0.3296\n",
      "Epoch [6/8], Step [108/179], Loss: 0.4023\n",
      "Epoch [6/8], Step [109/179], Loss: 0.6502\n",
      "Epoch [6/8], Step [110/179], Loss: 0.4223\n",
      "Epoch [6/8], Step [111/179], Loss: 0.4134\n",
      "Epoch [6/8], Step [112/179], Loss: 0.3670\n",
      "Epoch [6/8], Step [113/179], Loss: 0.4071\n",
      "Epoch [6/8], Step [114/179], Loss: 0.3765\n",
      "Epoch [6/8], Step [115/179], Loss: 1.0313\n",
      "Epoch [6/8], Step [116/179], Loss: 0.4333\n",
      "Epoch [6/8], Step [117/179], Loss: 0.4652\n",
      "Epoch [6/8], Step [118/179], Loss: 0.4341\n",
      "Epoch [6/8], Step [119/179], Loss: 0.2350\n",
      "Epoch [6/8], Step [120/179], Loss: 0.4133\n",
      "Epoch [6/8], Step [121/179], Loss: 0.5222\n",
      "Epoch [6/8], Step [122/179], Loss: 0.5922\n",
      "Epoch [6/8], Step [123/179], Loss: 0.2435\n",
      "Epoch [6/8], Step [124/179], Loss: 0.8162\n",
      "Epoch [6/8], Step [125/179], Loss: 0.5478\n",
      "Epoch [6/8], Step [126/179], Loss: 0.6365\n",
      "Epoch [6/8], Step [127/179], Loss: 0.5419\n",
      "Epoch [6/8], Step [128/179], Loss: 0.6192\n",
      "Epoch [6/8], Step [129/179], Loss: 0.5893\n",
      "Epoch [6/8], Step [130/179], Loss: 0.5381\n",
      "Epoch [6/8], Step [131/179], Loss: 0.6335\n",
      "Epoch [6/8], Step [132/179], Loss: 0.4755\n",
      "Epoch [6/8], Step [133/179], Loss: 0.4030\n",
      "Epoch [6/8], Step [134/179], Loss: 0.5089\n",
      "Epoch [6/8], Step [135/179], Loss: 0.4345\n",
      "Epoch [6/8], Step [136/179], Loss: 0.3421\n",
      "Epoch [6/8], Step [137/179], Loss: 0.4550\n",
      "Epoch [6/8], Step [138/179], Loss: 0.4499\n",
      "Epoch [6/8], Step [139/179], Loss: 0.4277\n",
      "Epoch [6/8], Step [140/179], Loss: 0.5383\n",
      "Epoch [6/8], Step [141/179], Loss: 0.4962\n",
      "Epoch [6/8], Step [142/179], Loss: 0.4123\n",
      "Epoch [6/8], Step [143/179], Loss: 0.4483\n",
      "Epoch [6/8], Step [144/179], Loss: 0.5158\n",
      "Epoch [6/8], Step [145/179], Loss: 0.4367\n",
      "Epoch [6/8], Step [146/179], Loss: 0.4123\n",
      "Epoch [6/8], Step [147/179], Loss: 0.3711\n",
      "Epoch [6/8], Step [148/179], Loss: 0.2929\n",
      "Epoch [6/8], Step [149/179], Loss: 0.5755\n",
      "Epoch [6/8], Step [150/179], Loss: 0.4846\n",
      "Epoch [6/8], Step [151/179], Loss: 0.3482\n",
      "Epoch [6/8], Step [152/179], Loss: 0.2739\n",
      "Epoch [6/8], Step [153/179], Loss: 0.3664\n",
      "Epoch [6/8], Step [154/179], Loss: 0.4554\n",
      "Epoch [6/8], Step [155/179], Loss: 0.4860\n",
      "Epoch [6/8], Step [156/179], Loss: 0.4913\n",
      "Epoch [6/8], Step [157/179], Loss: 0.4315\n",
      "Epoch [6/8], Step [158/179], Loss: 0.3416\n",
      "Epoch [6/8], Step [159/179], Loss: 0.4003\n",
      "Epoch [6/8], Step [160/179], Loss: 0.5539\n",
      "Epoch [6/8], Step [161/179], Loss: 0.3148\n",
      "Epoch [6/8], Step [162/179], Loss: 0.4095\n",
      "Epoch [6/8], Step [163/179], Loss: 0.4137\n",
      "Epoch [6/8], Step [164/179], Loss: 0.4311\n",
      "Epoch [6/8], Step [165/179], Loss: 0.2473\n",
      "Epoch [6/8], Step [166/179], Loss: 0.4546\n",
      "Epoch [6/8], Step [167/179], Loss: 0.4638\n",
      "Epoch [6/8], Step [168/179], Loss: 0.1913\n",
      "Epoch [6/8], Step [169/179], Loss: 0.3762\n",
      "Epoch [6/8], Step [170/179], Loss: 0.6321\n",
      "Epoch [6/8], Step [171/179], Loss: 0.3498\n",
      "Epoch [6/8], Step [172/179], Loss: 0.5307\n",
      "Epoch [6/8], Step [173/179], Loss: 0.6465\n",
      "Epoch [6/8], Step [174/179], Loss: 0.5973\n",
      "Epoch [6/8], Step [175/179], Loss: 0.5461\n",
      "Epoch [6/8], Step [176/179], Loss: 0.2512\n",
      "Epoch [6/8], Step [177/179], Loss: 0.5541\n",
      "Epoch [6/8], Step [178/179], Loss: 0.4392\n",
      "Epoch [6/8], Step [179/179], Loss: 0.5314\n",
      "Epoch [7/8], Step [1/179], Loss: 0.3996\n",
      "Epoch [7/8], Step [2/179], Loss: 0.5268\n",
      "Epoch [7/8], Step [3/179], Loss: 0.5694\n",
      "Epoch [7/8], Step [4/179], Loss: 0.6487\n",
      "Epoch [7/8], Step [5/179], Loss: 0.4423\n",
      "Epoch [7/8], Step [6/179], Loss: 0.4437\n",
      "Epoch [7/8], Step [7/179], Loss: 0.5902\n",
      "Epoch [7/8], Step [8/179], Loss: 0.6775\n",
      "Epoch [7/8], Step [9/179], Loss: 0.3920\n",
      "Epoch [7/8], Step [10/179], Loss: 0.3379\n",
      "Epoch [7/8], Step [11/179], Loss: 0.3339\n",
      "Epoch [7/8], Step [12/179], Loss: 0.3620\n",
      "Epoch [7/8], Step [13/179], Loss: 0.5596\n",
      "Epoch [7/8], Step [14/179], Loss: 0.2393\n",
      "Epoch [7/8], Step [15/179], Loss: 0.4144\n",
      "Epoch [7/8], Step [16/179], Loss: 0.3844\n",
      "Epoch [7/8], Step [17/179], Loss: 0.2755\n",
      "Epoch [7/8], Step [18/179], Loss: 0.2259\n",
      "Epoch [7/8], Step [19/179], Loss: 0.3012\n",
      "Epoch [7/8], Step [20/179], Loss: 0.5896\n",
      "Epoch [7/8], Step [21/179], Loss: 0.2181\n",
      "Epoch [7/8], Step [22/179], Loss: 0.3335\n",
      "Epoch [7/8], Step [23/179], Loss: 0.3932\n",
      "Epoch [7/8], Step [24/179], Loss: 0.3574\n",
      "Epoch [7/8], Step [25/179], Loss: 0.4166\n",
      "Epoch [7/8], Step [26/179], Loss: 0.4713\n",
      "Epoch [7/8], Step [27/179], Loss: 0.3540\n",
      "Epoch [7/8], Step [28/179], Loss: 0.6040\n",
      "Epoch [7/8], Step [29/179], Loss: 0.4296\n",
      "Epoch [7/8], Step [30/179], Loss: 0.2320\n",
      "Epoch [7/8], Step [31/179], Loss: 0.8846\n",
      "Epoch [7/8], Step [32/179], Loss: 0.8769\n",
      "Epoch [7/8], Step [33/179], Loss: 0.3360\n",
      "Epoch [7/8], Step [34/179], Loss: 0.1525\n",
      "Epoch [7/8], Step [35/179], Loss: 0.4073\n",
      "Epoch [7/8], Step [36/179], Loss: 0.5244\n",
      "Epoch [7/8], Step [37/179], Loss: 0.4431\n",
      "Epoch [7/8], Step [38/179], Loss: 0.4412\n",
      "Epoch [7/8], Step [39/179], Loss: 0.7984\n",
      "Epoch [7/8], Step [40/179], Loss: 0.4557\n",
      "Epoch [7/8], Step [41/179], Loss: 0.2409\n",
      "Epoch [7/8], Step [42/179], Loss: 0.3833\n",
      "Epoch [7/8], Step [43/179], Loss: 0.3141\n",
      "Epoch [7/8], Step [44/179], Loss: 0.5434\n",
      "Epoch [7/8], Step [45/179], Loss: 0.5286\n",
      "Epoch [7/8], Step [46/179], Loss: 0.5194\n",
      "Epoch [7/8], Step [47/179], Loss: 0.4508\n",
      "Epoch [7/8], Step [48/179], Loss: 0.4125\n",
      "Epoch [7/8], Step [49/179], Loss: 0.6661\n",
      "Epoch [7/8], Step [50/179], Loss: 0.6654\n",
      "Epoch [7/8], Step [51/179], Loss: 0.1951\n",
      "Epoch [7/8], Step [52/179], Loss: 0.2372\n",
      "Epoch [7/8], Step [53/179], Loss: 0.4977\n",
      "Epoch [7/8], Step [54/179], Loss: 0.4392\n",
      "Epoch [7/8], Step [55/179], Loss: 0.6645\n",
      "Epoch [7/8], Step [56/179], Loss: 0.3852\n",
      "Epoch [7/8], Step [57/179], Loss: 0.3406\n",
      "Epoch [7/8], Step [58/179], Loss: 0.3269\n",
      "Epoch [7/8], Step [59/179], Loss: 0.5328\n",
      "Epoch [7/8], Step [60/179], Loss: 0.4171\n",
      "Epoch [7/8], Step [61/179], Loss: 0.2928\n",
      "Epoch [7/8], Step [62/179], Loss: 0.3061\n",
      "Epoch [7/8], Step [63/179], Loss: 0.7915\n",
      "Epoch [7/8], Step [64/179], Loss: 0.6771\n",
      "Epoch [7/8], Step [65/179], Loss: 0.3724\n",
      "Epoch [7/8], Step [66/179], Loss: 0.4417\n",
      "Epoch [7/8], Step [67/179], Loss: 0.6250\n",
      "Epoch [7/8], Step [68/179], Loss: 0.3046\n",
      "Epoch [7/8], Step [69/179], Loss: 0.2695\n",
      "Epoch [7/8], Step [70/179], Loss: 0.3950\n",
      "Epoch [7/8], Step [71/179], Loss: 0.3339\n",
      "Epoch [7/8], Step [72/179], Loss: 0.4479\n",
      "Epoch [7/8], Step [73/179], Loss: 0.4007\n",
      "Epoch [7/8], Step [74/179], Loss: 0.6323\n",
      "Epoch [7/8], Step [75/179], Loss: 0.7297\n",
      "Epoch [7/8], Step [76/179], Loss: 0.5553\n",
      "Epoch [7/8], Step [77/179], Loss: 0.4558\n",
      "Epoch [7/8], Step [78/179], Loss: 0.3086\n",
      "Epoch [7/8], Step [79/179], Loss: 0.4264\n",
      "Epoch [7/8], Step [80/179], Loss: 0.5595\n",
      "Epoch [7/8], Step [81/179], Loss: 0.6016\n",
      "Epoch [7/8], Step [82/179], Loss: 0.5635\n",
      "Epoch [7/8], Step [83/179], Loss: 0.4700\n",
      "Epoch [7/8], Step [84/179], Loss: 0.4069\n",
      "Epoch [7/8], Step [85/179], Loss: 0.4946\n",
      "Epoch [7/8], Step [86/179], Loss: 0.3704\n",
      "Epoch [7/8], Step [87/179], Loss: 0.4351\n",
      "Epoch [7/8], Step [88/179], Loss: 0.4163\n",
      "Epoch [7/8], Step [89/179], Loss: 0.4699\n",
      "Epoch [7/8], Step [90/179], Loss: 0.3475\n",
      "Epoch [7/8], Step [91/179], Loss: 0.5046\n",
      "Epoch [7/8], Step [92/179], Loss: 0.4705\n",
      "Epoch [7/8], Step [93/179], Loss: 0.8160\n",
      "Epoch [7/8], Step [94/179], Loss: 0.4132\n",
      "Epoch [7/8], Step [95/179], Loss: 0.3772\n",
      "Epoch [7/8], Step [96/179], Loss: 0.3322\n",
      "Epoch [7/8], Step [97/179], Loss: 0.4649\n",
      "Epoch [7/8], Step [98/179], Loss: 0.4190\n",
      "Epoch [7/8], Step [99/179], Loss: 0.6700\n",
      "Epoch [7/8], Step [100/179], Loss: 0.4099\n",
      "Epoch [7/8], Step [101/179], Loss: 0.2508\n",
      "Epoch [7/8], Step [102/179], Loss: 0.2614\n",
      "Epoch [7/8], Step [103/179], Loss: 0.3711\n",
      "Epoch [7/8], Step [104/179], Loss: 0.3569\n",
      "Epoch [7/8], Step [105/179], Loss: 0.3384\n",
      "Epoch [7/8], Step [106/179], Loss: 0.5072\n",
      "Epoch [7/8], Step [107/179], Loss: 0.3875\n",
      "Epoch [7/8], Step [108/179], Loss: 0.4537\n",
      "Epoch [7/8], Step [109/179], Loss: 0.8186\n",
      "Epoch [7/8], Step [110/179], Loss: 0.1857\n",
      "Epoch [7/8], Step [111/179], Loss: 0.3223\n",
      "Epoch [7/8], Step [112/179], Loss: 0.3786\n",
      "Epoch [7/8], Step [113/179], Loss: 0.1939\n",
      "Epoch [7/8], Step [114/179], Loss: 0.3696\n",
      "Epoch [7/8], Step [115/179], Loss: 0.5968\n",
      "Epoch [7/8], Step [116/179], Loss: 0.5632\n",
      "Epoch [7/8], Step [117/179], Loss: 0.3838\n",
      "Epoch [7/8], Step [118/179], Loss: 0.4366\n",
      "Epoch [7/8], Step [119/179], Loss: 0.3459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [120/179], Loss: 0.3596\n",
      "Epoch [7/8], Step [121/179], Loss: 0.3058\n",
      "Epoch [7/8], Step [122/179], Loss: 0.6774\n",
      "Epoch [7/8], Step [123/179], Loss: 0.2004\n",
      "Epoch [7/8], Step [124/179], Loss: 0.8125\n",
      "Epoch [7/8], Step [125/179], Loss: 0.6529\n",
      "Epoch [7/8], Step [126/179], Loss: 0.3618\n",
      "Epoch [7/8], Step [127/179], Loss: 0.5673\n",
      "Epoch [7/8], Step [128/179], Loss: 0.3919\n",
      "Epoch [7/8], Step [129/179], Loss: 0.3711\n",
      "Epoch [7/8], Step [130/179], Loss: 0.1585\n",
      "Epoch [7/8], Step [131/179], Loss: 0.4503\n",
      "Epoch [7/8], Step [132/179], Loss: 0.2184\n",
      "Epoch [7/8], Step [133/179], Loss: 0.3822\n",
      "Epoch [7/8], Step [134/179], Loss: 0.5962\n",
      "Epoch [7/8], Step [135/179], Loss: 0.4533\n",
      "Epoch [7/8], Step [136/179], Loss: 0.3344\n",
      "Epoch [7/8], Step [137/179], Loss: 0.3311\n",
      "Epoch [7/8], Step [138/179], Loss: 0.3545\n",
      "Epoch [7/8], Step [139/179], Loss: 0.4376\n",
      "Epoch [7/8], Step [140/179], Loss: 0.2465\n",
      "Epoch [7/8], Step [141/179], Loss: 0.4337\n",
      "Epoch [7/8], Step [142/179], Loss: 0.1734\n",
      "Epoch [7/8], Step [143/179], Loss: 0.3678\n",
      "Epoch [7/8], Step [144/179], Loss: 0.3468\n",
      "Epoch [7/8], Step [145/179], Loss: 0.3828\n",
      "Epoch [7/8], Step [146/179], Loss: 0.4293\n",
      "Epoch [7/8], Step [147/179], Loss: 0.2686\n",
      "Epoch [7/8], Step [148/179], Loss: 0.5757\n",
      "Epoch [7/8], Step [149/179], Loss: 0.2098\n",
      "Epoch [7/8], Step [150/179], Loss: 0.5517\n",
      "Epoch [7/8], Step [151/179], Loss: 0.5488\n",
      "Epoch [7/8], Step [152/179], Loss: 0.2464\n",
      "Epoch [7/8], Step [153/179], Loss: 0.3636\n",
      "Epoch [7/8], Step [154/179], Loss: 0.4308\n",
      "Epoch [7/8], Step [155/179], Loss: 0.5190\n",
      "Epoch [7/8], Step [156/179], Loss: 0.5005\n",
      "Epoch [7/8], Step [157/179], Loss: 0.2604\n",
      "Epoch [7/8], Step [158/179], Loss: 0.4667\n",
      "Epoch [7/8], Step [159/179], Loss: 0.2830\n",
      "Epoch [7/8], Step [160/179], Loss: 0.5228\n",
      "Epoch [7/8], Step [161/179], Loss: 0.3544\n",
      "Epoch [7/8], Step [162/179], Loss: 0.3783\n",
      "Epoch [7/8], Step [163/179], Loss: 0.1732\n",
      "Epoch [7/8], Step [164/179], Loss: 0.3599\n",
      "Epoch [7/8], Step [165/179], Loss: 0.4785\n",
      "Epoch [7/8], Step [166/179], Loss: 0.5494\n",
      "Epoch [7/8], Step [167/179], Loss: 0.5724\n",
      "Epoch [7/8], Step [168/179], Loss: 0.4164\n",
      "Epoch [7/8], Step [169/179], Loss: 0.2637\n",
      "Epoch [7/8], Step [170/179], Loss: 0.4363\n",
      "Epoch [7/8], Step [171/179], Loss: 0.3080\n",
      "Epoch [7/8], Step [172/179], Loss: 0.6210\n",
      "Epoch [7/8], Step [173/179], Loss: 0.3615\n",
      "Epoch [7/8], Step [174/179], Loss: 0.6416\n",
      "Epoch [7/8], Step [175/179], Loss: 0.3987\n",
      "Epoch [7/8], Step [176/179], Loss: 0.2787\n",
      "Epoch [7/8], Step [177/179], Loss: 0.2023\n",
      "Epoch [7/8], Step [178/179], Loss: 0.3781\n",
      "Epoch [7/8], Step [179/179], Loss: 0.3480\n",
      "Epoch [8/8], Step [1/179], Loss: 0.4100\n",
      "Epoch [8/8], Step [2/179], Loss: 0.4055\n",
      "Epoch [8/8], Step [3/179], Loss: 0.3658\n",
      "Epoch [8/8], Step [4/179], Loss: 0.2736\n",
      "Epoch [8/8], Step [5/179], Loss: 0.4584\n",
      "Epoch [8/8], Step [6/179], Loss: 0.4691\n",
      "Epoch [8/8], Step [7/179], Loss: 0.2655\n",
      "Epoch [8/8], Step [8/179], Loss: 0.2496\n",
      "Epoch [8/8], Step [9/179], Loss: 0.7000\n",
      "Epoch [8/8], Step [10/179], Loss: 0.2741\n",
      "Epoch [8/8], Step [11/179], Loss: 0.5786\n",
      "Epoch [8/8], Step [12/179], Loss: 0.5280\n",
      "Epoch [8/8], Step [13/179], Loss: 0.4643\n",
      "Epoch [8/8], Step [14/179], Loss: 0.5217\n",
      "Epoch [8/8], Step [15/179], Loss: 0.4781\n",
      "Epoch [8/8], Step [16/179], Loss: 0.7846\n",
      "Epoch [8/8], Step [17/179], Loss: 0.4483\n",
      "Epoch [8/8], Step [18/179], Loss: 0.2510\n",
      "Epoch [8/8], Step [19/179], Loss: 0.3743\n",
      "Epoch [8/8], Step [20/179], Loss: 0.3749\n",
      "Epoch [8/8], Step [21/179], Loss: 0.3197\n",
      "Epoch [8/8], Step [22/179], Loss: 0.4169\n",
      "Epoch [8/8], Step [23/179], Loss: 0.3010\n",
      "Epoch [8/8], Step [24/179], Loss: 0.4201\n",
      "Epoch [8/8], Step [25/179], Loss: 0.3770\n",
      "Epoch [8/8], Step [26/179], Loss: 0.5075\n",
      "Epoch [8/8], Step [27/179], Loss: 0.4819\n",
      "Epoch [8/8], Step [28/179], Loss: 0.3148\n",
      "Epoch [8/8], Step [29/179], Loss: 0.4003\n",
      "Epoch [8/8], Step [30/179], Loss: 0.2798\n",
      "Epoch [8/8], Step [31/179], Loss: 0.5420\n",
      "Epoch [8/8], Step [32/179], Loss: 0.2357\n",
      "Epoch [8/8], Step [33/179], Loss: 0.4185\n",
      "Epoch [8/8], Step [34/179], Loss: 0.4857\n",
      "Epoch [8/8], Step [35/179], Loss: 0.3385\n",
      "Epoch [8/8], Step [36/179], Loss: 0.4354\n",
      "Epoch [8/8], Step [37/179], Loss: 0.3034\n",
      "Epoch [8/8], Step [38/179], Loss: 0.5560\n",
      "Epoch [8/8], Step [39/179], Loss: 0.2348\n",
      "Epoch [8/8], Step [40/179], Loss: 0.3091\n",
      "Epoch [8/8], Step [41/179], Loss: 0.1775\n",
      "Epoch [8/8], Step [42/179], Loss: 0.5344\n",
      "Epoch [8/8], Step [43/179], Loss: 0.2422\n",
      "Epoch [8/8], Step [44/179], Loss: 0.4155\n",
      "Epoch [8/8], Step [45/179], Loss: 0.4171\n",
      "Epoch [8/8], Step [46/179], Loss: 0.2296\n",
      "Epoch [8/8], Step [47/179], Loss: 0.5684\n",
      "Epoch [8/8], Step [48/179], Loss: 0.6256\n",
      "Epoch [8/8], Step [49/179], Loss: 0.3511\n",
      "Epoch [8/8], Step [50/179], Loss: 0.6561\n",
      "Epoch [8/8], Step [51/179], Loss: 0.2817\n",
      "Epoch [8/8], Step [52/179], Loss: 0.3809\n",
      "Epoch [8/8], Step [53/179], Loss: 0.3705\n",
      "Epoch [8/8], Step [54/179], Loss: 0.3701\n",
      "Epoch [8/8], Step [55/179], Loss: 0.5160\n",
      "Epoch [8/8], Step [56/179], Loss: 0.4004\n",
      "Epoch [8/8], Step [57/179], Loss: 0.4472\n",
      "Epoch [8/8], Step [58/179], Loss: 0.3721\n",
      "Epoch [8/8], Step [59/179], Loss: 0.4493\n",
      "Epoch [8/8], Step [60/179], Loss: 0.3065\n",
      "Epoch [8/8], Step [61/179], Loss: 0.3359\n",
      "Epoch [8/8], Step [62/179], Loss: 0.4247\n",
      "Epoch [8/8], Step [63/179], Loss: 0.2283\n",
      "Epoch [8/8], Step [64/179], Loss: 0.2639\n",
      "Epoch [8/8], Step [65/179], Loss: 0.5520\n",
      "Epoch [8/8], Step [66/179], Loss: 0.4709\n",
      "Epoch [8/8], Step [67/179], Loss: 0.2295\n",
      "Epoch [8/8], Step [68/179], Loss: 0.4990\n",
      "Epoch [8/8], Step [69/179], Loss: 0.4105\n",
      "Epoch [8/8], Step [70/179], Loss: 0.3160\n",
      "Epoch [8/8], Step [71/179], Loss: 0.3558\n",
      "Epoch [8/8], Step [72/179], Loss: 0.3578\n",
      "Epoch [8/8], Step [73/179], Loss: 0.2360\n",
      "Epoch [8/8], Step [74/179], Loss: 0.3765\n",
      "Epoch [8/8], Step [75/179], Loss: 0.4320\n",
      "Epoch [8/8], Step [76/179], Loss: 0.4060\n",
      "Epoch [8/8], Step [77/179], Loss: 0.2244\n",
      "Epoch [8/8], Step [78/179], Loss: 0.4499\n",
      "Epoch [8/8], Step [79/179], Loss: 0.3796\n",
      "Epoch [8/8], Step [80/179], Loss: 0.3596\n",
      "Epoch [8/8], Step [81/179], Loss: 0.3435\n",
      "Epoch [8/8], Step [82/179], Loss: 0.1920\n",
      "Epoch [8/8], Step [83/179], Loss: 0.4963\n",
      "Epoch [8/8], Step [84/179], Loss: 0.3541\n",
      "Epoch [8/8], Step [85/179], Loss: 0.3177\n",
      "Epoch [8/8], Step [86/179], Loss: 0.1793\n",
      "Epoch [8/8], Step [87/179], Loss: 0.2311\n",
      "Epoch [8/8], Step [88/179], Loss: 0.2386\n",
      "Epoch [8/8], Step [89/179], Loss: 0.6409\n",
      "Epoch [8/8], Step [90/179], Loss: 0.3823\n",
      "Epoch [8/8], Step [91/179], Loss: 0.5604\n",
      "Epoch [8/8], Step [92/179], Loss: 0.4362\n",
      "Epoch [8/8], Step [93/179], Loss: 0.3669\n",
      "Epoch [8/8], Step [94/179], Loss: 0.4592\n",
      "Epoch [8/8], Step [95/179], Loss: 0.2048\n",
      "Epoch [8/8], Step [96/179], Loss: 0.5046\n",
      "Epoch [8/8], Step [97/179], Loss: 0.2718\n",
      "Epoch [8/8], Step [98/179], Loss: 0.5797\n",
      "Epoch [8/8], Step [99/179], Loss: 0.4492\n",
      "Epoch [8/8], Step [100/179], Loss: 0.4454\n",
      "Epoch [8/8], Step [101/179], Loss: 0.5142\n",
      "Epoch [8/8], Step [102/179], Loss: 0.4108\n",
      "Epoch [8/8], Step [103/179], Loss: 0.3748\n",
      "Epoch [8/8], Step [104/179], Loss: 0.2329\n",
      "Epoch [8/8], Step [105/179], Loss: 0.6051\n",
      "Epoch [8/8], Step [106/179], Loss: 0.2356\n",
      "Epoch [8/8], Step [107/179], Loss: 0.2713\n",
      "Epoch [8/8], Step [108/179], Loss: 0.4632\n",
      "Epoch [8/8], Step [109/179], Loss: 0.4637\n",
      "Epoch [8/8], Step [110/179], Loss: 0.5753\n",
      "Epoch [8/8], Step [111/179], Loss: 0.4758\n",
      "Epoch [8/8], Step [112/179], Loss: 0.4580\n",
      "Epoch [8/8], Step [113/179], Loss: 0.3749\n",
      "Epoch [8/8], Step [114/179], Loss: 0.2499\n",
      "Epoch [8/8], Step [115/179], Loss: 0.3933\n",
      "Epoch [8/8], Step [116/179], Loss: 0.2463\n",
      "Epoch [8/8], Step [117/179], Loss: 0.3642\n",
      "Epoch [8/8], Step [118/179], Loss: 0.3331\n",
      "Epoch [8/8], Step [119/179], Loss: 0.3901\n",
      "Epoch [8/8], Step [120/179], Loss: 0.3538\n",
      "Epoch [8/8], Step [121/179], Loss: 0.3341\n",
      "Epoch [8/8], Step [122/179], Loss: 0.5147\n",
      "Epoch [8/8], Step [123/179], Loss: 0.2594\n",
      "Epoch [8/8], Step [124/179], Loss: 0.2199\n",
      "Epoch [8/8], Step [125/179], Loss: 0.3977\n",
      "Epoch [8/8], Step [126/179], Loss: 0.5016\n",
      "Epoch [8/8], Step [127/179], Loss: 0.4557\n",
      "Epoch [8/8], Step [128/179], Loss: 0.3548\n",
      "Epoch [8/8], Step [129/179], Loss: 0.3810\n",
      "Epoch [8/8], Step [130/179], Loss: 0.3293\n",
      "Epoch [8/8], Step [131/179], Loss: 0.3372\n",
      "Epoch [8/8], Step [132/179], Loss: 0.3413\n",
      "Epoch [8/8], Step [133/179], Loss: 0.4048\n",
      "Epoch [8/8], Step [134/179], Loss: 0.2892\n",
      "Epoch [8/8], Step [135/179], Loss: 0.3590\n",
      "Epoch [8/8], Step [136/179], Loss: 0.3008\n",
      "Epoch [8/8], Step [137/179], Loss: 0.2993\n",
      "Epoch [8/8], Step [138/179], Loss: 0.3327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [139/179], Loss: 0.3748\n",
      "Epoch [8/8], Step [140/179], Loss: 0.3445\n",
      "Epoch [8/8], Step [141/179], Loss: 0.3539\n",
      "Epoch [8/8], Step [142/179], Loss: 0.5876\n",
      "Epoch [8/8], Step [143/179], Loss: 0.3553\n",
      "Epoch [8/8], Step [144/179], Loss: 0.2297\n",
      "Epoch [8/8], Step [145/179], Loss: 0.3178\n",
      "Epoch [8/8], Step [146/179], Loss: 0.2297\n",
      "Epoch [8/8], Step [147/179], Loss: 0.6281\n",
      "Epoch [8/8], Step [148/179], Loss: 0.3240\n",
      "Epoch [8/8], Step [149/179], Loss: 0.1383\n",
      "Epoch [8/8], Step [150/179], Loss: 0.4396\n",
      "Epoch [8/8], Step [151/179], Loss: 0.5042\n",
      "Epoch [8/8], Step [152/179], Loss: 0.6130\n",
      "Epoch [8/8], Step [153/179], Loss: 0.3769\n",
      "Epoch [8/8], Step [154/179], Loss: 0.2892\n",
      "Epoch [8/8], Step [155/179], Loss: 0.5277\n",
      "Epoch [8/8], Step [156/179], Loss: 0.3835\n",
      "Epoch [8/8], Step [157/179], Loss: 0.5956\n",
      "Epoch [8/8], Step [158/179], Loss: 0.3270\n",
      "Epoch [8/8], Step [159/179], Loss: 0.4092\n",
      "Epoch [8/8], Step [160/179], Loss: 0.4509\n",
      "Epoch [8/8], Step [161/179], Loss: 0.3343\n",
      "Epoch [8/8], Step [162/179], Loss: 0.5064\n",
      "Epoch [8/8], Step [163/179], Loss: 0.2492\n",
      "Epoch [8/8], Step [164/179], Loss: 0.5871\n",
      "Epoch [8/8], Step [165/179], Loss: 0.1644\n",
      "Epoch [8/8], Step [166/179], Loss: 0.6333\n",
      "Epoch [8/8], Step [167/179], Loss: 0.4700\n",
      "Epoch [8/8], Step [168/179], Loss: 0.4650\n",
      "Epoch [8/8], Step [169/179], Loss: 0.2647\n",
      "Epoch [8/8], Step [170/179], Loss: 0.4586\n",
      "Epoch [8/8], Step [171/179], Loss: 0.3196\n",
      "Epoch [8/8], Step [172/179], Loss: 0.3642\n",
      "Epoch [8/8], Step [173/179], Loss: 0.2014\n",
      "Epoch [8/8], Step [174/179], Loss: 0.2146\n",
      "Epoch [8/8], Step [175/179], Loss: 0.2667\n",
      "Epoch [8/8], Step [176/179], Loss: 0.6748\n",
      "Epoch [8/8], Step [177/179], Loss: 0.9283\n",
      "Epoch [8/8], Step [178/179], Loss: 0.5854\n",
      "Epoch [8/8], Step [179/179], Loss: 0.2614\n",
      "Accuracy of the network: 81.61708619374524 %\n",
      "Accuracy of glioma: 80.0 %\n",
      "Accuracy of meningioma: 80.0 %\n",
      "Accuracy of notumor: 84.0909090909091 %\n",
      "Accuracy of pituitary: 87.5 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "from Train import TumorModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48811faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TumorModel(3, 4, 4)\n",
    "model.load_state_dict(torch.load('Tumor_Model.pth'))\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load your new image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f19268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 2\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "new_image_path = './multiple_brain_tumor/Testing/notumor/Te-no_0010.jpg'\n",
    "new_image = Image.open(new_image_path)\n",
    "\n",
    "input_data = transform(new_image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_data)\n",
    "\n",
    "predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "\n",
    "\n",
    "def label():\n",
    "    if predicted_class == 2:\n",
    "        print(\"Healthy\")\n",
    "    else:\n",
    "        print(\"Tumor\")\n",
    "\n",
    "\n",
    "label()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8aef008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n",
      "Tumor\n"
     ]
    }
   ],
   "source": [
    "new_image_path = './multiple_brain_tumor/Testing/glioma/Te-gl_0015.jpg'\n",
    "new_image = Image.open(new_image_path)\n",
    "\n",
    "input_data = transform(new_image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_data)\n",
    "\n",
    "predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "\n",
    "\n",
    "def label():\n",
    "    if predicted_class == 2:\n",
    "        print(\"Healthy\")\n",
    "    else:\n",
    "        print(\"Tumor\")\n",
    "\n",
    "\n",
    "label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843eedca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n",
      "Tumor\n"
     ]
    }
   ],
   "source": [
    "new_image_path = './Brain_Tumor_processed/no/Image100.jpg'\n",
    "new_image = Image.open(new_image_path)\n",
    "\n",
    "input_data = transform(new_image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_data)\n",
    "\n",
    "predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "\n",
    "\n",
    "def label():\n",
    "    if predicted_class == 2:\n",
    "        print(\"Healthy\")\n",
    "    else:\n",
    "        print(\"Tumor\")\n",
    "\n",
    "\n",
    "label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc13e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
